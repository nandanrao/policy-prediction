@article{Jan,
archivePrefix = {arXiv},
arxivId = {arXiv:1812.11806v2},
author = {Jan, L G},
eprint = {arXiv:1812.11806v2},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/1812.11806.pdf:pdf},
keywords = {covariate shift,domain adapta-,machine learning,pattern recognition,sample selection bias,tion,transfer learning},
title = {domain adaptation and transfer learning}
}
@article{Pinheiro,
author = {Pinheiro, Pedro O},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/Pinheiro{\_}Unsupervised{\_}Domain{\_}Adaptation{\_}CVPR{\_}2018{\_}paper.pdf:pdf},
number = {i},
title = {{Unsupervised Domain Adaptation with Similarity Learning}}
}
@book{Weiss2016,
abstract = {Machine learning and data mining techniques have been used in numerous real-world applications. An assumption of traditional machine learning methodologies is the training data and testing data are taken from the same domain, such that the input feature space and data distribution characteristics are the same. However, in some real-world machine learning scenarios, this assumption does not hold. There are cases where training data is expensive or difficult to collect. Therefore, there is a need to create high-performance learners trained with more easily obtained data from different domains. This methodology is referred to as transfer learning. This survey paper formally defines transfer learning, presents information on current solutions, and reviews applications applied to transfer learning. Lastly, there is information listed on software downloads for various transfer learning solutions and a discussion of possible future research work. The transfer learning solutions surveyed are independent of data size and can be applied to big data environments.},
author = {Weiss, Karl and Khoshgoftaar, Taghi M. and Wang, Ding Ding},
booktitle = {Journal of Big Data},
doi = {10.1186/s40537-016-0043-6},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/10.1186{\_}s40537-016-0043-6.pdf:pdf},
isbn = {4053701600},
issn = {21961115},
keywords = {Data mining,Domain adaptation,Machine learning,Survey,Transfer learning},
number = {1},
publisher = {Springer International Publishing},
title = {{A survey of transfer learning}},
volume = {3},
year = {2016}
}
@article{John2010,
author = {John, Shai Ben-david and Koby, Blitzer and Kulesza, Alex and Pereira, Fernando and Vaughan, Jennifer Wortman},
doi = {10.1007/s10994-009-5152-4},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/36364.pdf:pdf},
pages = {151--175},
title = {{A theory of learning from different domains}},
year = {2010}
}
@article{Ben-David2006,
author = {Ben-David, Shai and Blitzer, John and Crammer, Koby and Pereira, Fernando},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/nips06.pdf:pdf},
title = {{Analysis of Representations for Domain Adaptation}},
year = {2006}
}
@article{Pan2010,
abstract = {{\textcopyright} Springer International Publishing AG, part of Springer Nature 2018. Recommending tweets that a user might retweet plays an important role either in satisfying users' information needs or in the dissemination of information in microblogging services such as Twitter. In this paper, we propose a deep neural network for tweet recommendations with author-based Long Short-Term Memory networks for learning the latent representations/embeddings of tweets. Our approach predicts the preference score of a tweet based on (1) the similarity between the embeddings of a user and the tweet, (2) the similarity between the embeddings of the user and the author (who posted the tweet). Despite its simplicity, we present that our approach can significantly outperform state-of-the-art methods with or without explicit features for recommending tweets in terms of five evaluation metrics.},
archivePrefix = {arXiv},
arxivId = {1603.06111},
author = {Pan, Sinno Jialin and Yang, Qiang},
doi = {10.1109/TKDE.2009.191},
eprint = {1603.06111},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/tkde{\_}transfer{\_}learning.pdf:pdf},
isbn = {9791095546009},
issn = {1041-4347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Ambiguity,Automated essay scoring,Automated essay scoring (AES),Automated scoring system,Automatic reference answer generation,Class imbalance,Convolutional neural networks,Data mining,Deep learning,Deep neural network,Domain adaptation,Emotion Classification,Essay,Finite state automata,Generalized Latent Semantic Analysis,Hierarchical Recurrent Neural Networks,Image classification,Imbalanced training,Latent Semantic Analysis,Levenshtein distance,Long short-term memory,Machine learning,Maximum marginal relevance,N-gram,Natural Language Processing,Neural network,Oversampling,Penilaian Otomatis,Rating criteria,Semantic word representations,Sentence similarity,Sentiment Classification,Short answer scoring,Singular Value Decomposition,Spelling correction,Survey,Technical Papers: Machine Learning Methods,Technical Papers: Natural Language Processing and,Topic information,Transfer learning,and engineering,and identify the substances,asap dataset,assessment mecha- did you,attention layer,automatic scoring,background information about mr,bi-lstm,char embedding,clustering,computer-assisted language learning,deep learning,ding,during the story,effect that background information,ek laboratories,essay exams,evolutionary,explain the,fault diagnosis,finite state automata,fully connected layer,glove,has on paul,how,icle,in mock rocks,keyword,learning to rank,leonard,levenshtein distance,long short-term memory,lstm,n-gram,nanyang technological university,prompt,question,recurrent neural network,remaining useful life,school of computer science,semantic similarity,semantics similarity,sentence similarity,separate the salt from,short answers are powerful,short-answer scoring,siamese neural network,singapore,spelling,support,swam intelligence,text mining,text similarity,the reader gets,the water,to separate,transfer learning,word embed-,you used several methods},
month = {oct},
number = {10},
pages = {1345--1359},
pmid = {24086296},
title = {{A Survey on Transfer Learning}},
url = {https://doi.org/10.1016/j.artmed.2018.03.006{\%}0Ahttp://arxiv.org/abs/1801.06146{\%}0Ahttp://arxiv.org/abs/1902.01382{\%}0Ahttp://arxiv.org/abs/1902.09092{\%}0Ahttp://arxiv.org/abs/1703.06345{\%}0Ahttp://dblp.org/rec/conf/aaai/ZhouPTH16{\%}0Awww.aaai.org{\%}0Ahttp://arxiv.or http://ieeexplore.ieee.org/document/5288526/},
volume = {22},
year = {2010}
}
