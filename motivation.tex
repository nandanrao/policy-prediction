\documentclass[a4paper,12pt]{article}
\usepackage[backend=biber, citestyle=authoryear, bibencoding=utf8]{biblatex}
\addbibresource{external-validity.bib}
\addbibresource{econ-causal-history.bib}
\addbibresource{causality.bib}
\addbibresource{domain-adaptation.bib}
\addbibresource{economic-forests.bib}
\addbibresource{extra-citations.bib}

\usepackage{amsmath, amsfonts, mathtools, csquotes, bm, centernot, bbm}

\usepackage[default,light,bold]{sourceserifpro}
\usepackage[T1]{fontenc}


\usepackage{pgf, tikz}
\usetikzlibrary{arrows, automata}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\D}{\mathcal{D}}



\newcommand{\CI}{\mathrel{\perp\mspace{-10mu}\perp}}
\newcommand{\nCI}{\centernot{\CI}}

\title{ Policy Prediction: The Missing Tool in Experimental Econometrics and a Roadmap to Fix It  }

\author{Nandan Rao}


\begin{document}

\maketitle

\begin{abstract}
Applied economics research is most often ``applied'' to policy making, yet formal frameworks for policy prediction are largely ignored in the literature of experimental and quasi-experimental econometrics. I explain the existence of this gap from a historical perspective, review the current methods that do exist, and set a research agenda to fill it using recent ideas from econometrandics and machine learning. 
\end{abstract}


\section{Introduction}

Randomized control trials, or natural experiments that replicate them, have become the official gold standard of empirical work in economics and many related fields. More and more, policy makers are encouraged to look to RCTs to make ``evidence-based'' policy decisions (\cite{Manski2013, Cartwright2013}). 

Many prominent economists have expressed a concern that RCTs, and the quasi-experimental methods that seek to replicate them (i.e. natural experiments, intrumental variables, regression discontinuity, etc.) are particularly difficult to generalize to new contexts due to their orverarching concern for internal validity, often at the expense of external validity (\cite{Heckman1995, Heckman2008, Deaton2010, Manski2013, Deaton2018}). 

Making evidence-based policy decisions is an act of generalizing from previous studies to decisions about the future. Charles Manski \parencite*{Manski2013}, voices the concern that experimental studies have tended to ``be silent'' on the question of external validity and that ``from the perspective of policy choice... What matters is the informativeness of a study for policy making, which depends jointly on internal and external validity.''

This silence on external validity means that, despite the huge field of research and techniques for ensuring causal identification (internal validity) via experimental or quasi-experimental methods, very little has been done to either A) create tools to prove that the same results will apply elsewhere or B) create tools to predict the results in a new location, given proven results from one or more experiments. 

Banjerjee himself \parencite{Snowberg2016}, a large proponent of RCTs, has recently echoed the current lack of and need for more formal systems for the generalization of experimental studies, saying ``it is our belief that creating a rigorous framework for external validity is an important step in completing an ecosystem for social science field experiments, and a complement to many other aspects of experimentation.''

A small but growing literature has sprung up around empirically proving that results from prominent RCT or other causally identified studies do not easily extrapolate to new contexts (\cite{Pritchett2016}, \cite{Allcott2015}, \cite{Bisbee2017}, \cite{Rosenzweig2019}). These results emphasize a need for powerful extrapolation tools if policy makers are realistically expected to make decisions based on ``evidence.'' 

Predicting the results of of a policy in a new location is, tautologically, a prediction problem. Machine learning is a field which has been very successful at formalizing prediction problems. Domain adaptation, a subfield of machine learning, formalizes the problem of moving from one (or more) domains with labelled data to a new domain where only unlaballed data is available (\cite[for a survey, see][]{Pan2010}).

This article will take the following form: Section 2 will define external and internal validity, their role in scientific inference, review the development of the Fisherian experimental model of statistical inference, and set out the need for a formal framework of extrapolation. Section 3 will argue that a target context is necessary by reviewing the development of such an argument in mid-century econometrics and formalizing their argument in probablistic terms. Section 4 will introduce the concept of domain adaptation in machine learning. Section 5 will review current discussions and state of the art for formal frameworks within the literature of empirical economics and policy prediction. Section 6 will present recent work in machine learning that combines the ideas from structural econometrics and domain adaptation. Section 7 will present conclusions and frame a research agenda. 


% Nineteenth-century economists thought of mathematics as a tool for deductive reasoning in economics, used to derive the logical conclusions of known laws (\cite{Greasley1991}). Statistics, on the other hand, was viewed as a tool for inductive reasoning, used to establish economic regularities that could be considered general laws. Greasley and Morgan \parencite*{Greasley1991} quote Stanley Jevons as saying: 

% \begin{displayquote}
% ``The deductive science of Economy must be verified and rendered useful by the purely inductive science of Statistics. Theory must be invested with the reality and life of fact. But the difficulties of this union are immensely great.''
% \end{displayquote}




\section{Validity and Counterfactual Identification}

\subsection{A Taxonomy of Validities}

Critiques against the holy status of RCTs have focused on their preference for internal validity and tendency to completely ignore external validity. It is worth defining these terms and reflecting on exactly what it is that experimental methods provide us. The most agreed upon definition of these terms comes from \cite{Shadish2002} (an update of their previously popular taxonomy of validities from \cite{Cook1979}):

\begin{displayquote}
\textbf{Statistical Conclusion Validity:} The validity of inferences about the correlation (covariation) between treatment and outcome.

\textbf{Internal Validity:} The validity of inferences about whether observed covariation between A (the presumed treatment) and B (the presumed outcome) reflects a causal relationship from A to B as those variables were manipulated or measured.

\textbf{Construct Validity:} The validity of inferences about the higher order constructs that represent sampling particulars.
 
\textbf{External Validity:} The validity of inferences about whether the cause-effect relationship holds over variation in persons, settings, treatment variables, and measurement variables.
\end{displayquote}

Many authors use the term external validity to refer to what Shadish, Cook, and Campbell separate into construct and external validity. As both are involved in generalization, and both are external to the particulars of the sample, I will follow that abuse of terminology and refer to all the challenges of both as ``external validity.''

What, then, is meant by the term ``inference?''

\subsection{Inference and Statistics}

Following from Aristotelian tradition of logic, inference is the process of reasoning and can be broken into two parts: reasoning from particulars to generals (induction) and reasoning from generals to particulars (deduction). Deduction is often recognized by Aristotle's syllogisms, such as:

\begin{displayquote}
All men are mortal \\
Socrates is a man  \\
Therefore, Socrates is mortal
\end{displayquote}

Once one has induced a general law (``all men are mortal''), one can deduce facts that might otherwise not yet be apparent (``Socrates will die''). There is, of course, a natural contradiction in this process: how can one know that all men are mortal, if one did not already know that Socrates will die? In other words, how can one ever claim that ``all men are mortal'' until one has seen every man die?

This problem forms the foundation of David Hume's ``Problem of Induction'':

\begin{displayquote}
``As to past Experience, it can be allowed to give direct and certain information of those precise objects only, and that precise period of time, which fell under its cognizance: but why this experience should be extended to future times, and to other objects, which for aught we know, may be only in appearance similar, this is the main question on which I would insist'' (Hume ).
\end{displayquote}

There are two distinct problems Hume raises, that of generalizing from one object to another (from seeing some men die to assuming all men have died) and that of generalizing from the past to the future (because all men have died, thus, all men will die). He goes on to explain that this extrapolation is only valid under strong assumptions as to the ``uniformity of nature.'' One must assume nature is uniform in such a way as to enable extrapolation from one object to another or from the past to the future.

R.A. Fisher framed his statistical techniques in terms of ``inductive inference.'' In the introduction to The Design of Experiments \parencite*{Fisher1935}, he explicitly frames his canonical book and its techniques in the terms of logicians such as Hume, arguing for the possibility of induction via statistical methods:
%
\begin{displayquote}
``it is possible to draw valid inferences from the results of experimentation... as a statistician would say, from a sample to the population from which the sample was drawn, or, as a logician might put it, from the particular to the general.'' (Fisher   )
\end{displayquote}

Fisherian statistical inference is a tool in the process of induction that seeks to address Hume's problem of extending experience with one object to that of similar objects. In particular, the ``similar objects'' to which conclusions are extended are not just similar in appearance, but rather have a distinct relationship to the experienced objects: they are the population from which the experienced objects represent a sample.

Fisher's methodologies for significance testing relate to drawing conclusions about populations given a sample. The validity of the use of such techniques in a study falls squarely under the category of ``statistically conclusion validity'' in the taxonomy of Shadish, Cook, and Campbell.

Fisher's theory of experiments (in particular, the RCT) addresses internal validity and causality. The connection between the RCT and the identification of a causal relationship comes straight from John Stuart Mill's (1843) ``method of difference'' for causal discovery:

\begin{displayquote}
  ``If an instance in which the phenomenon under investigation occurs, and an instance in which it does not occur, have every circumstance save one in common, that one occurring only in the former; the circumstance in which alone the two instances differ, is the effect, or cause, or a necessary part of the cause, of the phenomenon.''
\end{displayquote}

Fisher \parencite*{Fisher1935} clearly had this in mind when he defended randomization, claiming that holding all other variables constant was not feasible, and thus, holding them to the same distribution, by making the assignment of treatment independent of those variables, was desirable (\cite{Rosenbaum2005}). It should be noted, however, that for the subjects of interest to Fisher, the difference between something being a ``cause'' or ``a necessary part of the cause'' was not especially important. What Fisher was essentially interested in was interventional prediction: what is the effect when one makes individual changes to the growing conditions of these plants.

Holland \parencite*{Holland1986} begins his landmark paper, before formulating the Rubin-Neyman causal model, by framing his goal:

\begin{displayquote}
``Others are concerned with deducing the causes of a given effect. Still others are interested in understanding the details of causal mechanisms. The emphasis here will be on measuring the effects of causes because this seems to be a place where statistics, which is concerned with measurement, has contributions to make.''
\end{displayquote}

The success of Fisher's framework of randomization and the Rubin-Neyman causal model comes down to this razor focus in purpose: they make no claims to discover all the causes of a given effect, to discover the mechanism of the cause, or even to separate between a cause or a necessary part of a cause. They allow us to reason about counterfactuals: what would have happened, on average and in the past, had we treated our entire population rather than a randomized part of a randomized sample drawn from that population. It operationalizes Mill's method of differences, creating a pathway to internal validity that is achievable in the real world.

The counterfactual causal model, however, provides no framework for generalizing from a specific population to a more general one or from the past to the future (\cite{Heckman2008}). The ``effects of causes'' is a black-box methodology for causal identification  (\cite{Heckman1995}). It does not require one to answer the question ``why'' and it is therefore inherently context specific: it asks, ``what happened when I did $X$ in context $D$?'' In Fisher's line of work (agriculture), none of those shortcomings were problematic. He was able to randomly sample from the exact population (seeds of grain) to which his inferences needed to generalize.

The success of Fisher's randomization in his field of agriculture, and the subsequent success of the Rubin-Neyman causal model in epidemiology, is in no way incidental. These are fields that deal with encapsulated biological units where agreed-upon scientific theory tells us that the relationships in these systems will be invariant to a wide degree of changes that happen in the world from one year to the next or from one country to the next.

For example, it is implicitly assumed that the growth of corn will not be affected by its expectations of how it will be cut down and processed during the harvest. Corn grows according to the properties of the soil and the sun it receives. Its growth will be independent of its destination in corn flakes or arepas, conditional on those properties. Opioids will inhibit pain in humans, regardless of their political ideology, faith in their governmental institutions, or their love of Shark Tank.

These arguments are not made explicitly, but their validity is absolutely necessary to enable the application of their findings: they create laws defined in relation to the entire range of contextual changes that one might want for prediction- and decision-making and that allows the laws to be applied deductively in a wide array of useful situations.

While the validity of these arguments is taken for granted in contained biological processes, this is simply not the case in the social sciences. This is why Shadish, Cook, and Campbell lay out 19 different threats to construct and external validity of social science experiments. In the case of growing corn, the threats either simply do not apply or are implicitly neutralized through basic scientific understanding of plant growth.

In the case of economics, the outcomes are regularly based on individuals decisions to consume, work, study, invest, or move. The treatments are regularly subjected to a gauntlet of mediating factors and interacting variables that are highly context-dependent and correlated across individuals in a single place and time. The population for which one wants to draw actionable inference is in the future and it is fundamentally different from the population the sample from drawn from. Internal validity of a study in economics is not sufficient for actionable inference to take place in a new context. 

\subsection{The Danger of Informal Inference}

It might be argued that it is, and should be, up to the sound judgement and expert opinion of the policy maker to determine if a given counterfactual analysis should extrapolate to their context or not. Empirical studies only need to provide internal validity, according to this argument, as the extrapolation is done by experts who know their target domain.

While incorporating expert domain knowledge can only help predictions, we can differentiation between using a formal statistical framework and using an informal framework of judgement. John Stuart Mill, in his -----, reflects on these difference and the dangers of inferring without formal frameworks.

He begins by denying that the only process of inference consists of separately applying induction (particulars to generals) and then deduction (general law to particular context):

\begin{displayquote}
  ``All inference is from particulars to particulars: General propositions are merely registers of such inferences already made, and short formulae for making more: The major premise of a syllogism, consequently, is a formula of this description: and the conclusion is not an inference drawn from the formula, but an inference drawn according to the formula: the real logical antecedent, or premise, being the particular facts from which the general proposition was collected by induction.''  (John Stuart Mill  )
\end{displayquote}

In other words, in the process of creating the general law, one has created a series of particular laws, and only once assured that all the particular laws are valid can one be assured of the more general law. He goes on to caution against the direct reasoning of particulars to particulars because it is informal and we are likely to bring our own biases into the process and make mistakes:
%
\begin{displayquote}
``In reasoning from a course of individual observations to some new and unobserved case, which we are but imperfectly acquainted with (or we should not be inquiring into it), and in which, since we are inquiring into it, we probably feel a peculiar interest; there is very little to prevent us from giving way to negligence, or to any bias which may affect our wishes or our imagination, and, under that influence, accepting insufficient evidence as sufficient.'' (John Stuart Mill )
\end{displayquote}

John Stuart Mill argues that formal procedures, such as that implied by the framework of induction and deduction, allow individuals to avoid these biases. In his world, there was no formal procedure for reasoning from particulars to particulars, but there was for reasoning from particulars to general. As such, he recommends the latter as a way to avoid biases of ``wishes'' and ``imagination.''

In an ideal world, the research community has discovered a set of general laws that are invariant to all contexts and the policy maker can apply them, via the process of deduction, to get the desired result in their circumstance. But what if that general law has not yet been discovered? Then the policy maker must look at individual studies (particulars) and attempt to infer a prediction for the result of a similar policy in their context. This is exactly the situation that John Stuart Mill has warned is fertile ground for bias and imagination.

\section{The Origins of Structure}

Economists during the same period as Fisher took a different approach to conceptualizing and thinking about the inference they were doing. Ragnar Frisch, theorizing about macro-dynamic analysis, set out several key ideas relating to the structure of a system and the autonomy of a structure (\cite{Frisch1995}). For Frisch, the ``structure'' of a system was all the characteristics of the phenomena that could be quantitatively described. In his macrodynamic systems, the structure is defined by a set of functional (simultaneous) equations. He then poses the question: what would happen to the system due to an arbitrary change in a single variable? To do so could imply a different ``structure'' than the one which the equations describe, requiring a different set of equations altogether to describe the new system.

\begin{displayquote}
``But when we start speaking of the possibility of a structure different from what it actually is, we have introduced a fundamentally new idea. The big question will now be in what directions should we conceive of a possibility of changing the structure?\ldots To get a real answer we must introduce some fundamentally new information. We do this by investigating what features of our structure are in fact the most autonomous in the sense that they could be maintained unaltered while other features of the structure were changed\ldots So we are led to constructing a sort of super-structure, which helps us to pick out those particular equations in the main structure to which we can attribute a high degree of autonomy in the above sense. The higher this degree of autonomy, the more fundamental is the equation, the deeper is the insight which it gives us into the way in which the system functions, in short, the nearer it comes to being a real explanation. Such relations form the essence of 'theory'.''
\end{displayquote}

This concept, that of ``the essence of theory'' being the discovery of some relationship that is autonomous and invariant to a great degree of changes we can imagine performing to a system, is taken up by Trygve Haavelmo \parencite*{Haavelmo1944}, who writes that:

\begin{displayquote}
``The principal task of economic theory is to establish such relations as might be expected to possess as high a degree of autonomy as possible.''
\end{displayquote}

He then goes on to consider a distinction between the ``invariance'' of a relationship under hypothetical changes in structure versus the ``persistence'' of a relationship under observed changes in structure:

\begin{displayquote}
``\ldots if we always try to form such relations as are autonomous with respect to those changes that are in fact \textit{most likely to occur}, and if we succeed in doing so, then, of course, there will be a very close connection between actual persistence and theoretical degree of autonomy.''
\end{displayquote}

This implies a connection between autonomy and the \textit{type} of changes to which it is invariant \textit{with respect to}. This point is made even more explicitly by Leonid Hurwicz \parencite*{Hurwicz1966}. Similar to his predecessors, his model consists of a system of equations that constrain the state of the world, given a history of states. He calls this system of equations a ``behavior pattern.'' He states that:

\begin{displayquote}
``A great deal of effort is devoted in econometrics and elsewhere to attempts at finding the behavior pattern of an observed configuration\ldots But do we really need the knowledge of the behavior pattern of the configuration?\ldots It will be approached here from the viewpoint of prediction\ldots That is, the word `need' in the above question will be understood as `need' for purposes of prediction.''
\end{displayquote}

% He then goes on to explain that the need is driven by the type of prediction one hopes to make. If it can be assumed that the behavior pattern does not change (i.e. the joint distribution of our variables of interest comes from the same distribution tomorrow as today), then we do not actually need to find the behavioral pattern, as we can predict the state of the world tomorrow based on an expectation of the past.

He then goes on to define what he calls a ``structural form'' as one which is identified and identical across all possible behavior changes that one \textit{needs} to predict within. He stresses that:

\begin{displayquote}
``The most important point is that the concept of structure is relative to the domain of modifications anticipated.''
\end{displayquote}

Thus, there is an inherent and irrevocable connection between what we consider a ``law'' and the degree of changes we require the law to persist across. The law is defined in relation to those changes. Folowing Hume, the performance of induction is always connected to a specific assumption about the uniformity of nature. Additionally, the exact way in which nature must be uniform, for a particular system under study, is defined by the predictions we need to make with the discovered relations in that system.


\subsection{Invariant Conditionals}

Robert Engle, building on the work of Frisch and Hurwicz regarding invariant/autonomous structures, creates a statistical definition of what he terms ``super exogoneity.'' A good review of this historical evolution of autonomy can be found in \cite{Aldrich1989}.

Super exogoneity is defined by Engle as follows. Consider a model in which the ``structure'' (the functional form) of a relationship between outcome $y$ and variable $z$ is parameterized by ``structural'' parameters $\lambda_1, \lambda_2 \in \lambda$. If the joint density implied by the model can be factorized as follows:
%
$$
P(y, z, \lambda) = P(y | z, \lambda_1)P(z | \lambda_2)
$$

and the conditional, $P(y | z, \lambda_1)$ remains invariant to changes in the marginal $p(z)$ (presumably caused by changes in it's generating process, parameterized by $\lambda_2$), then $z$ is super exogenous. This definition allows super exogoneity to be refuted by data:

\begin{displayquote}
``It is clear that any assertion concerning super exogeneity is refutable in the data for past changes in $D(z, | X_{t-1}, \lambda_2)$ by examining the behavior of the conditional model for invariance when the parameters of the exogenous process changed...However, super exogeneity for all changes in the distribution of z, must remain a conjecture until refuted, both because nothing precludes agents from simply changing their behavior at a certain instant and because only a limited range of interventions will have occurred in any given sample period.''
\end{displayquote}

Writing a system in terms of super exogenous variables is akin to finding Frisch's ``super-structure.'' This is the part of the system that stays invariant to a set of allowable modifications, parameterized by $\lambda_2$. This is the part of the system that must be known in order to make policy predictions, when $\lambda_2$ includes the changes in the policy, whose total causal effect (\cite[in the sense of][]{Pearl2000}) is transmitted to the output variable $y$ through $z$. The existence of such an invariant super-structure is a necessary prerequisite to successfully predict the effects of policy and therefore to successfully inform policy choice from empirical data.

This method of looking in the data for a conditional distribution that is invariant to ``structural'' changes that lead to differences in the marginal distribution of its ``inputs'' has formed the basis of new line of work in statistics and machine learning around causal discovery (\cite{Peters2015, Heinze-deml2017, Rojas-carulla2018}). This line of research, in the terms of Engle, can be thought of as methods for model discovery by selecting covariates that are super exogenous to the output in question. They work without making any assumption as to strong or weak exogeneity (\cite[in the sense of][]{Engle1983}) of the covariates, assumptions that are often provided by basic theory in economic contexts. Implications for further research based on such additional assumptions will be returned to in the sequel. 

% It should be clear that, even if such a super-structure exists, it is possible that $z$ is either fully or partially unobserved. How will that effect the invariance? Consider again the model with a slight modification (we drop $\lambda_1$ for ease of notation, as we are only interested in the conditions under which the conditional distribution is invariant and thus $\lambda_1$ is static):
% %
% $$
% P(y, z_1, z_2, \lambda) = P(y | z_1, z_2)P(z_1 | z_2, \lambda_2)p(z_2 | \lambda_3)
% $$

% Where we consider a latent variable, $z_2$, which is independent of the policy modification of interest, $\lambda_2$, and whose generating process is controlled by structural parameter $\lambda_3$. We can marginalize out $z_2$ by fixing $\lambda_3 = \ell$:
% %
% $$
% P(y, z_1, z_2, \lambda_2, \lambda_3 = \ell_3) = P(y | z_1, \lambda_3 = \ell_3)P(z_1 | \lambda_2, \lambda_3 = \ell_3)
% $$

% Thus, our conditional is still invariant to modifications to $\lambda_2$, but it is fixed as regards the structural parameter $\lambda_3$ which determined the distribution of the latent variable $z_2$. If the latent variable, $z_2$, was also effected by the structural parameters $\lambda_2$, then one would have to fix that as well in order to get an invariant conditional:
% %
% $$
% P(y | z_1, \lambda_2 = \ell_2, \lambda_3 = \ell_3)
% $$

% It is worth considering the implications of the existence of latent variables in the super-structure that are evident in this simple exercise of probability algebra:

% \begin{enumerate}
% \item If the latent variable is not independent of the set of modifications to which a relationship should be invariant ($P(z | \lambda_2) \neq P(z)$), then the relationship cannot be determined generally.
% \item If the latent variable is independent of the set of modifications to which a relationship should be invariant ($P(z | \lambda_2) = P(z)$, then marginalizing out the latent variable results in an invariant relationship.
% \end{enumerate}

% This requirement is proved rigorously in \cite{Pearl2014}, where they refer to a relationship as ``transportable'' if it is invariant across a set of modifications.

% Given that $z_2$ is latent, it might be difficult, in practice, to know whether or not it is independent of $\lambda_2$: one cannot directly obtain an empirical estimate of $P(z_2)$. What about going in the reverse direction: if one had empirical data across a set of modifications (different values of $\lambda_2$) and one discovered an empirical conditional distribution $P(y | z_1)$ that is invariant across those modifications, does that imply that $P(z_2 | \lambda_2) = P(z_2)$?

% TODO: discuss what Peters says counts as an intervention

% TODO: summarize a few of the articles from the causal recovery literature around invariant conditionals??

% TODO: Put history of domain adaptation in machine learning.

\section{Domain Adaptation}

Consider a domain, $\D$, which we define as consisting of a feature space, $\mathcal{X}$, and a marginal distribution $P(X)$ where $X = \{x_1,\ldots,x_n\} \in \mathcal{X}$. A task, $\mathcal{T}$, consists of an outcome space, $\mathcal{Y}$ and a true generating mechanism $f: \mathcal{X} \rightarrow \mathcal{Y}$.

Many traditional machine learning applications involve the assumption that the domain and the task are the same in the training data and the prediction context. Transfer learning is the generic name for all frameworks that work outside these assumptions, when either the domain, the task, or both are different. 

I will consider the term domain adaptation in the sense of Ben-David \parencite*{Ben-David2006} and Pan and Yang \parencite*{Pan2010}. Under this definition, domain adaptation is a specific form of transfer learning where the task $\mathcal{T}$ is constant and the feature space, $\mathcal{X}$ is the same across all domains. There is a target domain, $\D_T$ from which one has samples $\{x_1,\ldots,x_i\} \in X$ and (one or more) source domain(s), $\D_S$, from which one has tuples $\{(y_1, x_1),\dots,(y_n, x_n)\} \in (Y, X)$. 

The consistency of the task relates to the construct validity in the taxonomy of \cite{Shadish2002}. If the outcome measured in one experiment might be considered to measure a different construct than the outcome measured in another experiment, then the task can not be said to be the same and this problem formulation fails. 

Allow $\mathcal{X} = \{\mathcal{W}, \mathcal{Z}\}$, where $\mathcal{W} = \{W_0, W_1\}$ a binary treatment and $\mathcal{Z} = {Z_1,...,Z_P}$ a set of covariates. The assumption of the consistency of the feature space relies on the construct validity of both the treatment and the covariates. If this validity does not hold, if the covariates or the treatment across domains represent a different construct, then the formulation of domain adaptation does not hold. A formulation in which the feature space is allowed to change would be more appropriate (see ...). 

If one considers a random-variable formulation of the true generating mechanism, $f(\cdot)$, then the assumption of task consistency implies the conditional distribution is consistent across tasks, thus $P(Y_S | X_S) = P(Y_T | X_T)$. This assumption is referred to as covariate-shift, as the only difference between domains $\D_S$ and $\D_T$ is that of the marginal covariate distribution, $P(X_S) \neq P(X_T)$. 

Under the assumptions of covariate shift, Shimodaira \parencite*{Shimodaira2000} shows that the optimum likelihood function for a maximum likelihood estimator of $P(Y_T | X_T)$, given sufficiently large sample size, is obtained by minimizing the weighted loss function trained on labeled data from the source domain and weighted by the ratio $w(x) = \frac{P_T(x)}{P_S(x)}$: 
%
$$
\argmin_{\beta} \sum_i -w(x_i) \log P_S(y_i | x_i, \beta)
$$

The use of this weighting ratio, $w(x)$ (referred to as the \textit{importance}) has led to many other importance estimation techniques to deal with covariate shift, including estimators that don't require the calculation of estimates of the marginal densities which is prohibitive in high-dimensional spaces or with limited data (\cite{foo}). 

Thus, the covariate shift problem is relatively well solved. The covariate shift assumption is equivalent to that of all the included covariates, $X$, being super-exogenous (as in \cite{Engle1983}) or to them making up the autonomous ``super-structure'' in the terms of Frisch. 

Allow $\mathcal{X} = \{ \mathcal{W}, \mathcal{Z}, \Lambda \}$, where $\Lambda = \{\lambda_1,\dots,\lambda_M\}$ consists of all unobserved variables for which $P_S(\lambda_i) \neq P_T(\lambda_i)$. 

Further, allow for a feature representation function, $g: \mathcal{Z} \rightarrow \mathbb{R}^K$ 

It should be clear that the covariate shift assumption for observed variables amounts to: 
% %
% $$
% P_j(Y | W, Z, \Lambda) = P_j(Y | W, Z)
% $$

% For $j \in \{S, T\}$.

\section{ Experimental Economics Formulation as Domain Adaptation }

Banerjee and Duflo \parencite*{Banerjee2014} discuss concerns to experimental validity and generalization. As a primary tool to address external validity of RCTs, they emphasize the need for replication studies. Indeed, they posit what can be thought of as asymptotic theory of domain adaptation for RCTs:

\begin{displayquote}
``If we were prepared to carry out enough experiments in varied enough locations, we could learn as much as we want to know about the distribution of the treatment effects across sites conditional on any given set of covariates.''
\end{displayquote}

Asymptotic theory can be comforting, it's nice to know that the road we are on leads somewhere. However, the key term ``any given set of covariates'' must be restricted for this to be actionable in the finite lifespan of our species. The definition of those covariates comes down to defining the uniformity of nature assumption posed by Hume. If the covariate set is infinite, then we assume nothing regarding the nature of uniformity, and induction is impossible.

How, then, do we define those covariates? Which parts of nature must be uniform and which parts are allowed to change? Our formulation allows us to 


This will answer the following practical question for any policy maker attempting to learn from previous studies: how many studies are enough studies? And if the answer to that question depends on where one intends to implement a new policy, then how can one relate the target context to the context of the studies? 

Hotz et al \parencite*{Hotz2005} provide one of the only canonical models in the econometrics literature for extrapolating from a source context with experimental data and measuring predictions in a target context where such data is not available. Their technical methodology follows \cite{Abadie2006}, using matching with replacement from the target to the source domain to create a bootstrapped dataset on which to run a regression. This can be seen as a reinvention of importance estimation (\cite{Shimodaira2000, Suigyama2008}). The implication of this choice of techniques is the assumption of covariate shift. The failure of their technique to work in certain contexts implies a violation of the covariate shift assumptions. 

\cite{Gechter2015} builds on the technique of Hotz to create bounds for prediction in a new domain, using the difference between outcomes for the control groups $P_S(Y|W_0, Z)$ and  $P_T(W_0, Z)$ to create the bounds. 

These methodologies are lacking two ingredients 

A) they do not incorporate a methodology for discovering heterogeneous treatment effects that can be used to re-marginalize the average treatment effects given a different covariate distribution in the target population and B) they do not provide any formal framework for model selection that determines which covariates to condition on and which to marginalize out to enable a transportable prediction.





% TODO

% covariate shift = Y|X independent of \Lambda, where \Lambda includes omitted variables for which P_T(\Lambda) \neg P_S(\Lambda)

% When would this fail? 1. if there is an omitted variable that causes y or 2. when X includes a variable that is caused by an variable that causes y and the variable has the same marginal distribution but the conditional on the included variable has changed from domain to domain. 

% it should be made clear this is not the unconfoundedness assumption of the rubin causal model. 

% How can one determine that such variables do not exist? One could test for the invariance of the conditional directly, the idea of rojas-carulla. 

% Formulating as decision problem / treatment effect can allow us to reduce the number of variables we care about, only those that have interactive effects. This is not done in Hotz! This is a relaxation of the requirements!

% Doing that allows us to focus on the X that have interactive effects with W/Y, and look for an invariant conditional across those X. Excluding variables implies the relationship problem #2. Lack of invariance implies problem #1. 

% Problem #1 can be solved with the creation or addition of new variables that proxy the missing variable. Latent feature space, ala Heinze-deml2017


% Given a set of interacting covariates, $\rho \subset \mathcal{X}$, one can determine that covariate shift holds when $P_S(\tau | g(\rho)) = P_T(\tau | g(\rho)$, where $g(\cdot)$ is a feature representation function that removes or transforms variables in $\rho$ to obtain an invariant conditional distribution. 

% The development of this feature representation function, $g\cdot)$, that maintains conditional invariance, is the subject of a new line of research in domain adaptation. \cite{Rojas-carulla2018} develops a feature selection algorithm whereby a subset of the features is selected subject to the constrain of independence between the empirical distributions $P_j(Y|g(X))$ be independent of index $j$, where index $j$ is the index of the source domain on which the estimate was made. \cite{Heinze-deml2017} creates a latent representation function $g(\cdot)$ in the form of a convolutional neural network for image processing and adds a regularization term to penalize the variance in $P(Y | g(X))$ across different "style features" (the features which change across domains). 
 



% \cite{Peters2015} develop a technique where, given an outcome variable $Y$, and given the assumption that there exists a conditional distribution of that outcome variable that is invariant across domains ($P(Y | X^*, \lambda) = P(Y | X^*)$) for some set of covariates $X^* \in X$, where $X$ is a set of potential covariates that have been measured, one can discover the subset $X^*$ by exploiting the invariance of the conditional distribution. They do this by representing the conditional distribution by a function $g(X)$, parameterizing the function, and looking for a set of invariant parameters across the domains.


\section{ Current State of the Art in Dealing with External Validity }



I will argue that frameworks for discovering heterogeneous treatment effects, such as causal trees (\cite{Athey2016}), must form a part of any methodology that hopes to transfer treatment effects from one domain to another. Following that, I will show that ideas of conditional invariance that can be traced back to mid-century econometricians can motivate the model selection process and that such motivation exists in some recent research in the field of domain adaptation of machine learning (\cite{Rojas-carulla2018}).


\cite{Shadish2002} lay out, along with their taxonomy of validities, a taxonomy of ``threats'' to each type of validity. While not a formal framework, per say, by creating the taxonomy they invite researchers to address each threat in turn. If a researcher is able to argue that all the threats to external validity have been addressed and protected against, then one might consider the work of generalizing to be finished. In a sense, they provide a checkbox of things that are worth worrying about. Unfortunately, echoing again Manski's (\parencite*{Manski2008} concern, researchers in applied economics have not systematically adopted a system of discussing threats to external validity.

GRADE is an example of this type of system applied in healthcare recommendations. It is a system, formalized to bring together expert opinion. It is not a statistical framework that starts from the original data, it starts from PDFs, read and interpreted by humans, and lets them give grades to the evidence based on stated criteria. 

It provides policy recommendation along the guidelines of patient age, setting, and intervention types. 

What is ``high and middle- income country'' -- what happens with countries on the border? this discretization is clearly problematic. But it's a start. 


Tamil Nadu / Bangladesh

Hock/Imbens - They conduct a test to see whether Y|X is consistent across locations, but this does not say anything about T|X. Indeed, the Y|X they test is linear, thus by assumption all of those variables fall out of the treatment effect and the identification of those variables is not actually important... 

However, if Y|X is consistent in the control, they show that it is also consistent in the treatment, which implies one can compute T|X. By using matching with replacement, they are essentially reweighting the errors by the density ratio of the covariates (show this relation!). 

However if Y|X is not consistent, that shows some other latent variable that is effecting Y. This does not mean the latent variable will effect the treatment, however, it might be additive (as was the assumption with all the other variables!). 

They seem to have found (through some ad-hoc method) that there is treatment effect heterogeneity among those who have previously worked and those you had not previously worked. Thus, they do the entire analysis separately for both of these groups. 





Athey and Imbens \parencite*{Athey2017} similarly reflect on the recent concerns over external validity quoted previously by the likes of Manski, Deaton, and Heckman. In an article aptly titled \textit{The State of Applied Econometrics: Causality and Policy Evaluation}, they lay out three main recent advances in addressing external validity.

The first advance is that of addressing the concerns about the Local Average Treatment Effect (LATE) measured by instrumental variables (IV) (...). The concerns relate to the generalization of the local effect caused by the variation in the instrument, to the global effect on the entire population (who were potentially not touched by the variation in the instrument in the study).

The second advance is that of addressing concerns regarding the local nature of regression discontinuity designs (...). These techniques all involve various methods to test whether the effect is only present locally at the discontinuity, or whether the effect is likely to extend to the rest of the sample.

Both of these techniques can be thought of addressing ``statistical validity'' in the validity taxonomy of Shadish, Cook, and Campbell, that of drawing inferences to the population from the sample. They do not address construct or external validity, they do not provide any information to the policy maker who is considering making a decision in another context.

The third advance is more salient to the major thrust of external validity: that of combining observational and experimental results.




% Much of the recent econometric literature on external validity focuses on the generalization from local effects to global effects, or from compliers to non-compliers, within the population studied. (EXAMPLES).

% There is, however, a paucity of literature that discusses external validity in the full sense implied by the definitions stated above, that is, valid in another population at another place and time.



Banjerjee and Snowberg \parencite*{Snowberg2016} create a formal system of ``speculation'' to create ``falsifiable claims'' of research studies as an attempt to codify the process of generalization and external validity that can be attached to any empirical study from the counterfactual paradigm. The falsifiable claims would presumably be used by other researchers to test the assumptions necessary for external validity of the original findings to hold true.

(TODO: something on meta-analysis methods -- Meager)

Nancy Cartwright and Jeremy Hardie \parencite*{Cartwright2013} lay out an intuitive, qualitative method for predicting the treatment effect of a policy in your context, given evidence from other contexts. 


% The most obvious alternative to the treatment effect approach, which addresses the problem of external validity, is that of structural economics (\cite{Heckman2008}).

% (TODO: some shortcoming of structural models).

% It is worth reviewing the history of structure in economics and seeing how they thought about the problem of external validity.

% Heckman (2008) explains that RCTs solve internal validity, while the problem that ``economic policy analysts have to solve daily'' involves more, and not just the generalization of previous experiments but also the prediction of the effects of policies ``never historically experienced''.

% Heckman (2008)...

% (TODO: some published criticism of the assumptions of structural models. Relate it to uniformity of nature and size of modifications -- very large!)

% Others have proposed methods for ``augmenting'' counterfactual anaylisis to take external validity into account.




\section{Invariance in Domain Adaptation}



% \cite{Rojas-carulla2018} propose a similar idea, but modify it in two ways which we will follow:

% \begin{enumerate}
% \item They use the formulation of Domain Adaptation to frame the problem and measure the effectiveness of their technique.
% \item They consider the invariance of the conditional to it's domain through an independence test between the conditional distribution and the index label of the domain: $P(Y | X^*, \lambda) = P(Y | X^*)$. Where $\lambda \in \{1,2,\ldots,S \}$ for $S$ source domains.
% \end{enumerate}


\section{Conclusions and Future Research}





Nancy Carwright \parencite*{Cartwright2016} presents an example of a policy decision by policy makers in New York City to implement a new program called Opportunity NYC. The program was modelled after a program in Mexico, Opportunidades, which had proved good results in reducing poverty there. Opportunity NYC was implemented in 2007 and shut down in 2010 due its failure to produce the desired effects. 

What went wrong? Should the policy makers in New York simply have known that Mexico is clearly different and no program implemented there should be expected to work in New York? Did the writers of the Opportunidades study have an obligation to investigate and consider the threats to external validity that such a study might have been in danger of? 

The purpose of this article was to argue that each of these cases is wrong: 

\begin{enumerate}
\item Generalization, causal mechanisms, structure, support factors, etc. are all relative to a set of changes to which they must be invariant. One cannot answer the question about external validity without information about the differences between the source and target contexts. 

\item The difference between New York and Mexico must be measured over a well-defined covariate space in order to make sense of the question of whether it is ``too different.'' A core part of applied economics research must, therefore, consist of determining, for a given intervention, what part of nature must be uniform, what makes up the super-structure, what are the support factors that will change, or similarly, what are the invariant mechanisms. New research from machine learning has shown that the discovery of invariant mechanisms, given muti-domain datasets, is feasible and allows for effective predictions in new domains. Thus, while the data requirement has gone up (more than one study is required to make a prediction in a new context without many further assumptions), the ability to give a formal answer to the question of whether it will probably work in New York or whether one has no idea is within our reach and should be pursued. 
\end{enumerate}










% \section{Problem Formulation}

% We consider the utility of individual $i$ given as $ u(\pi, X_i)$, where $X_i$ is a set of covariates, potentially unique to the individual, and $\pi$ some policy under question. The utility function is considered common across all individuals, but arbitrarily parameterized by individual covariates $X_i$, thus this assumption is non-restrictive.

% The social planner's problem consists of finding the ``best'' policy, where best can be defined in a number of different ways: greatest average utility, paretto optimal, least inequality, etc:
% %
% $$
% \pi^* = h(\bm{\pi}, X)
% $$

% Where $h(\cdot)$ represents a function that determines the best policy given a set of policies $\bm{\pi}$ and the population covariates $X$. We will make two assumptions to make this problem tractable for our framework:

% \begin{enumerate}
% \item We replace the utility function, $u(\cdot)$, with a surrogate outcome $Y(\cdot)$ of interest to the social planner. The surrogate is considered linear separable into two parts:
% %
% $$
% Y(\pi, X) = Y_1(\pi, Z) + Y_2(W)
% $$

% Where $Z \cup W = X$.

% \item In comparing two policies, a policy maker can pick one based solely on the individual differences in outcomes between the two policies:
% %
% $$
% \pi^* = h(Y(\pi_1, Z_1) - Y(\pi_0, Z_1),...,Y(\pi_1, Z_N) - Y(\pi_0, Z_N))
% $$
% When $\bm{\pi} = \{\pi_0, \pi_1\}$.
% \end{enumerate}

% The first assumption is non-restrictive, as $W$ can be null. The second assumption is a restriction on the type of policy objectives the social planner can pursue, however many naturally desireable objectives can still be pursued in realistic scenarios\footnote{The decision maker can directly pursue the greatest average utility and objectives based on paretto optimality, but cannot directly pursue least inequality, as an example. In realistic scenarios, however, reduction of inequality can be pursued via the maximization of utility (likely replaced with income) for those who currently are in the lowest x\% and minimization of income for those who are currently in the highest y\%, assuming that the class of policies, $\bm{\pi}$ is reasonable enough so that one doesn't accidentally flip the relationship and maintain the inequality!}.

% Finally, we will restrict our policy space, $\bm{\pi}$, to one of finite policies\footnote{Considering continuous policy spaces should be considered of great interest. This restriction is done to simplify the problem here, not because it is necessarily desireable. Embedding policies into continuous spaces is, additionally, not a solved problem and is a natural prerequisite to such a formulation being useful in practise.} and, without loss of generalization, consider the situation of two policies, $\{\pi_0, \pi_1\}$, where $\pi_0$ can be thought of as the null policy, or control, and $\pi_1$ the treatment under question. This allows us to rewrite:
% %
% $$
% \pi^* = h(\tau_1,...,\tau_n)
% $$

% Where $\tau_i \coloneqq Y(\pi_1, Z_{i}) - Y(\pi_0, Z_{i})$, the individual treatment effect of the policy. At the point in time in which the social planner is deciding on a policy, $\tau_i$ is unknown and can be formulated as a random variable.

% Consider $\hat{P}(\tau_i | X, \lambda = \lambda_T)$ to be a predictive probability distribution of the treatment effect of the policy on individual $i$ in a target context (domain) described by $\lambda = \lambda_T$\footnote{The parameter $\lambda$ is the same as $\lambda_2$ in Engle's formulation. We drop the subscript for ease of notation here.}. The planner knows the covariates for each individual, $X_i$, and estimates the predictive probability distribution via a prediction function, $f(\cdot)$, trained on a set of 4-tuples, $(Y_i, X_i, \pi_i, \lambda_i)$ where $\lambda_i \in \Lambda_S$ and $\Lambda_S$ consists of a series of domains in which the policy maker has labeled data and $\lambda_T \not \in \Lambda_S$. Thus, the decision function that picks the ``best'' policy must now accept probability distributions:
% %
% $$
% \pi^* = h(\hat{P}(\tau_1 | X, \lambda_T),...,\hat{P}(\tau_n | X, \lambda_T))
% $$

% The treatment effect can be reasonably replaced by its expectation, $ \mathbb{E} [ \tau_i | X, \lambda_T ] $, if the policy objective is maximizing average (or total) welfare. If the policy objective incorporates any sort of aversion to negative outcomes (``let's not completely ruin any of our citizens' lives''), risk aversion, or concerns about paretto efficiency, then the expectation of this random variable will not be sufficient information to pick the best policy $\pi^*$.

% We will assume that every policy object can be translated into a loss function that aligns with the decision strategy $h(\cdot)$:
% %
% $$
% \ell(\tau, \hat{P}(\tau | X, \lambda_T))
% $$

% Unfortunately, this loss function is technically impossible to calculate, even after the fact, due to Rubin's ``fundamental problem of causal inference'', the fact that $\tau_i$ is unobserved. While the formulation and minimization such a specific loss function would be desirable in the case of different policies, I will, for this article, pose a general formulation that will asymptotically minimize any reasonable loss function: the existence of a consistent estimator for $P(\tau | X)$ such that
% %
% $$
% \hat{P}(\tau | X, \lambda \in \Lambda_S) \rightarrow P(\tau | X, \lambda = \lambda_T)
% $$

% Such an estimator can be created if we have the following two conditions:

% \begin{enumerate}
% \item There exists a consistent estimator $\hat{P}(\tau | X, \lambda = \lambda_S) \rightarrow P(\tau | X, \lambda = \lambda_S)$ for any single domain defined by $\lambda_S$.
% \item $P(\tau | X, \lambda_S) = P(\tau | X) \ \forall \ X, \ P(X | \lambda_T) > 0$. The conditional is invariant across the differences that exist between the source domains and the target domain, within the support of the target domain.
% \end{enumerate}

% It should be noted that this general framework works for any predictive output that might be used in the policy choice. Thus, if the policy choice relies only on the average treatment effect, $h(\mathbb{E} \big[ \tau | X, \lambda_T \big])$, then one only needs a consistent estimator for $\mathbb{E} \big[ \tau | X, \lambda_S \big]$ and that the estimator is invariant across domains, $\mathbb{E} \big[ \tau | X, \lambda_S \big] = \mathbb{E} \big[ \tau | X \big]$, in order to have a consistent estimator that will asymptotically minimize the loss function related that policy choice.


% \section{Quantile Treatment Effects}

% rank preservation, if not, it might in and of itself be of interest to the policy maker, see blah for discussion

% We will apply this observation, that a QTE defined in this way is identical to the individual QTE with an assumption of rank preservation, and calculate the latter directly. Within each leaf, we will preserve the rank of the individuals and calculate the difference. Extra individuals in either group are compared to their closest match from the other group.



% \section{Picking causal covariates vs. creating them}

% Often the direct causes of the outcome in consideration are not observed. They are, as such, latent variables. These latent variables might be proxied by an observable (cause) or be caused by an observable. The two factors have very different effects:

% $P(X |Z)$ should be invariant in the case where the latent variable causes its observable proxy, thus, for the transformation to go from $\hat{Z} = f(X)$, it is $f^{-1}$ that must be invariant across domains, requiring a generative model of X from the latent variables to be learned.

% On the contrary, if $P(Z | X)$ is invariant, then a model which generates the latent variables which in turn cause the effect, $Y$, must be invariant. It should be noted that $P(Z)$ is not needed to be constant across domains, as long as the two conditionals can be recoverd.



% \section{Forests as Nonparametric Estimator of the Conditional Treatment Effect Distribution}

% Quantile treatment effect with the assumption of rank persistence.

% It follows that any consistent estimator of ATE is an consistent estimator of quantiles and the distribution as well.

% Causal forests are a consistent estimator of ATE (\cite{})

% Random forests are a consistent estimator of the full distribution (\cite{})


% \section{Distributional Invariance Across Domains}



% heinz-deml proposes a similar solution to the problem of domain adaptation in image recognition. Using a series of images in which the same individual, with the same causal characteristics, is captured across multiple domains where orthogonal changes take place. A regularization term is added to the optimization problem of the neural network trained on the source domains. The term penalizes the conditional variance...

% Much of the domain adaptation literature from machine learning focuses on the case where covariate shift holds. Covariate shift describes the situation where $P(Y|X)$ is assumed to be consistent and $P(X)$ is changing across domains. As we are in the situation of discovering the $x \in X$ which is causal for $Y$, we cannot simply make that assumption. Many of the techniques in domain adaptation, however, are developed for situations in which the model is learned on a latent space, thus the reweighting of $X$ consists of a transformation and checking of distributional differences between source domains. One technique for minimizing distances between distributions is that of minimizing the Wasserstein distance. This distance measure has grown in popularity recently due to it's advantageous qualities for optimization: it maintains a gradient even when distributions lack mutual positive support. Thus, the gradient is computable even when distributions are very different from one another.

% These two ideas can easily be combined, that of Wasserstein distance minimization for enforcing distributional similarity and that of regularization of the conditional differences to enforce domain-invariant conditionals.

% Thus, the optimization problem will contain a regularization term:

% $$
% R(\cdot) = \sum_{i,j \in S} W(P(\tau | X_i, \lambda_i),P(\tau | X_i, \lambda_j))
% $$

% Which sums the pairwise Wasserstein distance, $W(\cdot)$, of the conditional treatment effect distribution across different domains, $S$.


% Each split seeks to minimize the variance of the treatment effect ,








% \section{Empirical Performance}

% Data consists of 5 recent RCT studies performed on the impact of micro-credit programs on business profit and consumption in developing countries.

% With 5 different domains, I test my method by training on 4 source domains and predicting on the 5th. Results are compared with the estimated ATE implied by the studies, as well as with a ``baseline'' prediction using the coefficient of the CATE as estimated by a linear model in a single domain, comparable to coefficients reported in the original studies.


% Rojas-Carulla's technique, by assuming linearity, sidesteps the problem of support (a linear function has full support, as one just extrapolates the line into infinity, regardless of the support of the data from which the function was learned).

% This problem of support, and the realities of non-linearities, would seem to be important to economic work, where there can be substantial changes in distributions between countries and time periods across potential interacting variables.


% Despite the restrictions it places on policy objectives, we will restrict our scope, for the current article, to that of averaging total welfare. It would be valuable to extend these ideas to other policy objectives and determine loss functions that apply to them.

% We define the empirical conditional average treatment effect as:
% %
% $$
% \tau(x) \coloneqq \frac{1}{N} \sum_N \mathbb{E}[ \Delta U | X]
% $$

% If the goal is to pick the correct policy, one can consider a loss from picking the incorrect policy equal to the total lost utility:
% %
% $$
% \ell = \mathbbm{1} \big[ \int \hat{\tau}(x)p(x)dx < 0 \big] \int \tau(x)p(x)dx \\
% $$



% a reasonable loss function would be to minimize the squared loss between the empirical conditional average treatment effect and the predicted:
% %
% $$
% \ell = \sum_i \bigg( \tau(X_i) - \hat{\tau}(X_i) \bigg)^2
% $$


% This creates difficulties for the estimation of any empirical distribution of treatment effects (see \cite{Imbens2004} for a review).


% Rank preservation assumption necessary to estimate the quantile treatment effect...!!! Also necessary to estimate the distribution.


% THEOREM??
% Given two domains, $S, D$ and random variables $Y,X$, if $P_S(Y | X) = P_T(Y | X)$, then there exists no other variable $Z$ such that $P_S(Z) \neq P_D(Z)$ and $P_S(Y | X) \nCI Z$ and $P_D(Y | X) \nCI Z$ and $P_S(Y | X, Z) = P_T(Y | X, Z)$.




% information theoretic -- minimize entropy while being invariant.
% MSE - minimize variance while be invariant
% ??? What else ???
%


\section{}

% The expectation of the predictive treatment distribution for a give individual might, under certain policy objectives (increasing average utility), contain all the relevant information from the predictive treatment distribution. In this case, we might want to calculate the expected treatment effect (ETE).


% The expected treatment effect (ETE) for domain $D_t$ is not the same as the average treatment effect (ATE) on domain $D_s$ unless blah blah blah.

% The expected treatment effect is decomposed into:

% $$
% ETE  = ATE + ETD(S, D)
% $$


% The invariant average treatment effect (IATE)

% The invariant treatment effect distribution (ITED)



% It was clear to these economic theorists that human nature, and the all the factors that affect the functioning of an economy, must in some ways be uniform, but in many other ways changeable. Economic theory, any equations or models intended to represent invariant laws, must endogenous the changeable nature of human behavior, institutions, and technology (\cite{Marshack1950}).



% So what does a formal system that takes into account external validity look like?

% It must, in some way, deal with the assumption that Hume terms ``the uniformity of nature.'' Trivially, nature will not be uniform in every way, and clearly, we need it to be uniform in some way (Goodman   ). It is thus of interest to us to acknowledge that assumption and seek out frameworks that endogenize the ways in which nature is uniform and the ways in which it changes.

% One can think of a general causal law as a function. The output of the function is the output of interest. The function is parameterized by every variable that effects the output. A framework that endogenizes the process of learning the ways in which nature is uniform is nothing more than a process of finding a single function that is works consistently across time and space\footnote{This idea of an "invariant" mechanism that works across environments is one fundamental definition of causality that has been much discussed in the literature (...).}. If we think of this function nonparametrically, then finding this function consists of two steps: 1) deciding which variables should parameterize the function and 2) collecting output data across the joint support of all those input variables.

% Collecting data across the entire joint support of the inputs can potentially be extremely difficult in the context of the social sciences. If these input covariates are elements that change slowly over time and space with a high level of autocorrelation (such as the majority of traits that can be considered ``cultural'' or ``social'' in nature), then it will take a very long time, potentially infinite, until we have collected enough data and ``converged'' on the full joint distribution that defines our general causal law \footnote{A useful comparison is to think of Markov chain monte carlo. We might, as a society, be forced to slowly explore locally the evolution of cultural traits that have a large potential impact on outcomes of interest, and if there is some randomness in the system, then this path-dependent process will eventually cover the entire support of the distribution, but it is not clear that it will happen before the sun implodes.}.

% This is not to say that the recover of general causal laws is not of interest or possible in economics. It is definitely of interest and may be possible. Treating the problem as one of nonparametric statistical estimation might require an overwhelming amount of data, but attacking the problem with a hand-written parametric model doesn't require any data whatsoever, supposing one has the right theory.



% While RCTs, the counterfactual model, and the internal validity they aim to ensure are not sufficient for the process of scientific inference, that does not in any way detract from them being useful and even necessary. \cite{Deaton2018} provide a strong overview of the many ways in which RCTs are valuable even without any claims of external validity.

% While they echo the concerns of \cite{Heckman2007}, that RCTs and, more generally, the counterfactual approach does \textit{not} easily lead to invariant (autonomous) relationships, they also stress that RCTs can be useful even if they do not generalize to other populations. ``Demanding ‘external validity’ is unhelpful because it expects too much of an RCT while undervaluing its potential contribution.''





% \section{From Particulars to Particulars}

% It should be clear that, while having general causal laws, as parameterized functions, for every outcome of interest would be extremely useful, it is also extremely difficult. While much of theoretical economics often consists of the creation of such functions, they are rarely seriously defended as general causal laws to be used in the way we use Newton's laws in Physics: functions that predict with such accuracy that we regularly trust our lives to them.

% There are two useful simplifications we can make in this process of inference to make it more feasible.

% The first is to focus, not on all inputs that determine an outcome, but on one input in particular. We can think of this input as a treatment or a policy. Our function will thus not seek to predict the output given all inputs, but rather, seek to predict the change in the output given our treatment, ceteris paribus. This ceteris paribus clause allows us to ignore all additively separable causes of our effect of interest. Our function will thus be parameterized by our treatment along with a set of ``interacting covariates'' \footnote{These interacting covariates are nothing more than the support factors in the language of Nancy Cartwright or connecting-principles in the language of Nelson Goodman.} and the output of the function will be a level shift in our output variable of interest.

% It should be clear that this first simplification is not in any way novel, it also happens when we move from structural to reduced-form models in economics.


% \section{Autonomy}


% Here we can realize the difference between what is possible to falsify, and what is not.

% There might be a difference between a relation that is autonomous and one that is persistent and the relation might be that the autonomous relation is invariant to hypothetical variations never-before-seen while the persistent one can only be said to be invariant to variations for which we have data.

% However, the former cannot actually be falsified.

% That being said, it is important to keep the difference in mind. Bertrand Russel's chicken had discovered a persistent relationship but not one that was autonomous to a hypothetical change in the behavior of its farmer.

% I will focus on the case where, when we want to discuss claims that are falsifiable with current data on the world, persistent becomes the more actionable version of autonomous.


% It should be clear in the application of this proposed method that one must make an argument, via an appeal to common sense and behavioral theory, that for any latent variables that are: A) likely to have an interative effect and B) likely to vary in its distribution from domain to domain, the following will also hold: C) the variable has varied in its distribution across the various source domains.


% \section{Structure is Relative}

% >>> This can potentially be used -- instead of forcing variable sets with invariant properties, the effect of potential latent variables can be bound by the change in output distribution!!



\printbibliography

\end{document}
