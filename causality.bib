@article{Kocaoglu2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1709.02023v2},
author = {Kocaoglu, Murat and Snyder, Christopher and Dimakis, Alexandros G},
eprint = {arXiv:1709.02023v2},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/1709.02023.pdf:pdf},
pages = {1--37},
title = {{CausalGAN : Learning Causal Implicit Generative Models with Adversarial Training}},
year = {2017}
}
@article{Magliacane2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1707.06422v3},
author = {Magliacane, Sara and Versteeg, Philip and Claassen, Tom and Bongers, Stephan},
eprint = {arXiv:1707.06422v3},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/1707.06422.pdf:pdf},
number = {Nips},
title = {{Predict Invariant Conditional Distributions}},
year = {2018}
}
@article{Heinze-deml2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1710.11469v5},
author = {Heinze-deml, Christina and Meinshausen, Nicolai},
eprint = {arXiv:1710.11469v5},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/1710.11469.pdf:pdf},
keywords = {anti-,causal models,causal prediction,dataset shift,distributional robustness,domain shift,image classification},
number = {2009},
title = {{Conditional Variance Penalties and Domain Shift Robustness}},
year = {2017}
}
@article{Hoover2006,
abstract = {An entry for the New Palgrave Dictionary of Economics. Traces the history of causality in economics and econometrics since David Hume. Examines the main modern approaches to causal inference.},
author = {Hoover, Kevin D.},
doi = {10.2139/ssrn.930739},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/SSRN-id930739.pdf:pdf},
isbn = {978-0-333-78676-5},
issn = {1556-5068},
journal = {Ssrn},
keywords = {B16,B23,C10,Cowles Commission,Granger,Granger causality,Haavelmo,Hume,Jevons,Mill,Pearl,Simon,Spirtes,Zellner,causal inference,causality,econometrics,graph-theory,identification,structural vector autoregressions},
number = {June},
pmid = {24744601},
title = {{Causality in Economics and Econometrics}},
year = {2006}
}
@book{Keuzenkamp2000,
author = {Keuzenkamp, Hugo A.},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/Hugo A. Keuzenkamp - Probability, econometrics and truth{\_} the methodology of econometrics-Cambridge University Press (2000).pdf:pdf},
title = {{- Probability, econometrics and truth{\_} the methodology of econometrics (2000, Cambridge University Press)-1}},
year = {2000}
}
@article{Speed1992,
author = {Speed, T. P.},
doi = {10.1007/978-1-4612-4380-9_7},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/978-1-4612-4380-9{\_}7.pdf:pdf},
number = {1926},
pages = {71--81},
title = {{Introduction to Fisher (1926) The Arrangement of Field Experiments}},
year = {1992}
}
@book{Pearl2000,
abstract = {Chapter headings: Introduction to Probabilities, Graphs, and Causal Models; A Theory of Inferred Causation; Causal Diagrams and the Identification of Causal Effects; Actions, Plans, and Direct Effects; Causality and Structural Models in Social Science and Economics; Simpson's Paradoxon, Confounding, and Collapsibility; The Logic of Structure-Based Counterfactuals; Imperfect Experiments: Bounding Effects and Counterfactuals; Probability of Causation: Interpretation and Identification; The Actual Cause; Epilogue: The Art and Science of Cause and Effect},
author = {Pearl, Judea},
doi = {citeulike-article-id:3888442},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/Judea Pearl - Causality{\_} Models, Reasoning, and Inference (2000, Cambridge University Press).pdf:pdf},
isbn = {0521773628},
pages = {386},
title = {{Causality Second Edition}},
year = {2000}
}
@article{Sgouritsa2015,
abstract = {We address the problem of causal discov-ery in the two-variable case given a sample from their joint distribution. The proposed method is based on a known assumption that, if X → Y (X causes Y), the marginal distri-bution of the cause, P (X), contains no in-formation about the conditional distribution P (Y |X). Consequently, estimating P (Y |X) from P (X) should not be possible. However, estimating P (X|Y) based on P (Y) may be possible. This paper employs this asymmetry to pro-pose CURE, a causal discovery method which decides upon the causal direction by com-paring the accuracy of the estimations of P (Y |X) and P (X|Y). To this end, we pro-pose a method for estimating a conditional from samples of the corresponding marginal, which we call unsupervised inverse GP re-gression. We evaluate CURE on synthetic and real data. On the latter, our method outperforms existing causal inference meth-ods.},
author = {Sgouritsa, Eleni and Janzing, Dominik and Hennig, Philipp and Sch{\"{o}}lkopf, Bernhard},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/sgouritsa15.pdf:pdf},
issn = {15337928},
journal = {Artificial Intelligence and Statistics},
pages = {847----855},
title = {{Inference of Cause and Effect with Unsupervised Inverse Regression}},
volume = {38},
year = {2015}
}
@techreport{Morgan,
abstract = {In this completely revised and expanded second edition of Counterfactuals and Causal Inference, the essential features of the counterfactual approach to observational data analysis are presented with examples from the social, demographic, and health sciences. Alternative estimation techniques are first introduced using both the potential outcome model and causal graphs; after which conditioning techniques, such as matching and regression, are presented from a potential outcomes perspective. For research scenarios in which important determinants of causal exposure are unobserved, alternative techniques, such as instrumental variable estimators, longitudinal methods, and estimation via causal mechanisms, are then presented. The importance of causal effect heterogeneity is stressed throughout the book, and the need for deep causal explanation via mechanisms is discussed. Sciences and the director of the Center for the Study of Inequality at Cornell University. His current areas of interest include social stratification, the sociology of education, and quantitative methodology. He has published On the Edge of Commitment: Educational Attainment and Race in the United States (2005) and, as editor, the Handbook of Causal Analysis for Social Research (2013). Christopher Winship is the Diker-Tishman Professor of Sociology and a member of the senior faculty of Harvard's Kennedy School of Government. Prior to coming to Harvard in 1992, he was Professor of Sociology and Statistics and by courtesy Economics at Northwestern University. His research focuses on statistical models for causal inference, most recently mechanisms and endogenous selection; how black clergy in Boston have worked with police to reduce youth violence; the effects of education on mental ability; pragmatism as the basis for a theory of action; the implications of advances in cognitive psychology for sociology; and sociological approaches to how individuals understand justice. Since 1995 he has been editor of Sociological Methods and Research.},
author = {Morgan, Stephen L.},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/Counterfactuals-and-Causal-Inference-Models-and-Principles-for-Social-Research.pdf:pdf},
title = {{COUNTERFACTUALS AND CAUSAL INFERENCE Second Edition}}
}
@article{Sch2012,
author = {Sch, Bernhard and Janzing, Dominik and Peters, Jonas and Sgouritsa, Eleni and Zhang, Kun},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/625.pdf:pdf},
title = {{On Causal and Anticausal Learning}},
year = {2012}
}
@article{Janzing2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1402.2499v1},
author = {Janzing, Dominik and Shajarisales, Naji},
doi = {10.1007/978-3-319-21852-6},
eprint = {arXiv:1402.2499v1},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/1402.2499v1.pdf:pdf},
isbn = {9783319218526},
number = {August},
title = {{Justifying Information-Geometric Causal Inference}},
year = {2015}
}
@article{Heinze-Deml2018,
abstract = {Graphical models can represent a multivariate distribution in a convenient and accessible form as a graph. Causal models can be viewed as a special class of graphical models that represent not only the distribution of the observed system but also the distributions under external interventions. They hence enable predictions under hypothetical interventions, which is important for decision making. The challenging task of learning causal models from data always relies on some underlying assumptions. We discuss several recently proposed structure learning algorithms and their assumptions, and we compare their empirical performance under various scenarios.},
archivePrefix = {arXiv},
arxivId = {arXiv:1706.09141v1},
author = {Heinze-Deml, Christina and Maathuis, Marloes H. and Meinshausen, Nicolai},
doi = {10.1146/annurev-statistics-031017-100630},
eprint = {arXiv:1706.09141v1},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/1706.09141.pdf:pdf},
journal = {Ssrn},
title = {{Causal Structure Learning}},
year = {2018}
}
@book{VanderLaan2018,
author = {van der Laan, Mark J. and Rose, Sherri},
doi = {10.1007/978-3-319-65304-4},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/Targeted-Learning-in-Data-Science.pdf:pdf},
isbn = {978-3-319-65303-7},
title = {{Targeted Learning in Data Science}},
url = {http://link.springer.com/10.1007/978-3-319-65304-4},
year = {2018}
}
@article{Bareinboim2014,
author = {Bareinboim, Elias},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/r425.pdf:pdf},
keywords = {Reasoning under Uncertainty},
number = {July},
pages = {339--341},
title = {{Recovering from Selection Bias in Causal and Statistical Inference}},
year = {2014}
}
@book{Spirtes,
author = {Spirtes, Peter and Glymour, Clark},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/(Adaptive Computation and Machine Learning) Peter Spirtes, Clark Glymour, Richard Scheines - Causation, Prediction, and Search, Second Edition -The MIT Press (2001).pdf:pdf},
isbn = {0262194406},
title = {{Causation, Prediction, and Search}}
}
@article{Journal2016,
author = {Journal, Source and Statistical, Royal and Series, Society},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/2344179.pdf:pdf},
number = {2},
pages = {234--266},
title = {{The Planning of Observational Studies of Human Populations Author ( s ): W . G . Cochran and S . Paul Chambers Published by : Wiley for the Royal Statistical Society Stable URL : http://www.jstor.org/stable/2344179 Accessed : 31-07-2016 15 : 51 UTC Your u}},
volume = {128},
year = {2016}
}
@article{Suter,
archivePrefix = {arXiv},
arxivId = {arXiv:1811.00007v1},
author = {Suter, Raphael and Bauer, Stefan and Sch, Bernhard},
eprint = {arXiv:1811.00007v1},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/1811.00007.pdf:pdf},
keywords = {causal-,disentanglement,representation learning,variational auto-encoders},
pages = {1--34},
title = {{Interventional Robustness of Deep Latent Variable Models}}
}
@article{Shah2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1804.07203v2},
author = {Shah, Rajen D},
eprint = {arXiv:1804.07203v2},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/1804.07203.pdf:pdf},
number = {July 2017},
title = {{The Hardness of Conditional Independence Testing and the Generalised Covariance Measure}},
year = {2018}
}
@book{VanderWeele2015,
abstract = {The book provides an accessible but comprehensive overview of methods for mediation and interaction. There has been considerable and rapid methodological development on mediation and moderation/interaction analysis within the causal-inference literature over the last ten years. Much of this material appears in a variety of specialized journals, and some of the papers are quite technical. There has also been considerable interest in these developments from empirical researchers in the social and biomedical sciences. However, much of the material is not currently in a format that is accessible t},
author = {VanderWeele, Tyler},
doi = {http://dx.doi.org/10.1177/0146167212443896},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/Explanation-in-Causal-Inference-Methods-for-Mediation-and-Interaction.pdf:pdf},
isbn = {9780199325870},
issn = {0146-1672, 0146-1672},
pages = {729},
pmid = {29982528},
title = {{Explanation in Causal Inference}},
url = {http://gbv.eblib.com/patron/FullRecord.aspx?p=1920742},
year = {2015}
}
@article{Schulam2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1812.04597v2},
author = {Schulam, Peter},
eprint = {arXiv:1812.04597v2},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/1812.04597.pdf:pdf},
title = {{Preventing Failures Due to Dataset Shift: Learning Predictive Models That Transport}},
volume = {89},
year = {2019}
}
@article{Claassen,
archivePrefix = {arXiv},
arxivId = {arXiv:1611.10351v3},
author = {Mooij, Joris M and Magliacane, Sara and Claassen, Tom},
eprint = {arXiv:1611.10351v3},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/1611.10351.pdf:pdf},
keywords = {causal discovery,inter-,observational and experimental data,randomized controlled trials,structure learning,ventions},
pages = {1--37},
title = {{Joint Causal Inference from Multiple Datasets}},
year = {2016}
}
@article{Bareinboim2016a,
abstract = {We review concepts, principles, and tools that unify current approaches to causal analysis and attend to new challenges presented by big data. In particular, we address the problem of data fusion—piecing together multiple datasets collected under heterogeneous conditions (i.e., different populations, regimes, and sampling methods) to obtain valid answers to queries of interest. The availability of multiple heterogeneous datasets presents new opportunities to big data analysts, because the knowledge that can be acquired from combined data would not be possible from any individual source alone. However, the biases that emerge in heterogeneous environments require new analytical tools. Some of these biases, including confounding, sampling selection, and cross-population biases, have been addressed in isolation, largely in restricted parametric models. We here present a general, nonparametric framework for handling these biases and, ultimately, a theoretical solution to the problem of data fusion in causal inference tasks.},
author = {Bareinboim, Elias and Pearl, Judea},
doi = {10.1073/pnas.1510507113},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/7345.full (1).pdf:pdf},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
number = {27},
pages = {7345--7352},
title = {{Causal inference and the data-fusion problem}},
volume = {113},
year = {2016}
}
@book{Laan2011,
abstract = {The statistics profession is at a unique point in history. The need for valid statistical tools is greater than ever; data sets are massive, often measuring hundreds of thousands of measurements for a single subject. The field is ready to move towards clear objective benchmarks under which tools can be evaluated. Targeted learning allows (1) the full generalization and utilization of cross-validation as an estimator selection tool so that the subjective choices made by humans are now made by the machine, and (2) targeting the fitting of the probability distribution of the data toward the target parameter representing the scientific question of interest. This book is aimed at both statisticians and applied researchers interested in causal inference and general effect estimation for observational and experimental data. Part I is an accessible introduction to super learning and the targeted maximum likelihood estimator, including related concepts necessary to understand and apply these methods. Parts II-IX handle complex data structures and topics applied researchers will immediately recognize from their own research, including time-to-event outcomes, direct and indirect effects, positivity violations, case-control studies, censored data, longitudinal data, and genomic studies."Targeted Learning, by Mark J. van der Laan and Sherri Rose, fills a much needed gap in statistical and causal inference. It protects us from wasting computational, analytical, and data resources on irrelevant aspects of a problem and teaches us how to focus on what is relevant answering questions that researchers truly care about."-Judea Pearl, Computer Science Department, University of California, Los Angeles"In summary, this book should be on the shelf of every investigator who conducts observational research and randomized controlled trials. The concepts and methodology are foundational for causal inference and at the same time stay true to what the data at hand can say about the questions that motivate their collection."-Ira B. Tager, Division of Epidemiology, University of California, Berkeley},
author = {van der Laan, Mark J. and Rose, Sherri},
booktitle = {Springer Series in Statistics},
doi = {10.1007/978-1-4419-9782-1},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/Targeted-Learning-Causal-Inference-for-Observational-and-Experimental-Data-.pdf:pdf},
isbn = {1441997814},
number = {2},
pages = {626},
title = {{Targeted Learning - preface}},
volume = {27},
year = {2011}
}
@article{RubinD.B1994,
abstract = {A discussion of matching, randomization, random sampling, and other methods of controlling extraneous variation is presented. The objective is to specify the benefits of randomization in estimating causal effects of treatments. The basic conclusion is that randomization should be employed whenever possible but that the use of carefully controlled nonrandomized data to estimate causal effects is a reasonable and nec-essary procedure in many cases.},
author = {{Rubin D. B}},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/420{\_}paper{\_}Rubin74.pdf:pdf},
journal = {Journal of Educational Psychology},
number = {5},
pages = {688--701},
title = {{Estimating causal effects of treatment in randomized and nonrandomized studies}},
url = {http://www.fsb.muohio.edu/lij14/420{\_}paper{\_}Rubin74.pdf},
volume = {66},
year = {1994}
}
@article{Castelletti2018,
abstract = {A Markov equivalence class contains all the Directed Acyclic Graphs (DAGs) encoding the same conditional independencies, and is represented by a Completed Partially Directed Acyclic Graph (CPDAG), also named Essential Graph (EG). We approach the problem of model selection among noncausal sparse Gaussian DAGs by directly scoring EGs, using an objective Bayes method. Specifically , we construct objective priors for model selection based on the Fractional Bayes Factor, leading to a closed form expression for the marginal likelihood of an EG. Next we propose a Markov Chain Monte Carlo (MCMC) strategy to explore the space of EGs using sparsity constraints, and illustrate the performance of our method on simulation studies, as well as on a real dataset. Our method provides a coherent quantification of inferential uncertainty, requires minimal prior specification , and shows to be competitive in learning the structure of the data-generating EG when compared to alternative state-of-the-art algorithms.},
author = {Castelletti, Federico and Consonni, Guido and Vedova, Marco L.Della and Peluso, Stefano},
doi = {10.1214/18-BA1101},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/euclid.ba.1521079250.pdf:pdf},
issn = {19316690},
journal = {Bayesian Analysis},
keywords = {Bayesian model selection,CPDAG,Essential graph,Fractional Bayes factor,Graphical model},
number = {4},
pages = {1231--1256},
title = {{Learning Markov equivalence classes of Directed Acyclic Graphs: An objective bayes approach}},
volume = {13},
year = {2018}
}
@article{Bareinboim2016,
author = {Bareinboim, Elias and Pearl, Judea},
doi = {10.1073/pnas.1510507113},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/7345.full.pdf:pdf},
title = {{Causal inference and the data-fusion problem}},
year = {2016}
}
@book{Fisher1925,
abstract = {Contains revisions of probability formulas and treatment of correlations. Harvard Book List (edited) 1955 94 (PsycINFO Database Record (c) 2010 APA, all rights reserved)},
author = {Fisher, Ra},
booktitle = {Biological monographs and manuals},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/Statistical{\_}Methods{\_}for{\_}Research{\_}Workers.pdf:pdf},
isbn = {0050021702},
number = {V},
pages = {xv, 356 p.},
title = {{Fisher RA: Statistical methods for research workers. Edinburgh: Genesis Publishing, Oliver and Boyd;}},
url = {http://psychclassics.yorku.ca/Fisher/Methods},
year = {1925}
}
@article{Anderson2005,
abstract = {Theil, Basmann, and Sargan are often credited with the development of the two-stage least squares (TSLS) estimator of the coefficients of one structural equation in a simultaneous equations model. However, Anderson and Rubin had earlier derived the asymptotic distribution of the limited information maximum likelihood (LIML) estimator by finding the asymptotic distribution of what is essentially the TSLS estimator. {\textcopyright} 2005 Elsevier B.V. All rights reserved.},
author = {Anderson, T. W.},
doi = {10.1016/j.jeconom.2004.09.012},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/94e8b83798bf98fc30339fbf2b8b780cff9f.pdf:pdf},
issn = {03044076},
journal = {Journal of Econometrics},
keywords = {Asymptotic distributions,Limited information maximum likelihood estimator,Two-stage least squares estimator},
number = {1},
pages = {1--16},
title = {{Origins of the limited information maximum likelihood and two-stage least squares estimators}},
volume = {127},
year = {2005}
}
@article{Johansson2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1605.03661v3},
author = {Johansson, Fredrik D},
eprint = {arXiv:1605.03661v3},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/1605.03661.pdf:pdf},
title = {{Learning Representations for Counterfactual Inference}},
volume = {48},
year = {2016}
}
@article{Arjovsky2019,
abstract = {We introduce Invariant Risk Minimization (IRM), a learning paradigm to estimate invariant correlations across multiple training distributions. To achieve this goal, IRM learns a data representation such that the optimal classifier, on top of that data representation, matches for all training distributions. Through theory and experiments, we show how the invariances learned by IRM relate to the causal structures governing the data and enable out-of-distribution generalization.},
archivePrefix = {arXiv},
arxivId = {1907.02893},
author = {Arjovsky, Martin and Bottou, L{\'{e}}on and Gulrajani, Ishaan and Lopez-Paz, David},
eprint = {1907.02893},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/1907.02893.pdf:pdf},
pages = {1--30},
title = {{Invariant Risk Minimization}},
url = {http://arxiv.org/abs/1907.02893},
year = {2019}
}
@article{Gelman2005,
abstract = {Analysis of variance (ANOVA) is an extremely important method in exploratory and confirmatory data analysis. Unfortunately, in complex problems (e.g., split-plot designs), it is not always easy to set up an appropriate ANOVA. We propose a hierarchical analysis that automatically gives the correct ANOVA comparisons even in complex scenarios. The inferences for all means and variances are performed under a model with a separate batch of effects for each row of the ANOVA table. We connect to classical ANOVA by working with finite-sample variance components: fixed and random effects models are characterized by inferences about existing levels of a factor and new levels, respectively. We also introduce a new graphical display showing inferences about the standard deviations of each batch of effects. We illustrate with two examples from our applied data analysis, first illustrating the usefulness of our hierarchical computations and displays, and second showing how the ideas of ANOVA are helpful in understanding a previously fit hierarchical model. {\textcopyright} Institute of Mathematical Statistics, 2005.},
archivePrefix = {arXiv},
arxivId = {arXiv:math/0508530v1},
author = {Gelman, Andrew and Tjur, Tue and McCullagh, Peter and Hox, Joop and Hoijtink, Herbert and Zaslavsky, Alan M.},
doi = {10.1214/009053604000001048},
eprint = {0508530v1},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/euclid.aos.1112967698.pdf:pdf},
isbn = {0883423060000},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {ANOVA,Bayesian inference,Fixed effects,Hierarchical model,Linear regression,Multilevel model,Random effects,Variance components},
number = {1},
pages = {1--53},
pmid = {827961},
primaryClass = {arXiv:math},
title = {{Discussion paper analysis of variance - Why it is more important than ever}},
volume = {33},
year = {2005}
}
@article{Peters2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1501.01332v3},
author = {Peters, Jonas and B{\"{u}}hlmann, Peter and Meinshausen, Nicolai},
eprint = {arXiv:1501.01332v3},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/1501.01332.pdf:pdf},
pages = {1--51},
title = {{Causal inference using invariant prediction : identification and confidence intervals}},
year = {2015}
}
@article{Holland1986,
author = {Holland, Paul W},
doi = {10.2307/2289069},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/Journal of the American Statistical Association Volume 81 issue 396 1986 [doi 10.2307{\%}2F2289064] Paul W. Holland -- Statistics and Causal Inference.pdf:pdf},
issn = {0162-1459},
journal = {J Am Stat Assoc},
keywords = {causal model,philosophy},
number = {396},
pages = {968},
title = {{Statistics and Causal Inference: Rejoinder}},
volume = {81},
year = {1986}
}
@article{Rothenh2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1801.06229v2},
author = {Rothenh, Dominik and Meinshausen, Nicolai and Peter, B and Peters, Jonas},
eprint = {arXiv:1801.06229v2},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/1801.06229.pdf:pdf},
pages = {1--31},
title = {{Anchor regression : heterogeneous data meet causality}},
year = {2018}
}
@article{Drton2017,
author = {Drton, Mathias and Maathuis, Marloes H},
doi = {10.1146/annurev-statistics-060116-053803},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/Annual Review of Statistics and Its Application Volume 4 issue 1 2017 [doi 10.1146{\_}annurev-statistics-060116-053803] Drton, Mathias$\backslash$; Maathuis, Marloes H. -- Structure Learning in Graphical Modeling.pdf:pdf},
keywords = {bayesian network,graphical model,markov random field,model selection,multivariate statistics,network reconstruction},
number = {October 2016},
pages = {1--29},
title = {{Structure Learning in Graphical Modeling}},
year = {2017}
}
@article{Engle1983,
abstract = {The effects of firm pension plan provisions on the retirement decisions of older employees are analyzed. The empirical results are based on data from a large firm, with a typical defined benefit pension plan. The "option value" of continued work is the central feature of the analysis. Estimation relies on a retirement decision rule that is close in spirit to the dynamic programming rule but is considerably less complex than a compre- hensive implementation of that rule, thus greatly facilitating the numerical analysis},
author = {Engle, Robert F and Hendry, David F and Richard, Jean-francois},
doi = {10.2307/1911990},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/1911990.pdf:pdf},
issn = {00129682},
journal = {Econometrica},
month = {mar},
number = {2},
pages = {277},
title = {{Exogeneity}},
url = {https://www.jstor.org/stable/1911990?origin=crossref},
volume = {51},
year = {1983}
}
@article{Cox2007,
abstract = {Statistical aspects of causality are reviewed in simple form and the impact of recent work discussed. Three distinct notions of causality are set out and implications for densities and for linear dependencies explained. The importance of appreciating the possibility of effect modifiers is stressed, be they intermediate variables, background variables or unobserved confounders. In many contexts the issue of unobserved confounders is salient. The difficulties of interpretation when there are joint effects are discussed and possible modifications of analysis explained. The dangers of uncritical conditioning and marginalization over intermediate response variables are set out and some of the problems of generalizing conclusions to populations and individuals explained. In general terms the importance of search for possibly causal variables is stressed but the need for caution is emphasized. /// On fait une revue critique de la causalit{\'{e}} statistique. On presente trois definitions de la causalit{\'{e}} et on discute les consequences pour l'analyse statistique et l'interpretation. CR - Copyright {\&}{\#}169; 2004 International Statistical Institute (ISI)},
author = {Cox, D.R. and Wermuth, Nanny},
doi = {10.1111/j.1751-5823.2004.tb00237.x},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/papcaus.pdf:pdf},
journal = {International Statistical Review},
keywords = {able,chain block graph,counterfactual,explanation,instrumental vari-,interaction,markov graph,observational study,overview,regression analysis,surrogate variable,unit-treatment additivity,unobserved confounder},
number = {3},
pages = {285--305},
title = {{Causality: a Statistical View}},
volume = {72},
year = {2007}
}
@article{Fisher1935,
author = {Fisher, Ra},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/The-Design-of-Experiments-Ninth-Edition.pdf:pdf},
issn = {0002-8312},
month = {may},
title = {{The Design of Experiments}},
year = {1935}
}
@article{Rosenbaum2005,
abstract = {Before R. A. Fisher introduced randomized experimentation, the literature on empirical methods emphasized reducing heterogeneity of experimental units as the key to inference about the effects caused by treatments. To what extent is heterogeneity relevant to causal inference when ethicalor practical constraints make random assignment infeasible?},
author = {Rosenbaum, Paul R.},
doi = {10.1198/000313005X42831},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/000313005X42831.pdf:pdf},
issn = {00031305},
journal = {American Statistician},
keywords = {Causal effect,Observational study,Randomization inference,Randomized experiment,Sensitivity analysis,Wilcoxon's signed rank test},
number = {2},
pages = {147--152},
title = {{Heterogeneity and causality: Unit heterogeneity and design sensitivity in observational studies}},
volume = {59},
year = {2005}
}
@article{Holland1986a,
abstract = {Abstract Problems involving causal inference have dogged at the heels of statistics since its earliest days. Correlation does not imply causation , and yet causal conclusions drawn from a carefully designed experiment are often valid. What can a statistical model say about ... },
author = {Holland, Paul W.},
doi = {10.1080/01621459.1986.10478354},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/2289064.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Association,Causal effect,Causal model,Experiments,Granger causality,Hill's nine factors,Koch's postulates,Mill's methods,Path diagrams,Philosophy,Probabilistic causality},
number = {396},
pages = {945--960},
title = {{Statistics and causal inference}},
volume = {81},
year = {1986}
}
@book{Peters2017,
abstract = {"The mathematization of causality is a relatively recent development, and has become increasingly important in data science and machine learning. This book offers a self-contained and concise introduction to causal models and how to learn them from data"--Back of book.},
author = {Peters, J and Janzing, D and Sch{\"{o}}lkopf, B and {Peters, Jonas; Janzing, Dominik; Scholkopf}, Bernhard},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/11283.pdf:pdf},
isbn = {9780262037310},
number = {December},
pages = {1214--1216},
title = {{Elements of causal inference: foundations and learning algorithms}},
url = {file:///tmp/mozilla{\_}kuebler0/11283.pdf{\%}0Ahttps://books.google.com/books?hl=en{\&}lr={\&}id=XPpFDwAAQBAJ{\&}oi=fnd{\&}pg=PR7{\&}ots=GilrFtvtAZ{\&}sig=KYfn3zHUHfiWXhBTvmvwX33bLlA},
year = {2017}
}
@article{Mooij2016,
author = {Mooij, Joris M and Peters, Jonas and Janzing, Dominik and Zscheischler, Jakob},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/14-518.pdf:pdf},
pages = {1--102},
title = {{Distinguishing Cause from Effect Using Observational Data : Methods and Benchmarks}},
volume = {17},
year = {2016}
}
@article{Rojas-carulla2018,
author = {Rojas-Carulla, Mateo and Sch{\"{o}}lkopf, Bernhard and Turner, Richard},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/16-432.pdf:pdf},
keywords = {causality,do-,domain adaptation,main generalization,multi-task learning,transfer learning},
pages = {1--34},
title = {{Invariant Models for Causal Transfer Learning}},
volume = {19},
year = {2018}
}
