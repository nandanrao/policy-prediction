@article{Abadie2006,
abstract = {Matching estimators for average treatment effects are widely used in evaluation research despite the fact that their large sample properties have not been established in many cases. The absence of formal results in this area may be partly due to the fact that standard asymptotic expansions do not apply to matching estimators with a fixed number of matches because such estimators are highly nonsmooth functionals of the data. In this article we develop new methods for analyzing the large sample properties of matching estimators and establish a number of new results. We focus on matching with replacement with a fixed number of matches. First, we show that matching estimators are not N1/2-consistent in general and describe conditions under which matching estimators do attain N1/2-consistency. Second, we show that even in settings where matching estimators are N1/2-consistent, simple matching estimators with a fixed number of matches do not attain the semiparametric efficiency bound. Third, we provide a consistent estimator for the large sample variance that does not require consistent nonparametric estimation of unknown functions. Software for implementing these methods is available in Matlab, Stata, and R.},
author = {Abadie, Alberto and Imbens, Guido W.},
doi = {10.1111/j.1468-0262.2006.00655.x},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abadie, Imbens - 2006 - Large Sample Properties of Matching Estimators.pdf:pdf},
isbn = {1468-0262},
issn = {0012-9682},
journal = {Econometrica},
keywords = {Matching estimators,average treatment effects,potential outcomes,selection on observables,unconfoundedness},
number = {1},
pages = {235--267},
title = {{Large Sample Properties of Matching Estimators}},
url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1468-0262.2006.00655.x/abstract},
volume = {74},
year = {2006}
}
@article{Allcott2015,
address = {Cambridge, MA},
author = {Allcott, Hunt},
doi = {10.1093/qje/qjv015},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Allcott - 2015 - Site Selection Bias in Program Evaluation(2).pdf:pdf},
institution = {National Bureau of Economic Research},
issn = {0033-5533},
journal = {The Quarterly Journal of Economics},
month = {aug},
number = {3},
pages = {1117--1165},
title = {{Site Selection Bias in Program Evaluation}},
url = {http://www.nber.org/papers/w18373.pdf https://academic.oup.com/qje/article-lookup/doi/10.1093/qje/qjv015},
volume = {130},
year = {2015}
}
@article{Angrist2011,
abstract = {This paper develops a covariate-based approach to the external validity of instrumental variables (IV) estimates. Assuming that differences in observed complier characteristics are what make IV estimates differ from one another and from parameters like the effect of treatment on the treated, we show how to construct estimates for new subpopulations from a given set of covariate-specific LATEs. We also develop a reweighting procedure that uses the traditional overidentification test statistic to define a population for which a given pair of IV estimates has external validity. These ideas are illustrated through a comparison of twins and sex-composition IV estimates of the effects childbearing on labor supply.},
author = {Angrist, Joshua D. and Fern{\'{a}}ndez-Val, Iv{\'{a}}n},
doi = {10.1017/CBO9781139060035.012},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Angrist, Fern{\'{a}}ndez-Val - 2011 - ExtrapoLATE-ing External validity and overidentification in the LATE framework.pdf:pdf},
isbn = {9781139060035},
journal = {Advances in Economics and Econometrics: Tenth World Congress Volume 3, Econometrics},
pages = {401--434},
title = {{ExtrapoLATE-ing: External validity and overidentification in the LATE framework}},
year = {2011}
}
@article{Aronow2015,
author = {Aronow, Peter M and Samii, Cyrus},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Aronow, Samii - 2015 - Does Regression Produce Representative Estimates of Causal Effects.pdf:pdf},
keywords = {causal inference,external validity,multiple regression,observational studies,random-},
title = {{Does Regression Produce Representative Estimates of Causal Effects}},
volume = {06520},
year = {2015}
}
@article{Athey2017,
author = {Athey, Susan and Imbens, Guido W},
doi = {10.1257/jep.31.2.3},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Athey, Imbens - 2017 - The State of Applied Econometrics Causality and Policy Evaluation.pdf:pdf},
issn = {0895-3309},
journal = {Journal of Economic Perspectives},
month = {may},
number = {2},
pages = {3--32},
title = {{The State of Applied Econometrics: Causality and Policy Evaluation}},
url = {http://pubs.aeaweb.org/doi/10.1257/jep.31.2.3},
volume = {31},
year = {2017}
}
@article{Snowberg2016,
abstract = {A modern, decision-theoretic framework can help clarify important practical questions of experimental design. Building on our recent work, this chapter begins by summarizing our framework for understanding the goals of experimenters, and applying this to re-randomization. We then use this framework to shed light on questions related to experimental registries, preanalysis plans, and most importantly, external validity. Our framework implies that even when large samples can be collected, external decision-making remains inherently subjective. We embrace this conclusion, and argue that in order to improve external validity, experimental research needs to create a space for structured speculation.},
author = {Banerjee, Abhijit V. and Chassang, Sylvain and Snowberg, Erik},
doi = {10.2139/ssrn.2770140},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Banerjee, Chassang, Snowberg - 2016 - Decision Theoretic Approaches to Experiment Design and External Validity.pdf:pdf},
journal = {SSRN Electronic Journal},
title = {{Decision Theoretic Approaches to Experiment Design and External Validity}},
year = {2016}
}
@article{Banerjee2014,
abstract = {Randomized experiments have become a popular tool in development economics research and have been the subject of a number of criticisms. This paper reviews the recent literature and discusses the strengths and limitations of this approach in theory and in practice. We argue that the main virtue of randomized experiments is that, owing to the close collaboration between researchers and implementers, they allow the estimation of parameters that would not otherwise be possible to evaluate. We discuss the concerns that have been raised regarding experiments and generally conclude that, although real, they are often not specific to experiments. We conclude by discussing the relationship between theory and experiments.},
author = {Banerjee, Abhijit V. and Duflo, Esther},
doi = {10.1146/annurev.economics.050708.143235},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Banerjee, Duflo - 2014 - The experimental approach to development economics.pdf:pdf},
isbn = {9780300169409},
journal = {Field Experiments and Their Critics: Essays on the Uses and Abuses of Experimentation in the Social Sciences},
keywords = {development economics,program,randomized experiments},
pages = {78--114},
title = {{The experimental approach to development economics}},
year = {2014}
}
@article{Banerjee2006,
abstract = {By the fourth day after the October 2005 earthquake in northern Pakistan, the world had woken up to the fact that something very big had happened. The government was estimating that 50,000 or more people had been injured or killed, and many survivors were likely trapped somewhere without water or food. The reaction was immediate and life affirming. Everyone showed up to help: international and local NGOs, the United Nations, and groups of college students with rented trucks full of food and other necessities. Money flowed in from everywhere. The Indian government, reversing a policy of many years, announced that it would open the highly sensitive border between the two Kashmirs so that aid could flow more easily.},
author = {Banerjee, Abhijit V. and Ruimin, He},
doi = {10.1111/j.1475-4932.2010.00627.x},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Banerjee, Ruimin - 2006 - Making aid work.pdf:pdf},
isbn = {9780262550666 0262550660 9780262050906 0262050900},
issn = {01451707},
journal = {Finance and Development},
number = {4},
pages = {14--17},
pmid = {123349624},
title = {{Making aid work}},
volume = {43},
year = {2006}
}
@article{Banerjee2016,
abstract = {The objective of this case study was to obtain some first-hand information about the functional consequences of a cosmetic tongue split operation for speech and tongue motility. One male patient who had performed the operation on himself was interviewed and underwent a tongue motility assessment, as well as an ultrasound examination. Tongue motility was mildly reduced as a result of tissue scarring. Speech was rated to be fully intelligible and highly acceptable by 4 raters, although 2 raters noticed slight distortions of the sibilants /s/ and /z/. The 3-dimensional ultrasound demonstrated that the synergy of the 2 sides of the tongue was preserved. A notably deep posterior genioglossus furrow indicated compensation for the reduced length of the tongue blade. It is concluded that the tongue split procedure did not significantly affect the participant's speech intelligibility and tongue motility.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Banerjee, Abhijit and Duflo, Esther and Kremer, Michael},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1011.1669v3},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Banerjee, Duflo, Kremer - 2016 - The Influence of Randomized Controlled Trials on Development Economics Research and on Development Poli.pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
journal = {Working Paper},
pages = {1--76},
pmid = {15003161},
title = {{The Influence of Randomized Controlled Trials on Development Economics Research and on Development Policy}},
url = {http://www.americanbanker.com/issues/179{\_}124/which-city-is-the-next-big-fintech-hub-new-york-stakes-its-claim-1068345-1.html{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/15003161{\%}5Cnhttp://cid.oxfordjournals.org/lookup/doi/10.1093/cid/cir991{\%}5Cnhttp://www.scielo},
volume = {d},
year = {2016}
}
@article{Bisbee2017,
abstract = {We investigate whether local average treatment effects (LATE's) can be extrapolated to new settings. We extend the analysis and framework of Dehejia, Pop-Eleches, and Samii (2015), which examines the external validity of the Angrist-Evans (1998) reduced-form natural experiment of having two first children of the same sex on the probability of an incremental child and on mother's labor supply. We estimate Angrist and Evans's (1998) same-sex instrumental variable strategy in 139 country-year censuses using data from the Integrated Public Use Micro Sample International. We compare each country-year's LATE, as a hypothetical target, to the LATE extrapolated from other country-years (using the approach suggested by Angrist and Fernandez-Val 2010). Paralleling our findings in Dehejia, Pop-Eleches, and Samii (2015), we find that with a sufficiently large reference sample, we extrapolate the treatment effect reasonably well, but the degree of accuracy depends on the extent of covariate similarity between the target and reference settings. Our results suggest that – at least for our application – there is hope for external validity.},
author = {Bisbee, James and Dehejia, Rajeev and Pop-Eleches, Cristian and Samii, Cyrus},
doi = {10.1086/691280},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bisbee et al. - 2017 - Local Instruments, Global Extrapolation External Validity of the Labor Supply–Fertility Local Average Treatm(2).pdf:pdf},
issn = {0734-306X},
journal = {Journal of Labor Economics},
number = {S1},
pages = {S99--S147},
title = {{Local Instruments, Global Extrapolation: External Validity of the Labor Supply–Fertility Local Average Treatment Effect}},
volume = {35},
year = {2017}
}
@article{Braver2014,
author = {Braver, Sanford L and Thoemmes, Felix J and Rosenthal, Robert},
doi = {10.1177/1745691614529796},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Braver, Thoemmes, Rosenthal - 2014 - Continuously Cumulating Meta-Analysis and Replicability.pdf:pdf},
keywords = {1 proved their perspicacity,daniel,effect-size heterogeneity,future nobel prize winner,kahneman and amos tversky,meta-analysis,over 40 years ago,replication,statistical intuition},
title = {{Continuously Cumulating Meta-Analysis and Replicability}},
year = {2014}
}
@article{Buchanan2018,
abstract = {{\textcopyright} 2018 The Royal Statistical Society and Blackwell Publishing Ltd. Results obtained in randomized trials may not easily generalize to target populations. Whereas in randomized trials the treatment assignment mechanism is known, the sampling mechanism by which individuals are selected to participate in the trial is typically not known and assuming random sampling from the target population is often dubious. We consider an inverse probability of sampling weighted (IPSW) estimator for generalizing trial results to a target population. The IPSW estimator is shown to be consistent and asymptotically normal. A consistent sandwich-type variance estimator is derived and simulation results are presented comparing the IPSW estimator with a previously proposed stratified estimator. The methods are then utilized to generalize results from two randomized trials of human immunodeficiency virus treatment to all people living with the disease in the USA.},
author = {Buchanan, Ashley L. and Hudgens, Michael G. and Cole, Stephen R. and Mollan, Katie R. and Sax, Paul E. and Daar, Eric S. and Adimora, Adaora A. and Eron, Joseph J. and Mugavero, Michael J.},
doi = {10.1111/rssa.12357},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Buchanan et al. - 2018 - Generalizing evidence from randomized trials using inverse probability of sampling weights.pdf:pdf},
issn = {1467985X},
journal = {Journal of the Royal Statistical Society. Series A: Statistics in Society},
keywords = {Causal inference,External validity–generalizability,Human immunodeficiency virus–acquired immune defic,Inverse probability weights,Randomized controlled trial,Target population},
number = {4},
pages = {1193--1209},
title = {{Generalizing evidence from randomized trials using inverse probability of sampling weights}},
volume = {181},
year = {2018}
}
@article{Cartwright2013,
abstract = {Over the last twenty or so years, it has become standard to require policy makers to base their recommendations on evidence. That is now uncontroversial to the point of triviality—of course, policy should be based on the facts. But are the methods that policy makers rely on to gather and analyze evidence the right ones? In Evidence-Based Policy, Nancy Cartwright, an eminent scholar, and Jeremy Hardie, who has had a long and successful career in both business and the economy, explain that the dominant methods which are in use now—broadly speaking, methods that imitate standard practices in medicine like randomized control trials—do not work. They fail, Cartwright and Hardie contend, because they do not enhance our ability to predict if policies will be effective. The prevailing methods fall short not just because social science, which operates within the domain of real-world politics and deals with people, differs so much from the natural science milieu of the lab. Rather, there are principled reasons why the advice for crafting and implementing policy now on offer will lead to bad results. Current guides in use tend to rank scientific methods according to the degree of trustworthiness of the evidence they produce. That is valuable in certain respects, but such approaches offer little advice about how to think about putting such evidence to use. Evidence-Based Policy focuses on showing policymakers how to effectively use evidence. It also explains what types of information are most necessary for making reliable policy, and offers lessons on how to organize that information.},
author = {Cartwright, Nancy and Hardie, Jeremy},
doi = {10.5860/choice.50-5831},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cartwright, Hardie - 2013 - Evidence-based policy a practical guide to doing it better.pdf:pdf},
isbn = {978-0-19-984160-8},
issn = {0009-4978},
journal = {Choice Reviews Online},
number = {10},
pages = {50--5831--50--5831},
title = {{Evidence-based policy: a practical guide to doing it better}},
volume = {50},
year = {2013}
}
@article{Chassang2010,
author = {Chassang, Sylvain and i Miquel, Gerard Padro and Snowberg, Erik},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chassang, i Miquel, Snowberg - 2010 - Selective Trials and Information Production in Randomized Controlled Experiments.pdf:pdf},
keywords = {To Read},
title = {{Selective Trials and Information Production in Randomized Controlled Experiments}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.163.5979{\&}rep=rep1{\&}type=pdf{\%}5Cnpapers3://publication/uuid/81A54BE7-4ACE-4037-8DB5-66BEF6A134A0},
year = {2010}
}
@article{Deaton2010,
abstract = {There is currently much debate about the effectiveness of foreign aid and about what kind of projects can engender economic development. There is skepticism about the ability of econometric analysis to resolve these issues or of development agencies to learn from their own experience. In response, there is increasing use in development economics of randomized controlled trials (RCTs) to accumulate credible knowledge of what works, without overreliance on questionable theory or statistical methods. When RCTs are not possible, the proponents of these methods advocate quasi- randomization through instrumental variable (IV) techniques or natural experiments. I argue that many of these applications are unlikely to recover quantities that are useful for policy or understanding: two key issues are the misunderstanding of exogeneity and the handling of heterogeneity. I illustrate from the literature on aid and growth. Actual randomization faces similar problems as does quasi-randomization, notwithstanding rhetoric to the contrary. I argue that experiments have no special ability to produce more credible knowledge than other methods, and that actual experiments are frequently subject to practical problems that undermine any claims to statistical or epistemic superiority. I illustrate using prominent experiments in development and elsewhere. As with IV methods, RCT-based evaluation of projects, without guidance from an understanding of underlying mechanisms, is unlikely to lead to scientific progress in the understanding of economic development. I welcome recent trends in development experimentation away from the evaluation of projects and toward the evaluation of theoretical mechanisms. (JEL C21, F35, O19)},
author = {Deaton, Angus},
doi = {10.1257/jel.48.2.424},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Deaton - 2010 - Instruments, Randomization, and Learning about Development.pdf:pdf},
issn = {0022-0515},
journal = {Journal of Economic Literature},
month = {jun},
number = {2},
pages = {424--455},
title = {{Instruments, Randomization, and Learning about Development}},
url = {http://pubs.aeaweb.org/doi/10.1257/jel.48.2.424},
volume = {48},
year = {2010}
}
@article{Deaton2018,
author = {Deaton, Angus and Cartwright, Nancy},
doi = {10.1016/j.socscimed.2017.12.005},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Deaton, Cartwright - 2018 - Understanding and misunderstanding randomized controlled trials.pdf:pdf},
issn = {0277-9536},
journal = {Social Science {\&} Medicine},
keywords = {Balance,Bias,Economic development,External validity,Health,Precision,RCTs,Transportation of results},
number = {October 2017},
pages = {2--21},
publisher = {Elsevier},
title = {{Understanding and misunderstanding randomized controlled trials}},
url = {https://doi.org/10.1016/j.socscimed.2017.12.005},
volume = {210},
year = {2018}
}
@article{Flores2013,
abstract = {We study the effectiveness of nonexperimental strategies in adjusting for comparison group differences when using data from several programs, each implemented at a different location, to compare their effect if implemented at alternative locations. First, we adjust for individual characteristics differences simultaneously across all groups using unconfoundedness-based and conditional difference-in-difference methods for multiple treatments. Second, we adjust for differences in local economic conditions and stress their role after program participation. Our results show that it is critical to have sufficient overlap across locations in both dimensions and illustrate the difficulty of adjusting for local economic conditions that differ greatly across locations. {\textcopyright} 2013 by the President and Fellows of Harvard College and the Massachusetts Institute of Technology.},
author = {Flores, Carlos A. and Mitnik, Oscar A.},
doi = {10.1162/REST_a_00373},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Flores, Mitnik - 2013 - Comparing treatments across labor markets An assessment of nonexperimental multiple-treatment strategies.pdf:pdf},
issn = {15309142},
journal = {Review of Economics and Statistics},
number = {5},
pages = {1691--1707},
title = {{Comparing treatments across labor markets: An assessment of nonexperimental multiple-treatment strategies}},
volume = {95},
year = {2013}
}
@unpublished{Gechter2015,
abstract = {To what extent are causal effects estimated in one region or time period informative about another region or time? In this paper, I derive bounds on the average causal effect in a context of interest using experimental evidence from another context. I use differences in outcome distributions for individuals with the same characteristics and treatment status in the original study and the context of interest to learn about unobserved differences across contexts. Greater differences in outcome distributions generate wider bounds. Empirically, I explore using experimental results on the return to cash transfers to male microentrepreneurs in one Mexican city in 2006 to predict the returns among male microentrepreneurs in urban Mexico in 2012. I show that existing methods would lead us to be overconfident in extrapolating from the small experiment to all of urban Mexico in 2012. Using data from a pair of remedial education experiments carried out in urban India, I show that the methods suggested in this paper are able to recover average causal effects in one city using results from the other where existing methods are unsuccessful.},
author = {Gechter, Michael},
booktitle = {Working Paper},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gechter - 2015 - Generalizing the Results from Social Experiments Theory and Evidence from Mexico and India.pdf:pdf},
keywords = {external validity,partial identification,sensitivity analysis},
title = {{Generalizing the Results from Social Experiments: Theory and Evidence from Mexico and India}},
year = {2015}
}
@article{Heckman2008,
author = {Heckman, James J},
doi = {10.1111/j.1751-5823.2007.00024.x},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Heckman - 2008 - Econometric Causality.pdf:pdf},
issn = {0306-7734},
journal = {International Statistical Review},
month = {apr},
number = {1},
pages = {1--27},
title = {{Econometric Causality}},
url = {http://doi.wiley.com/10.1111/j.1751-5823.2007.00024.x},
volume = {76},
year = {2008}
}
@article{Heckman1995,
abstract = {This paper analyzes the method of social experiments. The assumptions that justify the experimental method are exposited. Parameters of interest in evaluating social programs are discussed. The authors show how experiments sometimes serve as instrumental variables to identify program impacts. The most favorable case for experiments ignores variability across persons in response to treatments received and assumes that mean impacts of a program are the main object of interest in conducting an evaluation. Experiments do not identify the distribution of program gains unless additional assumptions are maintained. Evidence on the validity of the assumptions used to justify social experiments is presented.},
author = {Heckman, James J and Smith, Jeffrey A},
doi = {10.1257/jep.9.2.85},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Heckman, Smith - 1995 - Assessing the Case for Social Experiments.pdf:pdf},
issn = {0895-3309},
journal = {Journal of Economic Perspectives},
number = {2},
pages = {85--110},
title = {{Assessing the Case for Social Experiments}},
volume = {9},
year = {1995}
}
@article{Heckman2007,
abstract = {This chapter relates the literature on the econometric evaluation of social programs to the literature in statistics on "causal inference". In it, we develop a general evaluation framework that addresses well-posed economic questions and analyzes agent choice rules and subjective evaluations of outcomes as well as the standard objective evaluations of outcomes. The framework recognizes uncertainty faced by agents and ex ante and ex post evaluations of programs. It also considers distributions of treatment effects. These features are absent from the statistical literature on causal inference. A prototypical model of agent choice and outcomes is used to illustrate the main ideas. We formally develop models for counterfactuals and causality that build on Cowles Commission econometrics. These models anticipate and extend the literature on causal inference in statistics. The distinction between fixing and conditioning that has recently entered the statistical literature was first developed by Cowles economists. Models of simultaneous causality were also developed by the Cowles group, as were notions of invariance to policy interventions. These basic notions are updated to nonlinear and nonparametric frameworks for policy evaluation more general than anything in the current statistical literature on "causal inference". A formal discussion of identification is presented and applied to clearly formulated choice models used to evaluate social programs. {\textcopyright} 2007 Elsevier B.V. All rights reserved.},
author = {Heckman, James J. and Vytlacil, Edward J.},
doi = {10.1016/S1573-4412(07)06070-9},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Heckman, Vytlacil - 2007 - Econometric Evaluation of Social Programs, Part I Causal Models, Structural Models and Econometric Policy Eva.pdf:pdf},
isbn = {9780444532008},
issn = {15734412},
journal = {Handbook of Econometrics},
keywords = {causal models,counterfactuals,identification,policy evaluation,policy invariance,structural models},
number = {SUPPL. PART B},
pages = {4779--4874},
title = {{Econometric Evaluation of Social Programs, Part I: Causal Models, Structural Models and Econometric Policy Evaluation}},
volume = {6},
year = {2007}
}
@article{Hotz2005,
abstract = {We investigate the problem of predicting the average effect of a new training program using experiences with previous implementations. There are two principal complications in doing so. First, the population in which the new program will be implemented may differ from the population in which the old program was implemented. Second, the two programs may differ in the mix or nature of their components, or in their efficacy across different sub-populations. The first problem is similar to the problem of non-experimental evaluations. The ability to adjust for population differences typically depends on the availability of characteristics of the two populations and the extent of overlap in their distributions. The ability to adjust for differences in the programs themselves may require more detailed data on the exact treatments received by individuals than are typically available. This problem has received less attention, although it is equally important for the prediction of the efficacy of new programs. To investigate the empirical importance of these issues, we compare four experimental Work INcentive demonstration programs implemented in the mid-1980s in different parts of the U.S. We find that adjusting for pre-training earnings and individual characteristics removes many of the differences between control units that have some previous employment experience. Since the control treatment is the same in all locations, namely embargo from the program services, this suggests that differences in populations served can be adjusted for in this sub-population. We also find that adjusting for individual characteristics is more successful at removing differences between control group members in different locations that have some employment experience in the preceding four quarters than for control group members with no previous work experience. Perhaps more surprisingly, our ability to predict the outcomes of trainees after adjusting for individual characteristics is similar. We surmise that differences in treatment components across training programs are not sufficiently large to lead to substantial differences in our ability to predict trainees' post-training earnings for many of the locations in this study. However, in the sub-population with no previous work experience there is some evidence that unobserved heterogeneity leads to difficulties in our ability to predict outcomes across locations for controls.},
author = {Hotz, V. Joseph and Imbens, Guido W. and Mortimer, Julie H.},
doi = {10.1016/j.jeconom.2004.04.009},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hotz, Imbens, Mortimer - 2005 - Predicting the efficacy of future training programs using past experiences at other locations.pdf:pdf},
issn = {03044076},
journal = {Journal of Econometrics},
keywords = {Efficacy,Heterogeneity,Prediction,Training programs},
number = {1-2 SPEC. ISS.},
pages = {241--270},
title = {{Predicting the efficacy of future training programs using past experiences at other locations}},
volume = {125},
year = {2005}
}
@article{Imbens2014,
author = {Imbens, Guido},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Imbens - 2014 - COMMENTS ON “UNDERSTANDING AND MISUNDERSTANDING RANDOMIZED CONTROLLED TRIALS” BY CARTWRIGHT AND DEATON GUIDO.pdf:pdf},
pages = {1--8},
title = {{COMMENTS ON: “UNDERSTANDING AND MISUNDERSTANDING RANDOMIZED CONTROLLED TRIALS” BY CARTWRIGHT AND DEATON GUIDO}},
year = {2014}
}
@article{Imbens2009,
abstract = {Two recent papers, Deaton (2009), and Heckman and Urzua (2009), argue against what they see as an excessive and inappropriate use of experimental and quasi-experimental methods in empirical work in economics in the last decade. They specifically question the increased use of instrumental variables and natural experiments in labor economics, and of randomized experiments in development economics. In these comments I will make the case that this move towards shoring up the internal validity of estimates, and towards clarifying the description of the population these estimates are relevant for, has been important and beneficial in increasing the credibility of empirical work in economics. I also address some other concerns raised by the Deaton and Heckman-Urzua papers.},
author = {Imbens, Guido W},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/w14896.pdf:pdf},
journal = {NBER Working Paper Series},
title = {{Better LATE Than Nothing}},
url = {http://www.nber.org/papers/w14896},
year = {2009}
}
@book{Lehmann1998,
abstract = {This second, much enlarged edition by Lehmann and Casella of Lehmann's classic text on point estimation maintains the outlook and general style of the first edition. All of the topics are updated. An entirely new chapter on Bayesian and hierarchical Bayesian approaches is provided, and there is much new material on simultaneous estimation. Each chapter concludes with a Notes section which contains suggestions for further study. The book is a companion volume to the second edition of Lehmann's "Testing Statistical Hypotheses". E.L. Lehmann is Professor Emeritus at the University of California, Berkeley. He is a member of the National Academy of Sciences and the American Academy of Arts and Sciences, and the recipient of honorary degrees from the University of Leiden, The Netherlands, and the University of Chicago. George Casella is the Liberty Hyde Bailey Professor of Biological Statistics in The College of Agriculture and Life Sciences at Cornell University. Casella has served as associate editor of The American Statistician, Statistical Science and JASA. He is currently the Theory and Methods Editor of JASA. Casella has authored two other textbooks (Statistical Inference, 1990, with Roger Berger and Variance Components, 1992, with Shayle A. Searle and Charles McCulloch). He is a fellow of the IMS and ASA, and an elected fellow of the ISI.Also available:E.L. Lehmann, Testing Statistical Hypotheses Second Edition, Springer-Verlag New York, Inc., ISBN 0-387-949194.},
author = {Lehmann, E L and Casella, G},
booktitle = {Design},
doi = {10.2307/1270597},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lehmann, Casella - 1998 - Theory of Point Estimation , Second Edition Springer Texts in Statistics.pdf:pdf},
isbn = {0387985026},
issn = {00401706},
number = {3},
pages = {589},
pmid = {3087590},
title = {{Theory of Point Estimation , Second Edition Springer Texts in Statistics}},
url = {http://www.amazon.com/dp/0387985026},
volume = {41},
year = {1998}
}
@article{Lesko2018,
author = {Lesko, Catherine R and Buchanan, Ashley L and Westreich, Daniel and Edwards, Jessie K},
doi = {10.1097/EDE.0000000000000664.Generalizing},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lesko et al. - 2018 - Generalizing study results a potential outcomes perspective.pdf:pdf},
isbn = {0000000000000},
number = {4},
pages = {553--561},
title = {{Generalizing study results: a potential outcomes perspective}},
volume = {28},
year = {2018}
}
@book{Manski2013,
abstract = {Manski argues that public policy is based on untrustworthy analysis. Failing to account for uncertainty in an uncertain world, policy analysis routinely misleads policy makers with expressions of certitude. Manski critiques the status quo and offers an innovation to improve both how policy research is conducted and how it is used by policy makers.},
address = {Cambridge, MA and London, England},
author = {Manski, Charles F.},
booktitle = {Public Policy in an Uncertain World},
doi = {10.4159/harvard.9780674067547},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Manski - 2012 - Public Policy in an Uncertain World.pdf:pdf},
isbn = {9780674067547},
month = {jan},
publisher = {Harvard University Press},
title = {{Public Policy in an Uncertain World}},
url = {http://www.degruyter.com/view/books/harvard.9780674067547/harvard.9780674067547/harvard.9780674067547.xml},
year = {2012}
}
@article{Meager2018,
author = {Meager, Rachael},
doi = {10.2139/ssrn.2620834},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Meager - 2018 - Aggregating Distributional Treatment Effects A Bayesian Hierarchical Analysis of the Microcredit Literature.pdf:pdf},
title = {{Aggregating Distributional Treatment Effects : A Bayesian Hierarchical Analysis of the Microcredit Literature}},
year = {2018}
}
@article{Pearl2018,
author = {Pearl, Judea and Bareinboim, Elias},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pearl, Bareinboim - 2018 - Commentary EPIDEMIOLOGY MS{\#} EDE18-0227 A note on Generalizability of Study Results.pdf:pdf},
keywords = {external validity,generalizability,selection bias,transportability},
number = {November},
pages = {1--5},
title = {{Commentary: EPIDEMIOLOGY MS{\#} EDE18-0227 A note on "Generalizability of Study Results"}},
volume = {1750807},
year = {2018}
}
@article{Pearl2014,
abstract = {The generalizability of empirical findings to new environments, settings or populations, often called “external validity,” is essential in most scientific explorations. This paper treats a particular problem of generalizability, called “transportability”, defined as a license to transfer causal effects learned in experimental studies to a new population, in which only observational studies can be conducted. We introduce a formal representation called “selection diagrams” for expressing knowledge about differences and commonalities between populations of interest and, using this representation, we reduce questions of transportability to symbolic derivations in the do-calculus. This reduction yields graph-based procedures for deciding whether causal effects in the target population can be inferred from experimental findings in the study population. When the answer is affirmative, the procedures identify what experimental and observational findings need be obtained from the two populations, and how they can be combined to ensure bias-free transport.},
archivePrefix = {arXiv},
arxivId = {arXiv:1503.01603v1},
author = {Pearl, Judea and Bareinboim, Elias},
doi = {10.1214/14-STS486},
eprint = {arXiv:1503.01603v1},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pearl, Bareinboim - 2014 - External validity From do-calculus to transportability across populations.pdf:pdf},
issn = {08834237},
journal = {Statistical Science},
keywords = {Causal effects,Experimental design,External validity,Generalizability},
number = {4},
pages = {579--595},
title = {{External validity: From do-calculus to transportability across populations}},
volume = {29},
year = {2014}
}
@article{Peters2018,
abstract = {{\textcopyright} The Author(s) 2018. Published by Oxford University Press on behalf of the International Bank for Reconstruction and Development / THE WORLD BANK. All rights reserved. When properly implemented, Randomized Controlled Trials (RCT) achieve a high degree of internal validity. Yet, if an RCT is to inform policy, it is critical to establish external validity. This paper systematically reviews all RCTs conducted in developing countries and published in leading economic journals between 2009 and 2014 with respect to how they deal with external validity. Following Duflo, Glennerster, and Kremer (2008), we scrutinize the following hazards to external validity: Hawthorne effects, general equilibrium effects, specific sample problems, and special care in treatment provision. Based on a set of objective indicators, we find that the majority of published RCTs does not discuss these hazards and many do not provide the necessary information to assess potential problems. The paper calls for including external validity dimensions in a more systematic reporting on the results of RCTs. Thismay create incentives to avoid overgeneralizing findings and help policymakers to interpret results appropriately.},
author = {Peters, J{\"{o}}rg and Langbein, J{\"{o}}rg and Roberts, Gareth},
doi = {10.1093/wbro/lkx005},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Peters, Langbein, Roberts - 2018 - Generalization in the tropics-development policy, randomized controlled trials, and external validity.pdf:pdf},
issn = {15646971},
journal = {World Bank Research Observer},
number = {1},
pages = {34--64},
title = {{Generalization in the tropics-development policy, randomized controlled trials, and external validity}},
volume = {33},
year = {2018}
}
@article{Pritchett2017,
author = {Pritchett, Lant},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pritchett - 2017 - “The Evidence” About “What Works” in Education Graphs to Illustrate External Validity and Construct Validity.pdf:pdf},
pages = {1--9},
title = {{“The Evidence” About “What Works” in Education: Graphs to Illustrate External Validity and Construct Validity}},
year = {2017}
}
@article{Pritchett2016,
author = {Pritchett, Lant and Sandefur, Justin},
doi = {10.1257/aer.p20151016},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pritchett, Sandefur - 2015 - Learning from Experiments when Context Matters(2).pdf:pdf},
issn = {0002-8282},
journal = {American Economic Review},
month = {may},
number = {5},
pages = {471--475},
title = {{Learning from Experiments when Context Matters}},
url = {http://eds.b.ebscohost.com.ezproxy.lib.usf.edu/eds/pdfviewer/pdfviewer?vid=5{\&}sid=aef21225-20b8-458e-842b-e860243ea5f7{\%}40sessionmgr104{\&}hid=104 http://pubs.aeaweb.org/doi/10.1257/aer.p20151016},
volume = {105},
year = {2015}
}
@article{Pritchett2013,
abstract = {In this paper we examine how policymakers and practitioners should interpret the impact evaluation literature when presented with conflicting experimental and non-experimental estimates of the same intervention across varying contexts. We show three things. First, as is well known, non-experimental estimates of a treatment effect comprise a causal treatment effect and a bias term due to endogenous selection into treatment. When non-experimental estimates vary across contexts any claim for external validity of an experimental result must make the assumption that (a) treatment effects are constant across contexts, while (b) selection processes vary across contexts. This assumption is rarely stated or defended in systematic reviews of evidence. Second, as an illustration of these issues, we examine two thoroughly researched literatures in the economics of education — class size effects and gains from private schooling — which provide experimental and non-experimental estimates of causal effects from the same context and across multiple contexts.  We show that the range of “true” causal effects in these literatures implies OLS estimates from the right context are, at present, a better guide to policy than experimental estimates from a different context. Third, we show that in important cases in economics, parameter heterogeneity is driven by economy- or institution-wide contextual factors, rather than personal characteristics, making it difficult to overcome external validity concerns through estimation of heterogeneous treatment effects within a single localized sample. We conclude with recommendations for research and policy, including the need to evaluate programs in context, and avoid simple analogies to clinical medicine in which “systematic reviews” attempt to identify best-practices by putting most (or all) weight on the most “rigorous” evidence with no allowance for context.},
author = {Pritchett, Lant and Sandefur, Justin},
doi = {10.2139/ssrn.2364580},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pritchett, Sandefur - 2013 - Context Matters for Size Why External Validity Claims and Development Practice Don't Mix.pdf:pdf},
journal = {SSRN Electronic Journal},
keywords = {causal inference,external validity,policy evaluation,treatment effects},
number = {August 2013},
title = {{Context Matters for Size: Why External Validity Claims and Development Practice Don't Mix}},
year = {2013}
}
@article{Rosenzweig2019,
author = {Rosenzweig, Mark Richard and Udry, Christopher},
doi = {10.2139/ssrn.3392657},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rosenzweig, Udry - 2019 - External Validity in a Stochastic World Evidence from Low-Income Countries(2).pdf:pdf},
journal = {SSRN Electronic Journal},
title = {{External Validity in a Stochastic World: Evidence from Low-Income Countries}},
year = {2019}
}
@article{Shadish2002a,
author = {Shadish, William R and Cook, Thomas D and Campbell, DT},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shadish, Cook, Campbell - 2002 - Experiemental and Quasi-Experimental for Generalized Designes for Generalized Causal lnference.pdf:pdf},
title = {{Experiemental and Quasi-Experimental for Generalized Designes for Generalized Causal lnference}},
year = {2002}
}
@article{Vivalt2017,
abstract = {Impact evaluations aim to predict the future, but they are rooted in particular contexts and to what extent they generalize is an open and important question. We exploit a new data set of results on a wide variety of interventions and find more heterogeneity than in other literatures. This has implications for how evidence is generated and used to inform policy.},
author = {Vivalt, Eva},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vivalt - 2017 - How Much Can We Generalize from Impact Evaluations.pdf:pdf},
journal = {Working Paper, Australian National University},
title = {{How Much Can We Generalize from Impact Evaluations ?}},
year = {2017}
}
@book{Shadish2002,
author = {{William R. Shadish} and Cook, Thomas D and Campbell, Donald T.},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/William R. Shadish, Cook, Campbell - 2001 - Experimental and Quasi-Experimental Designs for Generalized Causal Inference.pdf:pdf},
isbn = {0395615569},
pages = {656},
title = {{Experimental and Quasi-Experimental Designs for Generalized Causal Inference}},
year = {2001}
}
