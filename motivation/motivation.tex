\documentclass[a4paper,12pt]{article}
\usepackage[backend=biber, citestyle=authoryear, bibencoding=utf8]{biblatex}
\addbibresource{../bibs/external-validity.bib}
\addbibresource{../bibs/econ-causal-history.bib}
\addbibresource{../bibs/causality.bib}
\addbibresource{../bibs/domain-adaptation.bib}
\addbibresource{../bibs/economic-forests.bib}
\addbibresource{../bibs/microcredit.bib}
\addbibresource{../bibs/extra-citations.bib}
\addbibresource{../bibs/quantile-regression.bib}

\usepackage{amsmath, amsthm, amsfonts, mathtools, csquotes, bm, centernot, bbm}

\usepackage[default,light,bold]{sourceserifpro}
\usepackage[T1]{fontenc}

\newtheorem{prop}{Proposition}

\usepackage{pgf, tikz}
\usetikzlibrary{arrows, automata}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\D}{\mathcal{D}}



\newcommand{\CI}{\mathrel{\perp\mspace{-10mu}\perp}}
\newcommand{\nCI}{\centernot{\CI}}

\title{ Policy Prediction: The Missing Tool in Experimental Econometrics and a Roadmap to Fix It  }

\author{Nandan Rao}


\begin{document}

\maketitle

\begin{abstract}
Applied economics research is most often ``applied'' to policy making. In the literature on experimental and quasi-experimental econometrics, however, there are few formal frameworks that use the results of experimental studies to predict the effects of policies in new contexts. I explain the existence of this gap from a historical perspective, review the current methods that do exist, and set a research agenda to fill it using recent ideas from econometrics and machine learning.
\end{abstract}


\section{Introduction}

Randomized control trials (RCTs), or natural experiments that replicate them, have become the official gold standard of empirical work in economics and many related fields. More and more, policy makers are encouraged to look to RCTs to make ``evidence-based'' policy decisions (\cite{Manski2013, Cartwright2013}).

Many prominent economists have expressed a concern that RCTs and their quasi-experimental colleagues (i.e. natural experiments, instrumental variables, regression discontinuity, etc.) are particularly difficult to generalize to new contexts due to their overarching concern for internal validity, often at the expense of external validity (\cite{Heckman1995, Heckman2008, Deaton2010, Manski2013, Deaton2018}).

Making evidence-based policy decisions is an act of generalizing from previous studies to decisions about the future. Charles Manski \parencite*{Manski2013} voices the concern that experimental studies have tended to ``be silent'' on the question of external validity and that ``from the perspective of policy choice \ldots What matters is the informativeness of a study for policy making, which depends jointly on internal and external validity.''

This silence on external validity means that, despite the huge field of research and techniques for ensuring causal identification (internal validity) via experimental or quasi-experimental methods, very little has been done to either A) create tools to prove that the same results will apply generally in all contexts or B) create tools to predict the results in a specific context, given proven results from one or more experiments.

Abhijit Banjerjee himself \parencite*{Snowberg2016}, a large proponent of RCTs, has recently echoed the current lack of and need for more formal systems for the generalization of experimental studies, stating ``it is our belief that creating a rigorous framework for external validity is an important step in completing an ecosystem for social science field experiments, and a complement to many other aspects of experimentation.''

In response to such theoretical concerns, a small but growing literature  has sprung up around empirically proving that results from prominent RCT or other causally identified studies do not easily extrapolate to new contexts (\cite{Pritchett2016, Allcott2015, Bisbee2017, Rosenzweig2019}). These results emphasize a need for powerful extrapolation tools if policy makers are realistically expected to make decisions based on ``evidence.''

Predicting the results of a policy in a new location is, tautologically, a prediction problem. Machine learning is a field which has been very successful at formalizing prediction problems. Domain adaptation, a subfield of machine learning, formalizes the problem of moving from one (or more) domains with labelled data to a new domain where only unlaballed data is available (\cite[for a survey, see][]{Pan2010}).

Formulating the problem of policy prediction in terms of domain adaptation provides a rigorous way to think about the assumptions that may or may not hold, as well as a rich (if short) history of techniques that have been used to solve the problems associated with those assumptions. I will show how policy prediction can be formulated as a covariate shift problem and propose that the associated assumptions could be tested empirically in a treatment effects framework.

This article will take the following form: Section 2 will define external and internal validity, their role in scientific inference, review the development of the Fisherian experimental model of statistical inference, and set out the need for a formal framework of extrapolation. Section 3 will argue that a target context is necessary by reviewing the development of such an argument in mid-century econometrics and formalizing their arguments in probabilistic terms. Section 4 will introduce the concept of domain adaptation in machine learning, review econometric formulations of the same problem, and present recent work in machine learning and econometrics that can be used to solve the problem implied by the formulations. Section 5 will conclude.


\section{Validity and Counterfactual Identification}

\subsection{A Taxonomy of Validities}

Critiques against the holy status of RCTs have focused on the proponents' preference for internal validity and tendency to completely ignore external validity (\cite{Manski2013, Deaton2018}). It is worth defining these terms and reflecting on exactly what it is that experimental methods provide us. The most agreed upon definition of these terms comes from \cite{Shadish2002} (an update of their previously popular taxonomy of validities from \cite{Cook1979}):

\begin{displayquote}
\textbf{Statistical Conclusion Validity:} The validity of inferences about the correlation (covariation) between treatment and outcome.

\textbf{Internal Validity:} The validity of inferences about whether observed covariation between A (the presumed treatment) and B (the presumed outcome) reflects a causal relationship from A to B as those variables were manipulated or measured.

\textbf{Construct Validity:} The validity of inferences about the higher order constructs that represent sampling particulars.

\textbf{External Validity:} The validity of inferences about whether the cause-effect relationship holds over variation in persons, settings, treatment variables, and measurement variables.
\end{displayquote}

Many authors use the term external validity to refer to what Shadish, Cook, and Campbell separate into construct and external validity. As both are involved in generalization, and both are external to the particulars of the sample, I will follow that abuse of terminology and refer to all the challenges of both as ``external validity.''

What, then, is meant by the term ``inference?''

\subsection{Inference and Statistics}

Mary S. Morgan, in her \textit{History of Econometric Ideas} \parencite*{Morgan1991}, lays out the tension between mathematics and statistics in economics of the nineteenth-century: mathematics was seen as a tool for deductive reasoning, used to derive logical conclusions from known laws while statistics, on the other hand, was viewed as a tool for inductive reasoning, used to establish economic regularities.

This distinction between deduction and induction follows from the Aristotelian tradition of logic, where inference is the process of reasoning and can be broken into two parts: reasoning from particulars to generals (induction) and reasoning from generals to particulars (deduction). Once one has induced a general law (``all men are mortal''), one can deduce facts that might otherwise not yet be apparent (``Socrates will die''). There is, of course, a natural contradiction in this process: how can one know that all men are mortal, if one did not already know that Socrates will die? In other words, how can one ever claim that ``all men are mortal'' until one has seen every man die?

This problem forms the foundation of David Hume's \textit{Problem of Induction}:

\begin{displayquote}
``As to past Experience, it can be allowed to give direct and certain information of those precise objects only, and that precise period of time, which fell under its cognizance: but why this experience should be extended to future times, and to other objects, which for aught we know, may be only in appearance similar, this is the main question on which I would insist'' \parencite{hume1748}
\end{displayquote}

There are two distinct problems Hume raises, that of generalizing from one object to another (from seeing some men die to assuming all men have died) and that of generalizing from the past to the future (because all men have died, thus, all men will die). He goes on to explain that this extrapolation is only valid under strong assumptions as to the ``uniformity of nature.'' One must assume nature is uniform in such a way as to enable extrapolation from one object to another or from the past to the future.

R.A. Fisher also framed his statistical techniques in terms of ``inductive inference.'' In the introduction to \textit{The Design of Experiments} \parencite*{Fisher1935}, he explicitly frames his canonical book and its techniques in the terms of logicians such as Hume, arguing for the possibility of induction via statistical methods:
%
\begin{displayquote}
``\ldots it is possible to draw valid inferences from the results of experimentation\ldots as a statistician would say, from a sample to the population from which the sample was drawn, or, as a logician might put it, from the particular to the general.'' \parencite{Fisher1935}
\end{displayquote}

Fisherian statistical inference is a tool in the process of induction that seeks to address Hume's problem of extending experience with one object to that of similar objects. In particular, the ``similar objects'' to which conclusions are extended are not just similar in appearance, but rather have a distinct relationship to the experienced objects: they are the population from which the experienced objects represent a sample.

Fisher's methodologies for significance testing relate to drawing conclusions about populations given a sample. The validity of the use of such techniques in a study falls squarely under the category of ``statistically conclusion validity'' in the taxonomy of Shadish, Cook, and Campbell.

Fisher's theory of experiments (in particular, the RCT) addresses internal validity and causality. The connection between the RCT and the identification of a causal relationship comes straight from John Stuart Mill's \parencite*{mill1884} \textit{Method of Difference} for causal discovery:

\begin{displayquote}
  ``If an instance in which the phenomenon under investigation occurs, and an instance in which it does not occur, have every circumstance save one in common, that one occurring only in the former; the circumstance in which alone the two instances differ, is the effect, or cause, or a necessary part of the cause, of the phenomenon.''
\end{displayquote}

Fisher \parencite*{Fisher1935} had this in mind when he defended randomization, claiming that holding all other variables constant was not feasible, and thus, holding them to the same distribution, by making the assignment of treatment independent of those variables, was desirable (\cite{Rosenbaum2005}). It should be noted, however, that for the subjects of interest to Fisher, the difference between something being a ``cause'' or ``a necessary part of the cause'' was not especially important. What Fisher was interested in was essentially interventional prediction: what are the effects of a given cause (individual changes to the growing conditions of these plants).

Holland \parencite*{Holland1986} begins his landmark paper, before formulating the Rubin-Neyman causal model, by framing his goal:

\begin{displayquote}
``Others are concerned with deducing the causes of a given effect. Still others are interested in understanding the details of causal mechanisms. The emphasis here will be on measuring the effects of causes because this seems to be a place where statistics, which is concerned with measurement, has contributions to make.''
\end{displayquote}

The success of Fisher's framework of randomization and the Rubin-Neyman causal model comes down to this razor focus in purpose: they make no claims to discover all the causes of a given effect, to discover the mechanism of the cause, or even to separate between a cause or a necessary part of a cause. They allow us to reason about counterfactuals: what would have happened, on average and in the past, had we treated our entire population rather than a randomized part of a randomized sample drawn from that population. It operationalizes Mill's method of differences, creating a pathway to internal validity that is achievable in the real world.

The counterfactual causal model, however, provides no framework for generalizing from a specific population to a more general one or from the past to the future (\cite{Heckman2008}). The ``effects of causes'' is a black-box methodology for causal identification  (\cite{Heckman1995}). It does not require one to answer the question ``why'' and it is therefore inherently context specific: it asks, ``what happened when I did $X$ in context $D$?'' In Fisher's line of work (agriculture), none of those shortcomings were problematic. He was able to randomly sample from the exact population (seeds of grain) to which his inferences needed to generalize.

The success of Fisher's randomization in his field of agriculture, and the subsequent success of the Rubin-Neyman causal model in epidemiology, is in no way incidental. These are fields that deal with encapsulated biological units where agreed-upon scientific theory tells us that the relationships in these systems will be invariant to a wide degree of changes that happen in the world from one year to the next or from one country to the next.

For example, it is implicitly assumed that the growth of corn will not be affected by its expectations of how it will be cut down and processed during the harvest. Corn grows according to the properties of the soil and the sun it receives. Its growth will be independent of its destination in corn flakes or arepas, conditional on those properties. Opioids will inhibit pain in humans, regardless of their political ideology, faith in their governmental institutions, or their love of Shark Tank.

These arguments are not made explicitly, but their validity is absolutely necessary to enable the application of their findings: they create laws defined in relation to the entire range of contextual changes that one might want for prediction- and decision-making and that allows the laws to be applied deductively in a wide array of useful situations.

While the validity of these arguments is taken for granted in contained biological processes, this is simply not the case in the social sciences. This is why Shadish, Cook, and Campbell lay out 19 different threats to construct and external validity of social science experiments. In the case of growing corn, the threats either simply do not apply or are implicitly neutralized through basic scientific understanding of plant growth.

In the case of economics, the outcomes are regularly based on individuals decisions to consume, work, study, invest, or move. The treatments are regularly subjected to a gauntlet of mediating factors and interacting variables that are highly context-dependent and correlated across individuals in a single place and time. The population for which one wants to draw actionable inference is in the future and that alone might make them fundamentally different in a number of relevant ways. Internal validity of a study in economics is not automatically sufficient for actionable inference to take place in a new context.

\subsection{The Danger of Informal Inference}

It might be argued that it is, and should be, up to the sound judgement and expert opinion of the policy maker to determine if a given counterfactual analysis should extrapolate to their context or not. Empirical studies only need to provide internal validity, according to this argument, as the extrapolation is done by experts who know their target domain.

While incorporating expert domain knowledge can only help predictions, we can differentiation between using a formal statistical framework and using an informal framework of judgement. John Stuart Mill \parencite*{mill1884}, reflects on these difference and the dangers of inferring without formal frameworks.

He begins by denying that the only process of inference consists of separately applying induction (particulars to generals) and then deduction (general law to particular context):

\begin{displayquote}
  ``All inference is from particulars to particulars: General propositions are merely registers of such inferences already made, and short formulae for making more: The major premise of a syllogism, consequently, is a formula of this description: and the conclusion is not an inference drawn from the formula, but an inference drawn according to the formula: the real logical antecedent, or premise, being the particular facts from which the general proposition was collected by induction.''  \parencite{mill1884}
\end{displayquote}

In other words, in the process of creating the general law, one has created a series of particular laws, and only once assured that all the particular laws are valid can one be assured of the more general law. He goes on to caution against the direct reasoning of particulars to particulars because it is informal and we are likely to bring our own biases into the process and make mistakes:
%
\begin{displayquote}
``In reasoning from a course of individual observations to some new and unobserved case, which we are but imperfectly acquainted with (or we should not be inquiring into it), and in which, since we are inquiring into it, we probably feel a peculiar interest; there is very little to prevent us from giving way to negligence, or to any bias which may affect our wishes or our imagination, and, under that influence, accepting insufficient evidence as sufficient.'' \parencite{mill1884}
\end{displayquote}

John Stuart Mill argues that formal procedures, such as that implied by the framework of induction and deduction, allow individuals to avoid these biases. In his world, there was no formal procedure for reasoning from particulars to particulars, but there was for reasoning from particulars to general. As such, he recommends the latter as a way to avoid biases of ``wishes'' and ``imagination.''

In an ideal world, the research community has discovered a set of general laws that are invariant to all contexts and the policy maker can apply them, via the process of deduction, to get the desired result in their circumstance. But what if that general law has not yet been discovered? Then the policy maker must look at individual studies (particulars) and attempt to infer a prediction for the result of a similar policy in their context. This is exactly the situation that John Stuart Mill has warned is fertile ground for bias and imagination.

\section{The Origins of Structure}

Economists during the same period as Fisher took a different approach to conceptualizing and thinking about the inference they were doing. Ragnar Frisch, theorizing about macro-dynamic analysis, set out several key ideas relating to the structure of a system and the autonomy of a structure (\cite{Frisch1995}). For Frisch, the ``structure'' of a system was all the characteristics of the phenomena that could be quantitatively described. In his macrodynamic systems, the structure is defined by a set of functional (simultaneous) equations. He then poses the question: what would happen to the system due to an arbitrary change in a single variable? To do so could imply a different ``structure'' than the one which the equations describe, requiring a different set of equations altogether to describe the new system.

\begin{displayquote}
``But when we start speaking of the possibility of a structure different from what it actually is, we have introduced a fundamentally new idea. The big question will now be in what directions should we conceive of a possibility of changing the structure?\ldots To get a real answer we must introduce some fundamentally new information. We do this by investigating what features of our structure are in fact the most autonomous in the sense that they could be maintained unaltered while other features of the structure were changed\ldots So we are led to constructing a sort of super-structure, which helps us to pick out those particular equations in the main structure to which we can attribute a high degree of autonomy in the above sense. The higher this degree of autonomy, the more fundamental is the equation, the deeper is the insight which it gives us into the way in which the system functions, in short, the nearer it comes to being a real explanation. Such relations form the essence of 'theory'.''
\end{displayquote}

This concept, that of ``the essence of theory'' being the discovery of some relationship that is autonomous and invariant to a great degree of changes we can imagine performing to a system, is taken up by Trygve Haavelmo \parencite*{Haavelmo1944}, who writes that:

\begin{displayquote}
``The principal task of economic theory is to establish such relations as might be expected to possess as high a degree of autonomy as possible.''
\end{displayquote}

He then goes on to consider a distinction between the ``invariance'' of a relationship under hypothetical changes in structure versus the ``persistence'' of a relationship under observed changes in structure:

\begin{displayquote}
``\ldots if we always try to form such relations as are autonomous with respect to those changes that are in fact \textit{most likely to occur}, and if we succeed in doing so, then, of course, there will be a very close connection between actual persistence and theoretical degree of autonomy.''
\end{displayquote}

This implies a connection between autonomy and the \textit{type} of changes to which it is invariant \textit{with respect to}. This point is made even more explicitly by Leonid Hurwicz \parencite*{Hurwicz1966}. Similar to his predecessors, his model consists of a system of equations that constrain the state of the world, given a history of states. He calls this system of equations a ``behavior pattern.'' He states that:

\begin{displayquote}
``A great deal of effort is devoted in econometrics and elsewhere to attempts at finding the behavior pattern of an observed configuration\ldots But do we really need the knowledge of the behavior pattern of the configuration?\ldots It will be approached here from the viewpoint of prediction\ldots That is, the word `need' in the above question will be understood as `need' for purposes of prediction.''
\end{displayquote}

% He then goes on to explain that the need is driven by the type of prediction one hopes to make. If it can be assumed that the behavior pattern does not change (i.e. the joint distribution of our variables of interest comes from the same distribution tomorrow as today), then we do not actually need to find the behavioral pattern, as we can predict the state of the world tomorrow based on an expectation of the past.

He then goes on to define what he calls a ``structural form'' as one which is identified and identical across all possible behavior changes that one \textit{needs} to predict within. He stresses that:

\begin{displayquote}
``The most important point is that the concept of structure is relative to the domain of modifications anticipated.''
\end{displayquote}

Thus, there is an inherent and irrevocable connection between what we consider a ``law'' and the degree of changes we require the law to persist across. The law is defined in relation to those changes. Following Hume, the performance of induction is always connected to a specific assumption about the uniformity of nature. Additionally, the exact way in which nature must be uniform, for a particular system under study, is defined by the predictions we need to make with the discovered relations in that system.


\subsection{Invariant Conditionals}

Robert Engle, building on the work of Frisch and Hurwicz regarding invariant/autonomous structures, creates a statistical definition of what he terms ``super exogoneity'' (\cite{Engle1983}).A good review of this historical evolution of autonomy can be found in \cite{Aldrich1989}.

Super exogoneity is defined by Engle as follows. Consider a model in which the ``structure'' (the functional form) of a relationship between outcome $y$ and variable $z$ is parameterized by ``structural'' parameters $\lambda_1, \lambda_2 \in \lambda$. If the joint density implied by the model can be factorized as follows:
%
$$
P(y, z, \lambda) = P(y | z, \lambda_1)P(z | \lambda_2)
$$

and $\lambda_2$ is independent of, hence contains no information about, the structural parameter of interest $\lambda_1$, then the variable $z$ is said to be weakly exogenous to $\lambda_1$. If, additionally, the conditional distribution, $P(y | z, \lambda_1)$ remains invariant to changes in the marginal $p(z)$ (either caused by changes in it's generating process, parameterized by $\lambda_2$, or through other interventions that modify the values themselves), then $z$ is super exogenous.

This definition allows super exogoneity to be refuted by data:

\begin{displayquote}
``It is clear that any assertion concerning super exogeneity is refutable in the data for past changes in $D(z, | X_{t-1}, \lambda_2)$ by examining the behavior of the conditional model for invariance when the parameters of the exogenous process changed...However, super exogeneity for all changes in the distribution of z, must remain a conjecture until refuted, both because nothing precludes agents from simply changing their behavior at a certain instant and because only a limited range of interventions will have occurred in any given sample period.''
\end{displayquote}

This clarifies the difference between weak and super exogoneity: one is observational and the other interventional. Weak exogoneity is an observational characteristic, one must only show that structural parameters $\lambda_2$ provide no information for estimating $\lambda_1$. Super exogoneity implies invariance across any possible intervention which changes the distribution of $z$.

Writing a system in terms of super exogenous variables is akin to finding Frisch's ``super-structure.'' This is the part of the system that stays invariant to a set of allowable modifications: changes in $P(z)$. This is the part of the system that must be known in order to make policy predictions, where $z$ is changes directly or indirectly, whose total causal effect (\cite[in the sense of][]{Pearl2000}) is transmitted to the output variable $y$ through $z$. The existence of such an invariant super-structure is a necessary prerequisite to successfully predict the effects of policy and therefore to successfully inform policy choice from empirical data.

This method of looking in the data for a conditional distribution that is invariant to ``structural'' changes that lead to differences in the marginal distribution of its ``inputs'' has formed the basis of new line of work in statistics and machine learning around causal discovery (\cite{Peters2015, Heinze-deml2017, Rojas-carulla2018}). This line of research, in the terms of Engle, can be thought of as methods for model selection by selecting covariates that are super exogenous to the output in question. I will return to some of these methods in section 4.

Additionally, the connection between weak and super exogoneity is exploited by \cite{Zhang2015}, where the lack of weak exogoneity is used to determine the causal direction in the case of two correlated, unconfounded variables. Zhang's method points to the possibility of tests that exclude the assumptions of covariate shift with data from only a single domain. In other words, a general testing framework that works without testing directly for super exogoneity, which in theory requires proving the invariance of the conditional distribution $P(Y|Z)$ against all arbitrary manipulations of the marginal $P(Z)$.

% They work without making any assumption as to strong or weak exogeneity (\cite[in the sense of][]{Engle1983}) of the covariates, assumptions that are often provided by basic theory in economic contexts. Implications for further research based on such additional assumptions will be returned to in the sequel.

% It should be clear that, even if such a super-structure exists, it is possible that $z$ is either fully or partially unobserved. How will that effect the invariance? Consider again the model with a slight modification (we drop $\lambda_1$ for ease of notation, as we are only interested in the conditions under which the conditional distribution is invariant and thus $\lambda_1$ is static):
% %
% $$
% P(y, z_1, z_2, \lambda) = P(y | z_1, z_2)P(z_1 | z_2, \lambda_2)p(z_2 | \lambda_3)
% $$

% Where we consider a latent variable, $z_2$, which is independent of the policy modification of interest, $\lambda_2$, and whose generating process is controlled by structural parameter $\lambda_3$. We can marginalize out $z_2$ by fixing $\lambda_3 = \ell$:
% %
% $$
% P(y, z_1, z_2, \lambda_2, \lambda_3 = \ell_3) = P(y | z_1, \lambda_3 = \ell_3)P(z_1 | \lambda_2, \lambda_3 = \ell_3)
% $$

% Thus, our conditional is still invariant to modifications to $\lambda_2$, but it is fixed as regards the structural parameter $\lambda_3$ which determined the distribution of the latent variable $z_2$. If the latent variable, $z_2$, was also effected by the structural parameters $\lambda_2$, then one would have to fix that as well in order to get an invariant conditional:
% %
% $$
% P(y | z_1, \lambda_2 = \ell_2, \lambda_3 = \ell_3)
% $$

% It is worth considering the implications of the existence of latent variables in the super-structure that are evident in this simple exercise of probability algebra:

% \begin{enumerate}
% \item If the latent variable is not independent of the set of modifications to which a relationship should be invariant ($P(z | \lambda_2) \neq P(z)$), then the relationship cannot be determined generally.
% \item If the latent variable is independent of the set of modifications to which a relationship should be invariant ($P(z | \lambda_2) = P(z)$, then marginalizing out the latent variable results in an invariant relationship.
% \end{enumerate}

% This requirement is proved rigorously in \cite{Pearl2014}, where they refer to a relationship as ``transportable'' if it is invariant across a set of modifications.

% Given that $z_2$ is latent, it might be difficult, in practice, to know whether or not it is independent of $\lambda_2$: one cannot directly obtain an empirical estimate of $P(z_2)$. What about going in the reverse direction: if one had empirical data across a set of modifications (different values of $\lambda_2$) and one discovered an empirical conditional distribution $P(y | z_1)$ that is invariant across those modifications, does that imply that $P(z_2 | \lambda_2) = P(z_2)$?



\section{Domain Adaptation}

\subsection{Definition and Formulation}

Consider a domain, $\D$, which we define as consisting of a feature space, $\mathcal{X}$, and a marginal distribution $P(X)$ where $X = \{x_1,\ldots,x_n\} \in \mathcal{X}$. A task, $\mathcal{T}$, consists of an outcome space, $\mathcal{Y}$ and a true generating mechanism $f: \mathcal{X} \rightarrow \mathcal{Y}$.

Many techniques of traditional statistics and machine learning make the assumption that the domain and the task are the same in the training data and the prediction context. Transfer learning is the generic name for frameworks that work outside these assumptions, when either the domain, the task, or both are allowed to change.

I will consider the term domain adaptation in the sense of Ben-David \parencite*{Ben-David2006} and Pan and Yang \parencite*{Pan2010}. Under this definition, domain adaptation is a specific form of transfer learning where the task $\mathcal{T}$ is constant and the feature space, $\mathcal{X}$ is the same across all domains. There is a target domain, $\D_T$ from which one has samples $\{x_1,\ldots,x_n\} \in X$ and (one or more) source domain(s), $\D_S$, from which one has tuples $\{(y_1, x_1),\dots,(y_n, x_n)\} \in (\mathcal{Y}, \mathcal{X})$.

The consistency of the task relates to the construct validity in the taxonomy of \cite{Shadish2002}. If the outcome measured in one experiment might be considered to measure a different construct than the outcome measured in another experiment, then the task can not be said to be the same and this problem formulation fails.

Allow $\mathcal{X} = \{\mathcal{W}, \mathcal{Z}\}$, where $\mathcal{W} = \{W_0, W_1\}$ a binary treatment and $\mathcal{Z} = {Z_1,\ldots,Z_P}$ a set of covariates. The assumption of the consistency of the feature space relies on the construct validity of both the treatment and the covariates. If this validity does not hold, if the covariates or the treatment across domains represent a different construct, then this formulation is not valid.

If one considers a random-variable formulation of the true generating mechanism, $f(\cdot)$, then the assumption of task consistency implies the conditional distribution is consistent across tasks, thus $P_S(Y | X) = P_T(Y | X)$. This assumption is referred to as ``covariate shift'', as the only difference between domains $\D_S$ and $\D_T$ is that of the marginal covariate distribution, $P_S(X) \neq P_T(X)$.

Under the assumptions of covariate shift, Shimodaira \parencite*{Shimodaira2000} shows that the optimum likelihood function for a maximum likelihood estimator of $P_T(Y | X)$, given sufficiently large sample size, is obtained by minimizing the weighted loss function trained on labeled data from the source domain and weighted by the ratio $w(x) = \frac{P_T(x)}{P_S(x)}$:
%
$$
\argmin_{\beta} \sum_{i=0}^N -w(x_i) \log P_S(y_i | x_i, \beta)
$$

The use of this weighting ratio, $w(x)$ (referred to as the \textit{importance}) has led to many other importance estimation techniques to deal with covariate shift (\cite{Suigyama2007, Pan2010}).

For the rest of this article, I will focus on the goal of formulating policy prediction problems such that the covariate shift assumption holds. If that assumption holds, the task of policy prediction in a new context can be solved with a relatively straight-forward application of importance estimation techniques.

\subsection{ Domain Adaptation in Econometrics }

The problem of covariate shift has a related, long history in the problem of overcoming sample selection bias in econometrics (\cite{Manski1977}). While the fundamental statistical problem is the same, the applications are specific.

The most obvious economic approach which addresses the problem of external validity is that of structural economics. Heckman \parencite*{Heckman2008} explains that RCTs solve internal validity, while the problem that ``economic policy analysts have to solve daily'' involves more, and not just the generalization of previous experiments but also the prediction of the effects of policies ``never historically experienced,'' which he argues only structural models based on behavioral choices are capable of doing.

The tension between structural and experimental economics is long standing and I will not succeed in resolving it here. This article seeks to lay out a research agenda which is firmly planted within the field of experimental econometrics, however, I do wish to acknowledge that structural econometrics does indeed solve the same problem with a different set of assumptions. While I do not believe that can be easily brushed off, I also do not have the space to address it here.

Hotz et al \parencite*{Hotz2005} provide one of the only canonical models in the experimental econometrics literature, that I am aware of, for extrapolating from a source context with experimental data and measuring predictions in a target context where such data is not available.

Their technical methodology follows \cite{Abadie2006}, using matching with replacement from the target to the source domain to create a bootstrapped, labeled dataset on which to run a regression. This can be seen as a reinvention of importance estimation (\cite{Shimodaira2000, Suigyama2007}). The implication of this choice of techniques, as shown previously, is the assumption of covariate shift. The failure of their technique to work in certain contexts implies a violation of the covariate shift assumptions. They try adding additional personal covariates to their model and find that doing so has little effect on their predictive ability. Their models are linear and additive and they deal with treatment heterogeneity by splitting the sample into two groups, according to a variable previously identified, and running all analyses separately. They predict the outcomes of control and treatment groups separately and introduce t-tests to compare the distributions of the predicted outcomes with that of the actual outcomes. This leads to a potential test of transportability by looking at the t-test of the control group outcomes.

\cite{Gechter2015} builds on the technique of Hotz, but rather than using a statistical test to determine if the model is transportable or not, he creates bounds for prediction in a new domain, using the difference between outcomes for the control groups $P_S(Y|W_0, Z)$ and  $P_T(Y|W_0, Z)$ to create the bounds.

I propose that these methodologies can be improved in the following areas:

\begin{enumerate}
\item A theoretical justification for the technique in terms of the assumptions involved, related to observable and unobservable variables, that may or may not fail in practice.
\item A formal framework for model selection that determines which covariates to condition on and which to marginalize out to enable a transportable prediction.
\item A focus on treatment effects rather than outcomes.
\end{enumerate}

The following sections will be devoted to showing why these features matter and how they can be addressed with recent ideas from both machine learning and econometrics.

% Hock/Imbens - They conduct a test to see whether Y|X is consistent across locations, but this does not say anything about T|X. Indeed, the Y|X they test is linear, thus by assumption all of those variables fall out of the treatment effect and the identification of those variables is not actually important...

% However, if Y|X is consistent in the control, they show that it is also consistent in the treatment, which implies one can compute T|X. By using matching with replacement, they are essentially reweighting the errors by the density ratio of the covariates (show this relation!).

% However if Y|X is not consistent, that shows some other latent variable that is effecting Y. This does not mean the latent variable will effect the treatment, however, it might be additive (as was the assumption with all the other variables!).


\subsection{ Exogoneity and Covariate Shift}

\cite{Pearl2014} offers a full set of conditions in which conditional distributions are transportable. As their setup relies on the do-calculus of Pearl \parencite*{Pearl2000} and it is more than needed for the illustrative purposes here, I leave the reader to follow the references if they are of interest. I will, however, provide a small set of propositions to connect exogoneity and the concept of covariate shift, whose connection to Pearl and Barenboim's derived conditions will be obvious to anyone familiar.

The following propositions will help us understand, in terms of exogoneity, different ways by which the covariate shift assumption can fail.

Consider a source domain, $\D_S$ and a target domain $\D_T$, with feature space defined as $\mathcal{X} = \mathcal{Z} \cup \mathcal{H}$. $Z \in \mathcal{Z}$ is a set of observable covariates, where $P_S(Z) \neq P_T(Z)$, and $H \in \mathcal{H}$ a set of unobservable covariates.

\begin{prop}
  The covariate shift assumption may be violated for $P(Y|Z)$ if the variable set $Z$ is not super exogenous.
\end{prop}

\begin{proof}
  This follows directly from the definition of super exogoneity: a failure of super exogoneity implies that the conditional distribution $P(Y|Z)$ is not invariant to changes in $P(Z)$, which implies that, potentially, $P_T(Y|Z) \neq P_S(Y|Z)$ given the assumption that $P_S(Z) \neq P_T(Z)$.
\end{proof}

\begin{prop}
  The covariate shift assumption holds for $P(Y|Z)$ if the variable set $Z$ is super exogenous and $P_S(Y|Z, H) = P_S(Y | Z)$.
\end{prop}

\begin{proof}
As $Z$ and $H$ encompass all variables and are thus the only possible changes across domains, this follows directly from super exogoneity of $Z$, $P_S(Y|Z, H) = P_S(Y | Z) = P_T(Y | Z)$.
\end{proof}

\begin{prop}
  The covariate shift assumption holds for $P(Y|Z)$ if the variable set $\{ Z \cup H \}$ is super exogenous and $P_S(H|Z) = P_T(H|Z)$.
\end{prop}

\begin{proof}
  Super exogoneity of $\{H \cup Z \}$, given they encompass all variables, implies $P_S(Y|Z,H) = P_T(Y|Z,H)$. Then $P_S(Y|Z) = \int P_S(Y|Z,H)P_S(H|Z) dH = \int P_T(Y|Z,H)P_T(H|Z) dH = P_T(Y|Z)$.
\end{proof}

\begin{prop}
  The covariate shift assumption holds for $P(Y|Z)$ if the variable set $\{ Z \cup H \}$ is super exogenous and $P_S(H|Z) = P_S(H) = P_T(H) = P_T(H|Z)$.
\end{prop}

\begin{proof}
  Super exogoneity of $\{H \cup Z \}$, given they encompass all variables, implies $P_S(Y|Z,H) = P_T(Y|Z,H)$. Then $P_S(Y|Z) = \int P_S(Y|Z,H)P_S(H) dH = \int P_T(Y|Z,H)P_T(H) dH = P_T(Y|Z)$.
\end{proof}

% if H is weakly exogenous and Z is super exogenous, the covariate shift assumption holds given there is no intervention on H:

% $\int P_S(Y|Z,H)P_S(H, \lambda_S)dH = \int P_T(Y|Z,H)P_T(H, \lambda_T) dH$. By assumption of weak exogoneity, $\lambda_2$ is independent of the estimation of $P_S(Y|H) = \int P_S(Y|H,Z)P(Z|H) dZ$

I will provide a concrete example to underscore the relevance of these propositions.

\subsection{An Example Application: Microcredit}

As a motivating example, I will refer to a set of recent microcredit RCTs that have been used as a dataset to examine external validity of individual studies (\cite{Pritchett2016, Meager2018}). The studies were all published in 2015, all carried out all within a few years of each other, and all in different locations: Mexico, Mongolia, India, Bosnia and Herzegovina, Ethiopia, Morocco (\cite{Attanasio2015, Angelucci2015, Augsburg2015, Banerjee2015, Crepon2015, Tarozzi2015}).

All the studies involved some randomization through which individuals were treated with ``greater access'' to microcredit lending, with the exact way in which greater access is defined and the exact terms of the microcredit lending differing across sites. The Bosnian study randomizes at individual level, taking those who were just below the cutoff to being accepted and offering them loans, while the other studies randomize at regional levels, marketing or offering their products in new regions. In line with the frameworks outlined above, before applying any techniques of knowledge transfer between these domains, we must ask ourselves: is the feature space the same? Do these treatments represent the same construct or not? In each case, various output variables were recorded, including household consumption in the months following the intervention and business profits, for which it is easier to argue that they measure the same construct.

Let us assume, for the sake of using this as an example, that the feature space is the same and that variables all measure the same constructs. It is easy to imagine unobservable variables that might effect the profit of the business and additionally might interact with the treatment of access to microcredit. One such unobservable was hypothesized by Abhijit Banerjee \parencite*{Banerjee2011}: he proposes that many of the individuals in these studies who have their own business do not actually romanticize entrepreneurship in the way that the average Schumpeter-loving economist might. Indeed, many would prefer to have salaried jobs than to run their own small business. Given credit, they may not invest very vigorously in their business because they do not actually believe it can grow, nor do they spend any time imagining it growing to be anything more than it is.

Let us call such an unobservable ``Schumpeterianness.'' As it is unobservable, we will refer to it with the variable $H$. This characteristic, however, certainly effects other characteristics that we do observe ($Z$) and might try to include in our predictive model $P(Y|Z)$. For example, the number of past loans or number of previous businesses.

Let $Z = \{ W, Z_2 \}$ where $W \in \{ W_0, W_1\}$ will represent the treatment (some increased availability of microcredit to the individual) and $Z_2$ will consists of the observed variables that proxy Schumpeterianness but do not effect profits in any direct way, thus $P(Y|H,W,Z_2) = P(Y|H,W)$. We will also assume that the treatment, $W$, is randomized (with full participation for the sake of simplicity) such that $P(H|W,Z_2) = P(H|Z_2)$.

It should be clear, based on our propositions, that the covariate shift assumptions can fail based on two conditions that may or may not hold between the source and the target domain:

\begin{enumerate}
\item $P_S(H) = P_T(H)$
\item $P_S(H|Z_2) = P_T(H|Z_2)$
\end{enumerate}

We take each possibility in turn (the technical conditions are taken directly from \cite{Pearl2014} and adapted to this scenario):

\subsubsection*{$P_S(H) = P_T(H)$ and $P_S(H|Z_2) \neq P_T(H|Z_2)$}

In this case, $P_S(Y|W) = \int P_S(Y|W,H)P_S(H) dH = P_T(Y|W)$, thus the invariant condition is given by $P(Y|W)$. In contrast, $P(Y|W,Z_2)$ will not be invariant: $P_S(Y|W,Z_2) = \int P_S(Y|W,H,Z_2)P_S(H|Z_2) dH \neq P_T(Y|W,Z_2)$.

\subsubsection*{$P_S(H) \neq P_T(H)$ and $P_S(H|Z_2) = P_T(H|Z_2)$}

This will provide an outcome that is the reverse of the above: $P_S(Y|W,Z_2) = \int P_S(Y|W,H,Z_2)P_S(H|Z_2) dH = P_T(Y|W,Z_2)$, thus the invariant condition is given by $P(Y|W,Z_2)$. In contrast, $P(Y|W)$ will not be invariant: $P_S(Y|W) = \int P_S(Y|W,H,Z_2)P_S(H) dH \neq P_T(Y|W)$.

\subsubsection*{$P_S(H) = P_T(H)$ and $P_S(H|Z_2) = P_T(H|Z_2)$}

In this case, both the conditionals will be invariant, the proof is the same as above.

\subsubsection*{$P_S(H) \neq P_T(H)$ and $P_S(H|Z_2) \neq P_T(H|Z_2)$}

In this case, neither of the conditionals will be invariant, the proof being again the same as above.
\mbox{}
\\


Given that $H$ is unobserved, the empirical evidence for the distribution of $P(H)$ and $P(H|Z_2)$ across domains is not easily recovered given a single source domain and a single target domain\footnote{It is potentially possible to say something about $P(H|Z_2)$. Under the assumption that $H$ is the direct cause of $Z_2$, without any unobserved confounding variables, we can say that $P(Z_2|H)$ will be invariant and independent from $P(Z_2)$ but $P(H|Z_2)$ will not (\cite[see][for an exposition of this feature which is closely related to weak exogoneity]{Daniusis2010, Sch2012, Peters2017}). Thus, $P(H|Z_2)$ will be invariant across domains if $P_S(Z_2) = P_T(Z_2)$. This is a testable assumption! } Similarly, one might not be able to theoretically prove that there is not another unobserved variable, $H_2$, for which no proxy was measured and whose marginal distribution might differ across domains ($P_S(H_2) \neq P_T(H_2)$). Given data from only one domain, we do not have any clear way to empirically show that $H_2$ does not exist and does not throw off the predictive distribution by a potentially large degree.

A reasonable seeming alternative would then be to try and find labeled data from more than one domain. If one has such data from two domains, $\D_{S1}$ and $\D_{S2}$, then one could directly check the conditional invariance of $P_{S1}(Y|W) = P_{S2}(Y|W)$ and $P_{S1}(Y|W,Z_2) = P_{S2}(Y|W,Z_2)$. Under the assumption that any changes to unobservable variables between $\D_S$  and $\D_T$ will also exist between $\D_{S1}$ and $\D_{S2}$, one can thus be said to be confirming the transportability of the two possible predictive conditional distributions.

This is very close to the idea pursued by Rojas-carulla et al (\cite{Rojas-carulla2018}). They operationalize the testing of invariance of the conditional to it's domain through an independence test between the residuals of a parametric model for the output and the index label of the domain: $P(Y - f(X^*, S)) = P(Y - f(X^*))$, where $S = \{1,2,\ldots,K \}$ for $K$ source domains. They use this constraint to look for a subset of the features $X^* \in X$ for which conditional invariance holds.

Christina Henze-Deml \parencite*{Heinze-deml2017} proposes a similar solution to the problem of domain adaptation in image recognition. Using a series of images in which the same individual, with the same causal characteristics, is captured across multiple domains subject to shifts in orthogonal features. A regularization term is added to the optimization problem of the neural network trained on the source domains. The term penalizes the conditional variance of the prediction $\mathbb{V}\big[ \hat{Y}| g(X) \big]$ for feature representation function $g(\cdot)$ applied to features where the ``causal characteristics'' are known to be the same. Through the regularization, the function $g(\cdot)$ learns a representation for which the conditional distribution of the outcome is invariant across domains.

Thus, the general formulation of these techniques is to consider the problem one of feature representation, where $g: \mathcal{Z} \rightarrow \mathbb{R}^D$ consists of a representation function that might search among variables $Z$ or search over latent representations of variables $Z$ to discover a model that is predictive of the outcome, subject to the constraint of a conditional distribution that is invariant across domains. Quite naturally, as done in both of the above references, one must do this in a way to avoid problems of multiple testing and overfitting either through levels of significance for variable inclusion or cross validation.

\subsection{Treatment Effects}

In both the econometric formulations of domain adaptation as well as those from machine learning, the focus of the literature has been on predicting the outcome itself. In many cases, a policy maker might be more interested in predicting the treatment effect, which can be thought of as a relaxation of the problem of predicting outcomes. Indeed, if one had the predicted outcomes, one would have the treatment effect, but not the other way around.

What implication does treatment effect prediction have for the extension of the frameworks introduced above? Consider the case where the true data generating process of the outcome, $Y$, is linearly separable into two parts:
%
$$
Y(W,Z) = f_t(W,Z_1) + f_e(Z_1, Z_2)
$$

Where $Z = \{ Z_1, Z_2 \}$, the set of observed covariates and $W \in \{ W_0, W_1 \}$ a binary treatment variable, as above. Let $\tau(z)$ be the conditional average treatment effect:

\begin{align*}
\tau(z) &= \mathbb{E} \big[ Y(W_0, Z) - Y(W_1, Z) | Z = z \big] = \tau(z_1)
\end{align*}

Thus, $\tau(\cdot)$ only relies on a subset of $Z_1 \subseteq Z$. Treating $\tau_i$ as a random variable (dropping the explicit dependence on $z_1$) this allows us to achieve the covariate shift assumption with the invariance of the conditional distribution $P(\tau | Z_1)$ across domains, which relies on weakly fewer variables. Given all the threats to transportability outlined by our propositions above, this could be a major benefit in practice.

Estimating the treatment effect, however, comes with its own complications. Due to Holland's \parencite*{Holland1986} \textit{Fundamental problem of Causal Inference}, $\tau_i$ is not actually observed and estimating its conditional distribution relies on additional assumptions (\cite{Firpo2007}). One must also also employ methods designed to find the heterogeneous treatment effect variables that make up $Z_1$ in a way which does not run into multiple testing problems. Luckily, there has been recent interest in developing powerful nonparametric techniques, such as causal trees (\cite{Athey2016}), that perform exactly this function.

Thus, by combining these recent techniques from econometrics and machine learning, I argue that it is both desirable and likely possible to develop techniques to predict the effect of a policy, $W$, on a new domain $\D_T$, given conditional treatment effect distributions, $P_1(\tau | g(Z_1)),\ldots, P_S(\tau | g(Z_1))$, from source domains $\D_S$ and a learned feature representation function $g(\cdot)$ that enforces conditional invariance such that the covariance shift assumption holds and importance estimation methods can be used to predict effects on a target population.




% Given a set of interacting covariates, $\rho \subset \mathcal{X}$, one can determine that covariate shift holds when $P_S(\tau | g(\rho)) = P_T(\tau | g(\rho)$, where $g(\cdot)$ is a feature representation function that removes or transforms variables in $\rho$ to obtain an invariant conditional distribution.




% \cite{Peters2015} develop a technique where, given an outcome variable $Y$, and given the assumption that there exists a conditional distribution of that outcome variable that is invariant across domains ($P(Y | X^*, \lambda) = P(Y | X^*)$) for some set of covariates $X^* \in X$, where $X$ is a set of potential covariates that have been measured, one can discover the subset $X^*$ by exploiting the invariance of the conditional distribution. They do this by representing the conditional distribution by a function $g(X)$, parameterizing the function, and looking for a set of invariant parameters across the domains.


% Weak exogoneity is a concept that relates directly to the autonomy of the mechanism, $f(Z)$, that generates the outcome, to the systems that generate $Z$. This assumption can reasonably be argued, and often is, theoretically in economic systems. One might have reasons to believe that the covariates, $Z$, are determined by a process that is fully independent of the process that controls the outcome.

% Even if weak and super exogoneity hold, that does not imply that the covariate shift assumption will hold across domains.



% Allow $\mathcal{X} = \{ \mathcal{W}, \mathcal{Z}, \Lambda \}$, However, Even if the considered covariates, $X$, are weakly exogenous to the generating mechanism of $Y$, however, the However, Even if the considered covariates, $X$, are weakly exogenous to the generating mechanism of $Y$, however, the where $\Lambda = \{\lambda_1,\dots,\lambda_M\}$ consists of all unobserved variables for which $P_S(\lambda_i) \neq P_T(\lambda_i)$.

% Further, allow for a feature representation function, $g: \mathcal{Z} \rightarrow \mathbb{R}^K$. In its simplest form, this feature representation function can be considered as the decision of which variables from $Z$ to include in the conditional distribution. We will allow the conditional

% %
% $$
% P_j(Y | W, Z, \Lambda) = P_j(Y | W, Z)
% $$

% For $j \in \{S, T\}$.










% I will argue that frameworks for discovering heterogeneous treatment effects, such as causal trees (\cite{Athey2016}), must form a part of any methodology that hopes to transfer treatment effects from one domain to another. Following that, I will show that ideas of conditional invariance that can be traced back to mid-century econometrics can motivate the model selection process and that such motivation exists in some recent research in the field of domain adaptation of machine learning (\cite{Rojas-carulla2018}).

% Banjerjee and Snowberg \parencite*{Snowberg2016} create a formal system of ``speculation'' to create ``falsifiable claims'' of research studies as an attempt to codify the process of generalization and external validity that can be attached to any empirical study from the counterfactual paradigm. The falsifiable claims would presumably be used by other researchers to test the assumptions necessary for external validity of the original findings to hold true.


% Nancy Cartwright and Jeremy Hardie \parencite*{Cartwright2013} lay out an intuitive, qualitative method for predicting the treatment effect of a policy in your context, given evidence from other contexts.


% \cite{Shadish2002} lay out, along with their taxonomy of validities, a taxonomy of ``threats'' to each type of validity. While not a formal framework, per say, by creating the taxonomy they invite researchers to address each threat in turn. If a researcher is able to argue that all the threats to external validity have been addressed and protected against, then one might consider the work of generalizing to be finished. In a sense, they provide a checkbox of things that are worth worrying about. Unfortunately, echoing again Manski's (\parencite*{Manski2008} concern, researchers in applied economics have not systematically adopted a system of discussing threats to external validity.

% GRADE is an example of this type of system applied in healthcare recommendations. It is a system, formalized to bring together expert opinion. It is not a statistical framework that starts from the original data, it starts from PDFs, read and interpreted by humans, and lets them give grades to the evidence based on stated criteria.

% It provides policy recommendation along the guidelines of patient age, setting, and intervention types.

% What is ``high and middle- income country'' -- what happens with countries on the border? this discretization is clearly problematic. But it's a start.


% Tamil Nadu / Bangladesh



% Athey and Imbens \parencite*{Athey2017} similarly reflect on the recent concerns over external validity quoted previously by the likes of Manski, Deaton, and Heckman. In an article aptly titled \textit{The State of Applied Econometrics: Causality and Policy Evaluation}, they lay out three main recent advances in addressing external validity.

% The first advance is that of addressing the concerns about the Local Average Treatment Effect (LATE) measured by instrumental variables (IV) (...). The concerns relate to the generalization of the local effect caused by the variation in the instrument, to the global effect on the entire population (who were potentially not touched by the variation in the instrument in the study).

% The second advance is that of addressing concerns regarding the local nature of regression discontinuity designs (...). These techniques all involve various methods to test whether the effect is only present locally at the discontinuity, or whether the effect is likely to extend to the rest of the sample.

% Both of these techniques can be thought of addressing ``statistical validity'' in the validity taxonomy of Shadish, Cook, and Campbell, that of drawing inferences to the population from the sample. They do not address construct or external validity, they do not provide any information to the policy maker who is considering making a decision in another context.

% The third advance is more salient to the major thrust of external validity: that of combining observational and experimental results.

% Much of the recent econometric literature on external validity focuses on the generalization from local effects to global effects, or from compliers to non-compliers, within the population studied. (EXAMPLES).

% There is, however, a paucity of literature that discusses external validity in the full sense implied by the definitions stated above, that is, valid in another population at another place and time.







% Others have proposed methods for ``augmenting'' counterfactual anaylisis to take external validity into account.


\section{Conclusions}

Banerjee and Duflo \parencite*{Banerjee2014} discuss concerns to experimental validity and generalization. As a primary tool to address external validity of RCTs, they emphasize the need for replication studies. Indeed, they posit what can be thought of as asymptotic theory of domain adaptation for RCTs:

\begin{displayquote}
``If we were prepared to carry out enough experiments in varied enough locations, we could learn as much as we want to know about the distribution of the treatment effects across sites conditional on any given set of covariates.''
\end{displayquote}

Asymptotic theory can be comforting, it's nice to know that the road we are on leads somewhere. However, the key term ``any given set of covariates'' must be restricted for this to be actionable in the finite lifespan of our species. The definition of those covariates comes down to defining the uniformity of nature assumption posed by Hume. If the covariate set is infinite, then we assume nothing regarding the nature of uniformity, and induction is impossible.

How, then, do we define those covariates? Which parts of nature must be uniform and which parts are allowed to change? How many studies are enough studies and does that depend on where on intends to implement a new policy?

I have laid out a research agenda to answer those questions and have shown that:

\begin{enumerate}
\item If the covariate shift assumption holds, importance estimation techniques can be used to predict in new contexts given data in a previous one.
\item In the face of unobservables, the covariate shift assumption must be proven to hold empirically by using labeled data from multiple contexts.
\item If that labeled data is experimental, it can be reasonable to estimate directly the treatment effect, potentially reducing the feature space and increasing the possibility of transportability.
\end{enumerate}




% Nancy Carwright \parencite*{Cartwright2016} presents an example of a policy decision by policy makers in New York City to implement a new program called Opportunity NYC. The program was modelled after a program in Mexico, Opportunidades, which had proved good results in reducing poverty there. Opportunity NYC was implemented in 2007 and shut down in 2010 due its failure to produce the desired effects.

% What went wrong? Should the policy makers in New York simply have known that Mexico is clearly different and no program implemented there should be expected to work in New York? Did the writers of the Opportunidades study have an obligation to investigate and consider the threats to external validity that such a study might have been in danger of?

% The purpose of this article was to argue that each of these cases is wrong:

% Firstly, generalization, causal mechanisms, structure, support factors, etc. are all relative to a set of changes to which they must be invariant. One cannot answer the question about external validity without information about the differences between the source and target contexts.

% Secondly, the difference between New York and Mexico must be measured over a well-defined covariate space in order to make sense of the question of whether it is ``too different.'' A core part of applied economics research must, therefore, consist of determining, for a given intervention, what part of nature must be uniform, what makes up the super-structure, what are the support factors that will change, or similarly, what are the invariant mechanisms. New research from machine learning has shown that the discovery of invariant mechanisms, given muti-domain datasets, is feasible and allows for effective predictions in new domains. Thus, while the data requirement has gone up (more than one study is required to make a prediction in a new context without further assumptions), the ability to give a formal answer to the question of whether it will probably work in New York or whether one has no idea is within our reach and should be pursued.









% \section{Problem Formulation}

% We consider the utility of individual $i$ given as $ u(\pi, X_i)$, where $X_i$ is a set of covariates, potentially unique to the individual, and $\pi$ some policy under question. The utility function is considered common across all individuals, but arbitrarily parameterized by individual covariates $X_i$, thus this assumption is non-restrictive.

% The social planner's problem consists of finding the ``best'' policy, where best can be defined in a number of different ways: greatest average utility, paretto optimal, least inequality, etc:
% %
% $$
% \pi^* = h(\bm{\pi}, X)
% $$

% Where $h(\cdot)$ represents a function that determines the best policy given a set of policies $\bm{\pi}$ and the population covariates $X$. We will make two assumptions to make this problem tractable for our framework:

% \begin{enumerate}
% \item We replace the utility function, $u(\cdot)$, with a surrogate outcome $Y(\cdot)$ of interest to the social planner. The surrogate is considered linear separable into two parts:
% %
% $$
% Y(\pi, X) = Y_1(\pi, Z) + Y_2(W)
% $$

% Where $Z \cup W = X$.

% \item In comparing two policies, a policy maker can pick one based solely on the individual differences in outcomes between the two policies:
% %
% $$
% \pi^* = h(Y(\pi_1, Z_1) - Y(\pi_0, Z_1),...,Y(\pi_1, Z_N) - Y(\pi_0, Z_N))
% $$
% When $\bm{\pi} = \{\pi_0, \pi_1\}$.
% \end{enumerate}

% The first assumption is non-restrictive, as $W$ can be null. The second assumption is a restriction on the type of policy objectives the social planner can pursue, however many naturally desireable objectives can still be pursued in realistic scenarios\footnote{The decision maker can directly pursue the greatest average utility and objectives based on paretto optimality, but cannot directly pursue least inequality, as an example. In realistic scenarios, however, reduction of inequality can be pursued via the maximization of utility (likely replaced with income) for those who currently are in the lowest x\% and minimization of income for those who are currently in the highest y\%, assuming that the class of policies, $\bm{\pi}$ is reasonable enough so that one doesn't accidentally flip the relationship and maintain the inequality!}.

% Finally, we will restrict our policy space, $\bm{\pi}$, to one of finite policies\footnote{Considering continuous policy spaces should be considered of great interest. This restriction is done to simplify the problem here, not because it is necessarily desireable. Embedding policies into continuous spaces is, additionally, not a solved problem and is a natural prerequisite to such a formulation being useful in practise.} and, without loss of generalization, consider the situation of two policies, $\{\pi_0, \pi_1\}$, where $\pi_0$ can be thought of as the null policy, or control, and $\pi_1$ the treatment under question. This allows us to rewrite:
% %
% $$
% \pi^* = h(\tau_1,...,\tau_n)
% $$

% Where $\tau_i \coloneqq Y(\pi_1, Z_{i}) - Y(\pi_0, Z_{i})$, the individual treatment effect of the policy. At the point in time in which the social planner is deciding on a policy, $\tau_i$ is unknown and can be formulated as a random variable.

% Consider $\hat{P}(\tau_i | X, \lambda = \lambda_T)$ to be a predictive probability distribution of the treatment effect of the policy on individual $i$ in a target context (domain) described by $\lambda = \lambda_T$\footnote{The parameter $\lambda$ is the same as $\lambda_2$ in Engle's formulation. We drop the subscript for ease of notation here.}. The planner knows the covariates for each individual, $X_i$, and estimates the predictive probability distribution via a prediction function, $f(\cdot)$, trained on a set of 4-tuples, $(Y_i, X_i, \pi_i, \lambda_i)$ where $\lambda_i \in \Lambda_S$ and $\Lambda_S$ consists of a series of domains in which the policy maker has labeled data and $\lambda_T \not \in \Lambda_S$. Thus, the decision function that picks the ``best'' policy must now accept probability distributions:
% %
% $$
% \pi^* = h(\hat{P}(\tau_1 | X, \lambda_T),...,\hat{P}(\tau_n | X, \lambda_T))
% $$

% The treatment effect can be reasonably replaced by its expectation, $ \mathbb{E} [ \tau_i | X, \lambda_T ] $, if the policy objective is maximizing average (or total) welfare. If the policy objective incorporates any sort of aversion to negative outcomes (``let's not completely ruin any of our citizens' lives''), risk aversion, or concerns about paretto efficiency, then the expectation of this random variable will not be sufficient information to pick the best policy $\pi^*$.

% We will assume that every policy object can be translated into a loss function that aligns with the decision strategy $h(\cdot)$:
% %
% $$
% \ell(\tau, \hat{P}(\tau | X, \lambda_T))
% $$

% Unfortunately, this loss function is technically impossible to calculate, even after the fact, due to Rubin's ``fundamental problem of causal inference'', the fact that $\tau_i$ is unobserved. While the formulation and minimization such a specific loss function would be desirable in the case of different policies, I will, for this article, pose a general formulation that will asymptotically minimize any reasonable loss function: the existence of a consistent estimator for $P(\tau | X)$ such that
% %
% $$
% \hat{P}(\tau | X, \lambda \in \Lambda_S) \rightarrow P(\tau | X, \lambda = \lambda_T)
% $$

% Such an estimator can be created if we have the following two conditions:

% \begin{enumerate}
% \item There exists a consistent estimator $\hat{P}(\tau | X, \lambda = \lambda_S) \rightarrow P(\tau | X, \lambda = \lambda_S)$ for any single domain defined by $\lambda_S$.
% \item $P(\tau | X, \lambda_S) = P(\tau | X) \ \forall \ X, \ P(X | \lambda_T) > 0$. The conditional is invariant across the differences that exist between the source domains and the target domain, within the support of the target domain.
% \end{enumerate}

% It should be noted that this general framework works for any predictive output that might be used in the policy choice. Thus, if the policy choice relies only on the average treatment effect, $h(\mathbb{E} \big[ \tau | X, \lambda_T \big])$, then one only needs a consistent estimator for $\mathbb{E} \big[ \tau | X, \lambda_S \big]$ and that the estimator is invariant across domains, $\mathbb{E} \big[ \tau | X, \lambda_S \big] = \mathbb{E} \big[ \tau | X \big]$, in order to have a consistent estimator that will asymptotically minimize the loss function related that policy choice.


% \section{Quantile Treatment Effects}

% rank preservation, if not, it might in and of itself be of interest to the policy maker, see blah for discussion

% We will apply this observation, that a QTE defined in this way is identical to the individual QTE with an assumption of rank preservation, and calculate the latter directly. Within each leaf, we will preserve the rank of the individuals and calculate the difference. Extra individuals in either group are compared to their closest match from the other group.



% \section{Picking causal covariates vs. creating them}

% Often the direct causes of the outcome in consideration are not observed. They are, as such, latent variables. These latent variables might be proxied by an observable (cause) or be caused by an observable. The two factors have very different effects:

% $P(X |Z)$ should be invariant in the case where the latent variable causes its observable proxy, thus, for the transformation to go from $\hat{Z} = f(X)$, it is $f^{-1}$ that must be invariant across domains, requiring a generative model of X from the latent variables to be learned.

% On the contrary, if $P(Z | X)$ is invariant, then a model which generates the latent variables which in turn cause the effect, $Y$, must be invariant. It should be noted that $P(Z)$ is not needed to be constant across domains, as long as the two conditionals can be recoverd.



% \section{Forests as Nonparametric Estimator of the Conditional Treatment Effect Distribution}

% Quantile treatment effect with the assumption of rank persistence.

% It follows that any consistent estimator of ATE is an consistent estimator of quantiles and the distribution as well.

% Causal forests are a consistent estimator of ATE (\cite{})

% Random forests are a consistent estimator of the full distribution (\cite{})


% \section{Distributional Invariance Across Domains}





% Much of the domain adaptation literature from machine learning focuses on the case where covariate shift holds. Covariate shift describes the situation where $P(Y|X)$ is assumed to be consistent and $P(X)$ is changing across domains. As we are in the situation of discovering the $x \in X$ which is causal for $Y$, we cannot simply make that assumption. Many of the techniques in domain adaptation, however, are developed for situations in which the model is learned on a latent space, thus the reweighting of $X$ consists of a transformation and checking of distributional differences between source domains. One technique for minimizing distances between distributions is that of minimizing the Wasserstein distance. This distance measure has grown in popularity recently due to it's advantageous qualities for optimization: it maintains a gradient even when distributions lack mutual positive support. Thus, the gradient is computable even when distributions are very different from one another.

% These two ideas can easily be combined, that of Wasserstein distance minimization for enforcing distributional similarity and that of regularization of the conditional differences to enforce domain-invariant conditionals.

% Thus, the optimization problem will contain a regularization term:

% $$
% R(\cdot) = \sum_{i,j \in S} W(P(\tau | X_i, \lambda_i),P(\tau | X_i, \lambda_j))
% $$

% Which sums the pairwise Wasserstein distance, $W(\cdot)$, of the conditional treatment effect distribution across different domains, $S$.


% Each split seeks to minimize the variance of the treatment effect ,








% \section{Empirical Performance}

% Data consists of 5 recent RCT studies performed on the impact of micro-credit programs on business profit and consumption in developing countries.

% With 5 different domains, I test my method by training on 4 source domains and predicting on the 5th. Results are compared with the estimated ATE implied by the studies, as well as with a ``baseline'' prediction using the coefficient of the CATE as estimated by a linear model in a single domain, comparable to coefficients reported in the original studies.


% Rojas-Carulla's technique, by assuming linearity, sidesteps the problem of support (a linear function has full support, as one just extrapolates the line into infinity, regardless of the support of the data from which the function was learned).

% This problem of support, and the realities of non-linearities, would seem to be important to economic work, where there can be substantial changes in distributions between countries and time periods across potential interacting variables.


% Despite the restrictions it places on policy objectives, we will restrict our scope, for the current article, to that of averaging total welfare. It would be valuable to extend these ideas to other policy objectives and determine loss functions that apply to them.

% We define the empirical conditional average treatment effect as:
% %
% $$
% \tau(x) \coloneqq \frac{1}{N} \sum_N \mathbb{E}[ \Delta U | X]
% $$

% If the goal is to pick the correct policy, one can consider a loss from picking the incorrect policy equal to the total lost utility:
% %
% $$
% \ell = \mathbbm{1} \big[ \int \hat{\tau}(x)p(x)dx < 0 \big] \int \tau(x)p(x)dx \\
% $$



% a reasonable loss function would be to minimize the squared loss between the empirical conditional average treatment effect and the predicted:
% %
% $$
% \ell = \sum_i \bigg( \tau(X_i) - \hat{\tau}(X_i) \bigg)^2
% $$


% This creates difficulties for the estimation of any empirical distribution of treatment effects (see \cite{Imbens2004} for a review).


% Rank preservation assumption necessary to estimate the quantile treatment effect...!!! Also necessary to estimate the distribution.


% THEOREM??
% Given two domains, $S, D$ and random variables $Y,X$, if $P_S(Y | X) = P_T(Y | X)$, then there exists no other variable $Z$ such that $P_S(Z) \neq P_D(Z)$ and $P_S(Y | X) \nCI Z$ and $P_D(Y | X) \nCI Z$ and $P_S(Y | X, Z) = P_T(Y | X, Z)$.




% information theoretic -- minimize entropy while being invariant.
% MSE - minimize variance while be invariant
% ??? What else ???
%



% The expectation of the predictive treatment distribution for a give individual might, under certain policy objectives (increasing average utility), contain all the relevant information from the predictive treatment distribution. In this case, we might want to calculate the expected treatment effect (ETE).


% The expected treatment effect (ETE) for domain $D_t$ is not the same as the average treatment effect (ATE) on domain $D_s$ unless blah blah blah.

% The expected treatment effect is decomposed into:

% $$
% ETE  = ATE + ETD(S, D)
% $$


% The invariant average treatment effect (IATE)

% The invariant treatment effect distribution (ITED)



% It was clear to these economic theorists that human nature, and the all the factors that affect the functioning of an economy, must in some ways be uniform, but in many other ways changeable. Economic theory, any equations or models intended to represent invariant laws, must endogenous the changeable nature of human behavior, institutions, and technology (\cite{Marshack1950}).



% So what does a formal system that takes into account external validity look like?

% It must, in some way, deal with the assumption that Hume terms ``the uniformity of nature.'' Trivially, nature will not be uniform in every way, and clearly, we need it to be uniform in some way (Goodman   ). It is thus of interest to us to acknowledge that assumption and seek out frameworks that endogenize the ways in which nature is uniform and the ways in which it changes.

% One can think of a general causal law as a function. The output of the function is the output of interest. The function is parameterized by every variable that effects the output. A framework that endogenizes the process of learning the ways in which nature is uniform is nothing more than a process of finding a single function that is works consistently across time and space\footnote{This idea of an "invariant" mechanism that works across environments is one fundamental definition of causality that has been much discussed in the literature (...).}. If we think of this function nonparametrically, then finding this function consists of two steps: 1) deciding which variables should parameterize the function and 2) collecting output data across the joint support of all those input variables.

% Collecting data across the entire joint support of the inputs can potentially be extremely difficult in the context of the social sciences. If these input covariates are elements that change slowly over time and space with a high level of autocorrelation (such as the majority of traits that can be considered ``cultural'' or ``social'' in nature), then it will take a very long time, potentially infinite, until we have collected enough data and ``converged'' on the full joint distribution that defines our general causal law \footnote{A useful comparison is to think of Markov chain monte carlo. We might, as a society, be forced to slowly explore locally the evolution of cultural traits that have a large potential impact on outcomes of interest, and if there is some randomness in the system, then this path-dependent process will eventually cover the entire support of the distribution, but it is not clear that it will happen before the sun implodes.}.

% This is not to say that the recover of general causal laws is not of interest or possible in economics. It is definitely of interest and may be possible. Treating the problem as one of nonparametric statistical estimation might require an overwhelming amount of data, but attacking the problem with a hand-written parametric model doesn't require any data whatsoever, supposing one has the right theory.



% While RCTs, the counterfactual model, and the internal validity they aim to ensure are not sufficient for the process of scientific inference, that does not in any way detract from them being useful and even necessary. \cite{Deaton2018} provide a strong overview of the many ways in which RCTs are valuable even without any claims of external validity.

% While they echo the concerns of \cite{Heckman2007}, that RCTs and, more generally, the counterfactual approach does \textit{not} easily lead to invariant (autonomous) relationships, they also stress that RCTs can be useful even if they do not generalize to other populations. ``Demanding external validity is unhelpful because it expects too much of an RCT while undervaluing its potential contribution.''





% \section{From Particulars to Particulars}

% It should be clear that, while having general causal laws, as parameterized functions, for every outcome of interest would be extremely useful, it is also extremely difficult. While much of theoretical economics often consists of the creation of such functions, they are rarely seriously defended as general causal laws to be used in the way we use Newton's laws in Physics: functions that predict with such accuracy that we regularly trust our lives to them.

% There are two useful simplifications we can make in this process of inference to make it more feasible.

% The first is to focus, not on all inputs that determine an outcome, but on one input in particular. We can think of this input as a treatment or a policy. Our function will thus not seek to predict the output given all inputs, but rather, seek to predict the change in the output given our treatment, ceteris paribus. This ceteris paribus clause allows us to ignore all additively separable causes of our effect of interest. Our function will thus be parameterized by our treatment along with a set of ``interacting covariates'' \footnote{These interacting covariates are nothing more than the support factors in the language of Nancy Cartwright or connecting-principles in the language of Nelson Goodman.} and the output of the function will be a level shift in our output variable of interest.

% It should be clear that this first simplification is not in any way novel, it also happens when we move from structural to reduced-form models in economics.


% \section{Autonomy}


% Here we can realize the difference between what is possible to falsify, and what is not.

% There might be a difference between a relation that is autonomous and one that is persistent and the relation might be that the autonomous relation is invariant to hypothetical variations never-before-seen while the persistent one can only be said to be invariant to variations for which we have data.

% However, the former cannot actually be falsified.

% That being said, it is important to keep the difference in mind. Bertrand Russel's chicken had discovered a persistent relationship but not one that was autonomous to a hypothetical change in the behavior of its farmer.

% I will focus on the case where, when we want to discuss claims that are falsifiable with current data on the world, persistent becomes the more actionable version of autonomous.


% It should be clear in the application of this proposed method that one must make an argument, via an appeal to common sense and behavioral theory, that for any latent variables that are: A) likely to have an interative effect and B) likely to vary in its distribution from domain to domain, the following will also hold: C) the variable has varied in its distribution across the various source domains.


% \section{Structure is Relative}

% >>> This can potentially be used -- instead of forcing variable sets with invariant properties, the effect of potential latent variables can be bound by the change in output distribution!!



\printbibliography

\end{document}
