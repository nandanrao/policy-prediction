@article{Abadie2006,
abstract = {Matching estimators for average treatment effects are widely used in evaluation research despite the fact that their large sample properties have not been established in many cases. The absence of formal results in this area may be partly due to the fact that standard asymptotic expansions do not apply to matching estimators with a fixed number of matches because such estimators are highly nonsmooth functionals of the data. In this article we develop new methods for analyzing the large sample properties of matching estimators and establish a number of new results. We focus on matching with replacement with a fixed number of matches. First, we show that matching estimators are not N1/2-consistent in general and describe conditions under which matching estimators do attain N1/2-consistency. Second, we show that even in settings where matching estimators are N1/2-consistent, simple matching estimators with a fixed number of matches do not attain the semiparametric efficiency bound. Third, we provide a consistent estimator for the large sample variance that does not require consistent nonparametric estimation of unknown functions. Software for implementing these methods is available in Matlab, Stata, and R.},
author = {Abadie, Alberto and Imbens, Guido W.},
doi = {10.1111/j.1468-0262.2006.00655.x},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/j.1468-0262.2006.00655.x.pdf:pdf},
isbn = {1468-0262},
issn = {0012-9682},
journal = {Econometrica},
keywords = {Matching estimators,average treatment effects,potential outcomes,selection on observables,unconfoundedness},
number = {1},
pages = {235--267},
title = {{Large Sample Properties of Matching Estimators}},
url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1468-0262.2006.00655.x/abstract        },
volume = {74},
year = {2006}
}
@article{Allcott2015,
author = {Allcott, Hunt},
doi = {10.1093/qje/qjv015.Advance},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/qjv015.pdf:pdf},
pages = {1117--1165},
title = {{Site Selection Bias in Program Evaluation}},
year = {2015}
}
@article{Angrist2011,
abstract = {This paper develops a covariate-based approach to the external validity of instrumental variables (IV) estimates. Assuming that differences in observed complier characteristics are what make IV estimates differ from one another and from parameters like the effect of treatment on the treated, we show how to construct estimates for new subpopulations from a given set of covariate-specific LATEs. We also develop a reweighting procedure that uses the traditional overidentification test statistic to define a population for which a given pair of IV estimates has external validity. These ideas are illustrated through a comparison of twins and sex-composition IV estimates of the effects childbearing on labor supply.},
author = {Angrist, Joshua D. and Fern{\'{a}}ndez-Val, Iv{\'{a}}n},
doi = {10.1017/CBO9781139060035.012},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/w16566.pdf:pdf},
isbn = {9781139060035},
journal = {Advances in Economics and Econometrics: Tenth World Congress Volume 3, Econometrics},
pages = {401--434},
title = {{ExtrapoLATE-ing: External validity and overidentification in the LATE framework}},
year = {2011}
}
@article{Aronow2015,
author = {Aronow, Peter M and Samii, Cyrus},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/SSRN-id2224964.pdf:pdf},
keywords = {causal inference,external validity,multiple regression,observational studies,random-},
title = {{Does Regression Produce Representative Estimates of Causal Effects}},
volume = {06520},
year = {2015}
}
@article{Athey2017,
author = {Athey, Susan and Imbens, Guido W},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/jep.31.2.3.pdf:pdf},
number = {2},
pages = {3--32},
title = {{The State of Applied Econometrics: Causality and Policy Evaluation}},
volume = {31},
year = {2017}
}
@article{Banerjee2014,
abstract = {Randomized experiments have become a popular tool in development economics research and have been the subject of a number of criticisms. This paper reviews the recent literature and discusses the strengths and limitations of this approach in theory and in practice. We argue that the main virtue of randomized experiments is that, owing to the close collaboration between researchers and implementers, they allow the estimation of parameters that would not otherwise be possible to evaluate. We discuss the concerns that have been raised regarding experiments and generally conclude that, although real, they are often not specific to experiments. We conclude by discussing the relationship between theory and experiments.},
author = {Banerjee, Abhijit V. and Duflo, Esther},
doi = {10.1146/annurev.economics.050708.143235},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/approach.pdf:pdf},
isbn = {9780300169409},
journal = {Field Experiments and Their Critics: Essays on the Uses and Abuses of Experimentation in the Social Sciences},
keywords = {development economics,program,randomized experiments},
pages = {78--114},
title = {{The experimental approach to development economics}},
year = {2014}
}
@article{Snowberg2016,
author = {Banerjee, Abhijit and Chassang, Sylvain and Snowberg, Erik},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/w22167.pdf:pdf},
title = {{Decision Theoretic Approaches to Experiment Desig}},
year = {2016}
}
@article{Banerjee2016,
abstract = {The objective of this case study was to obtain some first-hand information about the functional consequences of a cosmetic tongue split operation for speech and tongue motility. One male patient who had performed the operation on himself was interviewed and underwent a tongue motility assessment, as well as an ultrasound examination. Tongue motility was mildly reduced as a result of tissue scarring. Speech was rated to be fully intelligible and highly acceptable by 4 raters, although 2 raters noticed slight distortions of the sibilants /s/ and /z/. The 3-dimensional ultrasound demonstrated that the synergy of the 2 sides of the tongue was preserved. A notably deep posterior genioglossus furrow indicated compensation for the reduced length of the tongue blade. It is concluded that the tongue split procedure did not significantly affect the participant's speech intelligibility and tongue motility.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Banerjee, Abhijit and Duflo, Esther and Kremer, Michael},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1011.1669v3},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/the-influence-of-rcts-on-developmental-economics-research-and-development-policy.pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
journal = {Working Paper},
pages = {1--76},
pmid = {15003161},
title = {{The Influence of Randomized Controlled Trials on Development Economics Research and on Development Policy}},
url = {http://www.americanbanker.com/issues/179{\_}124/which-city-is-the-next-big-fintech-hub-new-york-stakes-its-claim-1068345-1.html{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/15003161{\%}5Cnhttp://cid.oxfordjournals.org/lookup/doi/10.1093/cid/cir991{\%}5Cnhttp://www.scielo},
volume = {d},
year = {2016}
}
@article{Bisbee2017,
abstract = {We investigate whether local average treatment effects (LATE's) can be extrapolated to new settings. We extend the analysis and framework of Dehejia, Pop-Eleches, and Samii (2015), which examines the external validity of the Angrist-Evans (1998) reduced-form natural experiment of having two first children of the same sex on the probability of an incremental child and on mother's labor supply. We estimate Angrist and Evans's (1998) same-sex instrumental variable strategy in 139 country-year censuses using data from the Integrated Public Use Micro Sample International. We compare each country-year's LATE, as a hypothetical target, to the LATE extrapolated from other country-years (using the approach suggested by Angrist and Fernandez-Val 2010). Paralleling our findings in Dehejia, Pop-Eleches, and Samii (2015), we find that with a sufficiently large reference sample, we extrapolate the treatment effect reasonably well, but the degree of accuracy depends on the extent of covariate similarity between the target and reference settings. Our results suggest that – at least for our application – there is hope for external validity.},
author = {Bisbee, James and Dehejia, Rajeev and Pop-Eleches, Cristian and Samii, Cyrus},
doi = {10.1086/691280},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/691280.pdf:pdf},
issn = {0734-306X},
journal = {Journal of Labor Economics},
number = {S1},
pages = {S99--S147},
title = {{Local Instruments, Global Extrapolation: External Validity of the Labor Supply–Fertility Local Average Treatment Effect}},
volume = {35},
year = {2017}
}
@article{Braver2014,
author = {Braver, Sanford L and Thoemmes, Felix J and Rosenthal, Robert},
doi = {10.1177/1745691614529796},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/1745691614529796.pdf:pdf},
keywords = {1 proved their perspicacity,daniel,effect-size heterogeneity,future nobel prize winner,kahneman and amos tversky,meta-analysis,over 40 years ago,replication,statistical intuition},
title = {{Continuously Cumulating Meta-Analysis and Replicability}},
year = {2014}
}
@article{Buchanan2018,
abstract = {{\textcopyright} 2018 The Royal Statistical Society and Blackwell Publishing Ltd. Results obtained in randomized trials may not easily generalize to target populations. Whereas in randomized trials the treatment assignment mechanism is known, the sampling mechanism by which individuals are selected to participate in the trial is typically not known and assuming random sampling from the target population is often dubious. We consider an inverse probability of sampling weighted (IPSW) estimator for generalizing trial results to a target population. The IPSW estimator is shown to be consistent and asymptotically normal. A consistent sandwich-type variance estimator is derived and simulation results are presented comparing the IPSW estimator with a previously proposed stratified estimator. The methods are then utilized to generalize results from two randomized trials of human immunodeficiency virus treatment to all people living with the disease in the USA.},
author = {Buchanan, Ashley L. and Hudgens, Michael G. and Cole, Stephen R. and Mollan, Katie R. and Sax, Paul E. and Daar, Eric S. and Adimora, Adaora A. and Eron, Joseph J. and Mugavero, Michael J.},
doi = {10.1111/rssa.12357},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/Buchanan{\_}et{\_}al-2018-Journal{\_}of{\_}the{\_}Royal{\_}Statistical{\_}Society{\_}{\_}Series{\_}A{\_}(Statistics{\_}in{\_}Society).pdf:pdf},
issn = {1467985X},
journal = {Journal of the Royal Statistical Society. Series A: Statistics in Society},
keywords = {Causal inference,External validity–generalizability,Human immunodeficiency virus–acquired immune defic,Inverse probability weights,Randomized controlled trial,Target population},
number = {4},
pages = {1193--1209},
title = {{Generalizing evidence from randomized trials using inverse probability of sampling weights}},
volume = {181},
year = {2018}
}
@article{Cartwright2013,
abstract = {Over the last twenty or so years, it has become standard to require policy makers to base their recommendations on evidence. That is now uncontroversial to the point of triviality—of course, policy should be based on the facts. But are the methods that policy makers rely on to gather and analyze evidence the right ones? In Evidence-Based Policy, Nancy Cartwright, an eminent scholar, and Jeremy Hardie, who has had a long and successful career in both business and the economy, explain that the dominant methods which are in use now—broadly speaking, methods that imitate standard practices in medicine like randomized control trials—do not work. They fail, Cartwright and Hardie contend, because they do not enhance our ability to predict if policies will be effective. The prevailing methods fall short not just because social science, which operates within the domain of real-world politics and deals with people, differs so much from the natural science milieu of the lab. Rather, there are principled reasons why the advice for crafting and implementing policy now on offer will lead to bad results. Current guides in use tend to rank scientific methods according to the degree of trustworthiness of the evidence they produce. That is valuable in certain respects, but such approaches offer little advice about how to think about putting such evidence to use. Evidence-Based Policy focuses on showing policymakers how to effectively use evidence. It also explains what types of information are most necessary for making reliable policy, and offers lessons on how to organize that information.},
author = {Cartwright, Nancy and Hardie, Jeremy},
doi = {10.5860/choice.50-5831},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/Evidence-Based-Policy-A-Practical-Guide-to-Doing-It-Better.pdf:pdf},
isbn = {978-0-19-984160-8},
issn = {0009-4978},
journal = {Choice Reviews Online},
number = {10},
pages = {50--5831--50--5831},
title = {{Evidence-based policy: a practical guide to doing it better}},
volume = {50},
year = {2013}
}
@article{Chassang2010,
author = {Chassang, Sylvain and i Miquel, Gerard Padro and Snowberg, Erik},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/w16343.pdf:pdf},
keywords = {To Read},
title = {{Selective Trials and Information Production in Randomized Controlled Experiments}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.163.5979{\&}rep=rep1{\&}type=pdf{\%}5Cnpapers3://publication/uuid/81A54BE7-4ACE-4037-8DB5-66BEF6A134A0},
year = {2010}
}
@article{Deaton2010,
abstract = {There is currently much debate about the effectiveness of foreign aid and about what kind of projects can engender economic development. There is skepticism about the ability of econometric analysis to resolve these issues or of development agencies to learn from their own experience. In response, there is increasing use in development economics of randomized controlled trials (RCTs) to accumulate credible knowledge of what works, without overreliance on questionable theory or statistical methods. When RCTs are not possible, the proponents of these methods advocate quasi- randomization through instrumental variable (IV) techniques or natural experiments. I argue that many of these applications are unlikely to recover quantities that are useful for policy or understanding: two key issues are the misunderstanding of exogeneity and the handling of heterogeneity. I illustrate from the literature on aid and growth. Actual randomization faces similar problems as does quasi-randomization, notwithstanding rhetoric to the contrary. I argue that experiments have no special ability to produce more credible knowledge than other methods, and that actual experiments are frequently subject to practical problems that undermine any claims to statistical or epistemic superiority. I illustrate using prominent experiments in development and elsewhere. As with IV methods, RCT-based evaluation of projects, without guidance from an understanding of underlying mechanisms, is unlikely to lead to scientific progress in the understanding of economic development. I welcome recent trends in development experimentation away from the evaluation of projects and toward the evaluation of theoretical mechanisms. (JEL C21, F35, O19)},
author = {Deaton, Angus},
doi = {10.1257/jel.48.2.424},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/deaton instruments randomization learning about development jel 2010.pdf:pdf},
issn = {0022-0515},
journal = {Journal of Economic Literature},
month = {jun},
number = {2},
pages = {424--455},
title = {{Instruments, Randomization, and Learning about Development}},
url = {http://pubs.aeaweb.org/doi/10.1257/jel.48.2.424},
volume = {48},
year = {2010}
}
@article{Deaton2018,
author = {Deaton, Angus and Cartwright, Nancy},
doi = {10.1016/j.socscimed.2017.12.005},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/1-s2.0-S0277953617307359-main.pdf:pdf},
issn = {0277-9536},
journal = {Social Science {\&} Medicine},
keywords = {Balance,Bias,Economic development,External validity,Health,Precision,RCTs,Transportation of results},
number = {October 2017},
pages = {2--21},
publisher = {Elsevier},
title = {{Understanding and misunderstanding randomized controlled trials}},
url = {https://doi.org/10.1016/j.socscimed.2017.12.005},
volume = {210},
year = {2018}
}
@article{Flores2013,
abstract = {We study the effectiveness of nonexperimental strategies in adjusting for comparison group differences when using data from several programs, each implemented at a different location, to compare their effect if implemented at alternative locations. First, we adjust for individual characteristics differences simultaneously across all groups using unconfoundedness-based and conditional difference-in-difference methods for multiple treatments. Second, we adjust for differences in local economic conditions and stress their role after program participation. Our results show that it is critical to have sufficient overlap across locations in both dimensions and illustrate the difficulty of adjusting for local economic conditions that differ greatly across locations. {\textcopyright} 2013 by the President and Fellows of Harvard College and the Massachusetts Institute of Technology.},
author = {Flores, Carlos A. and Mitnik, Oscar A.},
doi = {10.1162/REST_a_00373},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/Flores{\_}Mitnik{\_}Comparing{\_}Treatments.pdf:pdf},
issn = {15309142},
journal = {Review of Economics and Statistics},
number = {5},
pages = {1691--1707},
title = {{Comparing treatments across labor markets: An assessment of nonexperimental multiple-treatment strategies}},
volume = {95},
year = {2013}
}
@article{Gechter2015,
author = {Gechter, Michael},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/Gechter{\_}Generalizing{\_}Social{\_}Experiments.pdf:pdf},
number = {2008},
pages = {1--50},
title = {{Generalizing the Results from Social Experiments : Theory and Evidence from Mexico and India}},
year = {2015}
}
@article{Heckman2008,
author = {Heckman, James J},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/w13934.pdf:pdf},
title = {{Econometric Causality}},
year = {2008}
}
@article{Heckman1995,
abstract = {This paper analyzes the method of social experiments. The assumptions that justify the experimental method are exposited. Parameters of interest in evaluating social programs are discussed. The authors show how experiments sometimes serve as instrumental variables to identify program impacts. The most favorable case for experiments ignores variability across persons in response to treatments received and assumes that mean impacts of a program are the main object of interest in conducting an evaluation. Experiments do not identify the distribution of program gains unless additional assumptions are maintained. Evidence on the validity of the assumptions used to justify social experiments is presented.},
author = {Heckman, James J and Smith, Jeffrey A},
doi = {10.1257/jep.9.2.85},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/2138168.pdf:pdf},
issn = {0895-3309},
journal = {Journal of Economic Perspectives},
number = {2},
pages = {85--110},
title = {{Assessing the Case for Social Experiments}},
volume = {9},
year = {1995}
}
@article{Heckman2007,
abstract = {This chapter relates the literature on the econometric evaluation of social programs to the literature in statistics on "causal inference". In it, we develop a general evaluation framework that addresses well-posed economic questions and analyzes agent choice rules and subjective evaluations of outcomes as well as the standard objective evaluations of outcomes. The framework recognizes uncertainty faced by agents and ex ante and ex post evaluations of programs. It also considers distributions of treatment effects. These features are absent from the statistical literature on causal inference. A prototypical model of agent choice and outcomes is used to illustrate the main ideas. We formally develop models for counterfactuals and causality that build on Cowles Commission econometrics. These models anticipate and extend the literature on causal inference in statistics. The distinction between fixing and conditioning that has recently entered the statistical literature was first developed by Cowles economists. Models of simultaneous causality were also developed by the Cowles group, as were notions of invariance to policy interventions. These basic notions are updated to nonlinear and nonparametric frameworks for policy evaluation more general than anything in the current statistical literature on "causal inference". A formal discussion of identification is presented and applied to clearly formulated choice models used to evaluate social programs. {\textcopyright} 2007 Elsevier B.V. All rights reserved.},
author = {Heckman, James J. and Vytlacil, Edward J.},
doi = {10.1016/S1573-4412(07)06070-9},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/Econometric Evaluation Social Programs.pdf:pdf},
isbn = {9780444532008},
issn = {15734412},
journal = {Handbook of Econometrics},
keywords = {causal models,counterfactuals,identification,policy evaluation,policy invariance,structural models},
number = {SUPPL. PART B},
pages = {4779--4874},
title = {{Econometric Evaluation of Social Programs, Part I: Causal Models, Structural Models and Econometric Policy Evaluation}},
volume = {6},
year = {2007}
}
@article{Hotz2005,
abstract = {We investigate the problem of predicting the average effect of a new training program using experiences with previous implementations. There are two principal complications in doing so. First, the population in which the new program will be implemented may differ from the population in which the old program was implemented. Second, the two programs may differ in the mix or nature of their components, or in their efficacy across different sub-populations. The first problem is similar to the problem of non-experimental evaluations. The ability to adjust for population differences typically depends on the availability of characteristics of the two populations and the extent of overlap in their distributions. The ability to adjust for differences in the programs themselves may require more detailed data on the exact treatments received by individuals than are typically available. This problem has received less attention, although it is equally important for the prediction of the efficacy of new programs. To investigate the empirical importance of these issues, we compare four experimental Work INcentive demonstration programs implemented in the mid-1980s in different parts of the U.S. We find that adjusting for pre-training earnings and individual characteristics removes many of the differences between control units that have some previous employment experience. Since the control treatment is the same in all locations, namely embargo from the program services, this suggests that differences in populations served can be adjusted for in this sub-population. We also find that adjusting for individual characteristics is more successful at removing differences between control group members in different locations that have some employment experience in the preceding four quarters than for control group members with no previous work experience. Perhaps more surprisingly, our ability to predict the outcomes of trainees after adjusting for individual characteristics is similar. We surmise that differences in treatment components across training programs are not sufficiently large to lead to substantial differences in our ability to predict trainees' post-training earnings for many of the locations in this study. However, in the sub-population with no previous work experience there is some evidence that unobserved heterogeneity leads to difficulties in our ability to predict outcomes across locations for controls.},
author = {Hotz, V. Joseph and Imbens, Guido W. and Mortimer, Julie H.},
doi = {10.1016/j.jeconom.2004.04.009},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/1-s2.0-S0304407604000806-main.pdf:pdf},
issn = {03044076},
journal = {Journal of Econometrics},
keywords = {Efficacy,Heterogeneity,Prediction,Training programs},
number = {1-2 SPEC. ISS.},
pages = {241--270},
title = {{Predicting the efficacy of future training programs using past experiences at other locations}},
volume = {125},
year = {2005}
}
@article{Imbens2014,
author = {Imbens, Guido},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/3648.pdf:pdf},
pages = {1--8},
title = {{COMMENTS ON: “UNDERSTANDING AND MISUNDERSTANDING RANDOMIZED CONTROLLED TRIALS” BY CARTWRIGHT AND DEATON GUIDO}},
year = {2014}
}
@book{Lehmann1998,
abstract = {This second, much enlarged edition by Lehmann and Casella of Lehmann's classic text on point estimation maintains the outlook and general style of the first edition. All of the topics are updated. An entirely new chapter on Bayesian and hierarchical Bayesian approaches is provided, and there is much new material on simultaneous estimation. Each chapter concludes with a Notes section which contains suggestions for further study. The book is a companion volume to the second edition of Lehmann's "Testing Statistical Hypotheses". E.L. Lehmann is Professor Emeritus at the University of California, Berkeley. He is a member of the National Academy of Sciences and the American Academy of Arts and Sciences, and the recipient of honorary degrees from the University of Leiden, The Netherlands, and the University of Chicago. George Casella is the Liberty Hyde Bailey Professor of Biological Statistics in The College of Agriculture and Life Sciences at Cornell University. Casella has served as associate editor of The American Statistician, Statistical Science and JASA. He is currently the Theory and Methods Editor of JASA. Casella has authored two other textbooks (Statistical Inference, 1990, with Roger Berger and Variance Components, 1992, with Shayle A. Searle and Charles McCulloch). He is a fellow of the IMS and ASA, and an elected fellow of the ISI.Also available:E.L. Lehmann, Testing Statistical Hypotheses Second Edition, Springer-Verlag New York, Inc., ISBN 0-387-949194.},
author = {Lehmann, E L and Casella, G},
booktitle = {Design},
doi = {10.2307/1270597},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/E L Lehaman.pdf:pdf},
isbn = {0387985026},
issn = {00401706},
number = {3},
pages = {589},
pmid = {3087590},
title = {{Theory of Point Estimation , Second Edition Springer Texts in Statistics}},
url = {http://www.amazon.com/dp/0387985026},
volume = {41},
year = {1998}
}
@article{Lesko2018,
author = {Lesko, Catherine R and Buchanan, Ashley L and Westreich, Daniel and Edwards, Jessie K},
doi = {10.1097/EDE.0000000000000664.Generalizing},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/nihms862564.pdf:pdf},
isbn = {0000000000000},
number = {4},
pages = {553--561},
title = {{Generalizing study results: a potential outcomes perspective}},
volume = {28},
year = {2018}
}
@article{Manski2013,
abstract = {Manski argues that public policy is based on untrustworthy analysis. Failing to account for uncertainty in an uncertain world, policy analysis routinely misleads policy makers with expressions of certitude. Manski critiques the status quo and offers an innovation to improve both how policy research is conducted and how it is used by policy makers.},
author = {Manski, Charles F.},
doi = {10.4159/harvard.9780674067547},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/Public-Policy-in-an-Uncertain-World-Analysis-and-Decisions.pdf:pdf},
journal = {Public Policy in an Uncertain World},
title = {{Public Policy in an Uncertain World}},
year = {2013}
}
@article{Meager2018,
author = {Meager, Rachael},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/Meager-Distributional-Effects-Aggregation-November-2018-external-appendices.pdf:pdf},
title = {{Aggregating Distributional Treatment Effects : A Bayesian Hierarchical Analysis of the Microcredit Literature}},
year = {2018}
}
@article{Pearl2018,
author = {Pearl, Judea and Bareinboim, Elias},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/r478.pdf:pdf},
keywords = {external validity,generalizability,selection bias,transportability},
number = {November},
pages = {1--5},
title = {{Commentary: EPIDEMIOLOGY MS{\#} EDE18-0227 A note on "Generalizability of Study Results"}},
volume = {1750807},
year = {2018}
}
@article{Pearl2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1503.01603v1},
author = {Pearl, Judea and Bareinboim, Elias and Mar, M E},
doi = {10.1214/14-STS486},
eprint = {arXiv:1503.01603v1},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/1503.01603.pdf:pdf},
keywords = {Experimental design, generalizability, causal effe,and phrases,ap-,causal,effects,experimental design,external validity,generalizability,means that we usually,most studies are conducted,plying the results elsewhere,with the intention of},
number = {4},
pages = {579--595},
title = {{External Validity : From Do-Calculus to Transportability Across Populations}},
volume = {29},
year = {2014}
}
@article{Peters2018,
abstract = {{\textcopyright} The Author(s) 2018. Published by Oxford University Press on behalf of the International Bank for Reconstruction and Development / THE WORLD BANK. All rights reserved. When properly implemented, Randomized Controlled Trials (RCT) achieve a high degree of internal validity. Yet, if an RCT is to inform policy, it is critical to establish external validity. This paper systematically reviews all RCTs conducted in developing countries and published in leading economic journals between 2009 and 2014 with respect to how they deal with external validity. Following Duflo, Glennerster, and Kremer (2008), we scrutinize the following hazards to external validity: Hawthorne effects, general equilibrium effects, specific sample problems, and special care in treatment provision. Based on a set of objective indicators, we find that the majority of published RCTs does not discuss these hazards and many do not provide the necessary information to assess potential problems. The paper calls for including external validity dimensions in a more systematic reporting on the results of RCTs. Thismay create incentives to avoid overgeneralizing findings and help policymakers to interpret results appropriately.},
author = {Peters, J{\"{o}}rg and Langbein, J{\"{o}}rg and Roberts, Gareth},
doi = {10.1093/wbro/lkx005},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/lkx005.pdf:pdf},
issn = {15646971},
journal = {World Bank Research Observer},
number = {1},
pages = {34--64},
title = {{Generalization in the tropics-development policy, randomized controlled trials, and external validity}},
volume = {33},
year = {2018}
}
@article{Pritchett2017,
author = {Pritchett, Lant},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/ed1b75ed0076011aa93f37672d30393834fa.pdf:pdf},
pages = {1--9},
title = {{“The Evidence” About “What Works” in Education: Graphs to Illustrate External Validity and Construct Validity}},
year = {2017}
}
@article{Pritchett2013,
abstract = {In this paper we examine how policymakers and practitioners should interpret the impact evaluation literature when presented with conflicting experimental and non-experimental estimates of the same intervention across varying contexts. We show three things. First, as is well known, non-experimental estimates of a treatment effect comprise a causal treatment effect and a bias term due to endogenous selection into treatment. When non-experimental estimates vary across contexts any claim for external validity of an experimental result must make the assumption that (a) treatment effects are constant across contexts, while (b) selection processes vary across contexts. This assumption is rarely stated or defended in systematic reviews of evidence. Second, as an illustration of these issues, we examine two thoroughly researched literatures in the economics of education — class size effects and gains from private schooling — which provide experimental and non-experimental estimates of causal effects from the same context and across multiple contexts.  We show that the range of “true” causal effects in these literatures implies OLS estimates from the right context are, at present, a better guide to policy than experimental estimates from a different context. Third, we show that in important cases in economics, parameter heterogeneity is driven by economy- or institution-wide contextual factors, rather than personal characteristics, making it difficult to overcome external validity concerns through estimation of heterogeneous treatment effects within a single localized sample. We conclude with recommendations for research and policy, including the need to evaluate programs in context, and avoid simple analogies to clinical medicine in which “systematic reviews” attempt to identify best-practices by putting most (or all) weight on the most “rigorous” evidence with no allowance for context.},
author = {Pritchett, Lant and Sandefur, Justin},
doi = {10.2139/ssrn.2364580},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/context-matters-for-size{\_}1.pdf:pdf},
journal = {SSRN Electronic Journal},
keywords = {causal inference,external validity,policy evaluation,treatment effects},
number = {August 2013},
title = {{Context Matters for Size: Why External Validity Claims and Development Practice Don't Mix}},
year = {2013}
}
@article{Pritchett2016,
author = {Pritchett, Lant and Sandefur, Justin},
doi = {10.1257/aer.p20151016},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/aer.p20151016.pdf:pdf},
issn = {0002-8282},
journal = {American Economic Review},
month = {may},
number = {5},
pages = {471--475},
title = {{Learning from Experiments when Context Matters}},
url = {http://eds.b.ebscohost.com.ezproxy.lib.usf.edu/eds/pdfviewer/pdfviewer?vid=5{\&}sid=aef21225-20b8-458e-842b-e860243ea5f7{\%}40sessionmgr104{\&}hid=104 http://pubs.aeaweb.org/doi/10.1257/aer.p20151016},
volume = {105},
year = {2015}
}
@article{Rosenzweig2019,
author = {Rosenzweig, Mark R. and Udry, Christopher},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/SSRN-id3392657.pdf:pdf},
title = {{External Validity in a Stochastic World: Evidence from Low-Income Countries}},
year = {2019}
}
@book{Shadish2002,
author = {Shadish, William R and Cook, Thomas D and Campbell, Donald T.},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/Experimental-and-Quasi-Experimental-Designs-for-Generalized-Causal-Inference.pdf:pdf},
isbn = {0395615569},
title = {{Experimental and Designs for Generalized Causal Inference}},
year = {2002}
}
@article{Vivalt2017,
abstract = {Impact evaluations aim to predict the future, but they are rooted in particular contexts and to what extent they generalize is an open and important question. We exploit a new data set of results on a wide variety of interventions and find more heterogeneity than in other literatures. This has implications for how evidence is generated and used to inform policy.},
author = {Vivalt, Eva},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/How-Much-Can-We-Generalize.pdf:pdf},
journal = {Working Paper, Australian National University},
title = {{How Much Can We Generalize from Impact Evaluations ?}},
year = {2017}
}
