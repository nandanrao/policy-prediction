@article{Anderson2005,
abstract = {Theil, Basmann, and Sargan are often credited with the development of the two-stage least squares (TSLS) estimator of the coefficients of one structural equation in a simultaneous equations model. However, Anderson and Rubin had earlier derived the asymptotic distribution of the limited information maximum likelihood (LIML) estimator by finding the asymptotic distribution of what is essentially the TSLS estimator. {\textcopyright} 2005 Elsevier B.V. All rights reserved.},
author = {Anderson, T. W.},
doi = {10.1016/j.jeconom.2004.09.012},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Anderson - 2005 - Origins of the limited information maximum likelihood and two-stage least squares estimators.pdf:pdf},
issn = {03044076},
journal = {Journal of Econometrics},
keywords = {Asymptotic distributions,Limited information maximum likelihood estimator,Two-stage least squares estimator},
number = {1},
pages = {1--16},
title = {{Origins of the limited information maximum likelihood and two-stage least squares estimators}},
volume = {127},
year = {2005}
}
@article{Arjovsky2019,
abstract = {We introduce Invariant Risk Minimization (IRM), a learning paradigm to estimate invariant correlations across multiple training distributions. To achieve this goal, IRM learns a data representation such that the optimal classifier, on top of that data representation, matches for all training distributions. Through theory and experiments, we show how the invariances learned by IRM relate to the causal structures governing the data and enable out-of-distribution generalization.},
archivePrefix = {arXiv},
arxivId = {1907.02893},
author = {Arjovsky, Martin and Bottou, L{\'{e}}on and Gulrajani, Ishaan and Lopez-Paz, David},
eprint = {1907.02893},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Arjovsky et al. - 2019 - Invariant Risk Minimization.pdf:pdf},
pages = {1--30},
title = {{Invariant Risk Minimization}},
url = {http://arxiv.org/abs/1907.02893},
year = {2019}
}
@article{Bareinboim2014,
author = {Bareinboim, Elias},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bareinboim - 2014 - Recovering from Selection Bias in Causal and Statistical Inference.pdf:pdf},
keywords = {Reasoning under Uncertainty},
number = {July},
pages = {339--341},
title = {{Recovering from Selection Bias in Causal and Statistical Inference}},
year = {2014}
}
@article{Bareinboim2016,
author = {Bareinboim, Elias and Pearl, Judea},
doi = {10.1073/pnas.1510507113},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bareinboim, Pearl - 2016 - Causal inference and the data-fusion problem.pdf:pdf},
title = {{Causal inference and the data-fusion problem}},
year = {2016}
}
@article{Bareinboim2016a,
abstract = {We review concepts, principles, and tools that unify current approaches to causal analysis and attend to new challenges presented by big data. In particular, we address the problem of data fusionâ€”piecing together multiple datasets collected under heterogeneous conditions (i.e., different populations, regimes, and sampling methods) to obtain valid answers to queries of interest. The availability of multiple heterogeneous datasets presents new opportunities to big data analysts, because the knowledge that can be acquired from combined data would not be possible from any individual source alone. However, the biases that emerge in heterogeneous environments require new analytical tools. Some of these biases, including confounding, sampling selection, and cross-population biases, have been addressed in isolation, largely in restricted parametric models. We here present a general, nonparametric framework for handling these biases and, ultimately, a theoretical solution to the problem of data fusion in causal inference tasks.},
author = {Bareinboim, Elias and Pearl, Judea},
doi = {10.1073/pnas.1510507113},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bareinboim, Pearl - 2016 - Causal inference and the data-fusion problem(2).pdf:pdf},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
number = {27},
pages = {7345--7352},
title = {{Causal inference and the data-fusion problem}},
volume = {113},
year = {2016}
}
@article{Besserve2018,
abstract = {The postulate of independence of cause and mechanism (ICM) has recently led to several new causal discovery algorithms. The interpretation of independence and the way it is utilized, however, varies across these methods. Our aim in this paper is to propose a group theoretic framework for ICM to unify and generalize these approaches. In our setting, the cause-mechanism relationship is assessed by perturbing it with random group transformations. We show that the group theoretic view encompasses previous ICM approaches and provides a very general tool to study the structure of data generating mechanisms with direct applications to machine learning.},
archivePrefix = {arXiv},
arxivId = {1705.02212},
author = {Besserve, Michel and Shajarisales, Naji and Sch{\"{o}}lkopf, Bernhard and Janzing, Dominik},
eprint = {1705.02212},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Besserve et al. - 2018 - Group invariance principles for causal generative models.pdf:pdf},
journal = {International Conference on Artificial Intelligence and Statistics, AISTATS 2018},
pages = {557--565},
title = {{Group invariance principles for causal generative models}},
year = {2018}
}
@article{Breiman2001,
abstract = {There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commit-ment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current prob-lems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.},
author = {Breiman, Leo},
doi = {10.1214/ss/1009213726},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Breiman - 2001 - Statistical modeling The two cultures.pdf:pdf},
issn = {08834237},
journal = {Statistical Science},
number = {3},
pages = {199--215},
title = {{Statistical modeling: The two cultures}},
volume = {16},
year = {2001}
}
@article{Bridel2001,
author = {Bridel, Pascal and Hoover, Kevin D},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bridel, Hoover - 2001 - Centre Walras - Pareto , University of Lausanne.pdf:pdf},
isbn = {0521452171},
pages = {369--372},
title = {{Centre Walras - Pareto , University of Lausanne}},
year = {2001}
}
@book{Cartwrighta,
author = {Cartwright, Nancy},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cartwright - Unknown - Hunting Causes and Using Them.pdf:pdf},
isbn = {9780521860819},
title = {{Hunting Causes and Using Them}}
}
@article{Cartwright,
author = {Cartwright, Nancy and Montuschi, Eleonora},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cartwright, Montuschi - Unknown - Philosophy of Social Science A New Introduction.pdf:pdf},
title = {{Philosophy of Social Science: A New Introduction}}
}
@article{Castelletti2018,
abstract = {A Markov equivalence class contains all the Directed Acyclic Graphs (DAGs) encoding the same conditional independencies, and is represented by a Completed Partially Directed Acyclic Graph (CPDAG), also named Essential Graph (EG). We approach the problem of model selection among noncausal sparse Gaussian DAGs by directly scoring EGs, using an objective Bayes method. Specifically , we construct objective priors for model selection based on the Fractional Bayes Factor, leading to a closed form expression for the marginal likelihood of an EG. Next we propose a Markov Chain Monte Carlo (MCMC) strategy to explore the space of EGs using sparsity constraints, and illustrate the performance of our method on simulation studies, as well as on a real dataset. Our method provides a coherent quantification of inferential uncertainty, requires minimal prior specification , and shows to be competitive in learning the structure of the data-generating EG when compared to alternative state-of-the-art algorithms.},
author = {Castelletti, Federico and Consonni, Guido and Vedova, Marco L.Della and Peluso, Stefano},
doi = {10.1214/18-BA1101},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Castelletti et al. - 2018 - Learning Markov equivalence classes of Directed Acyclic Graphs An objective bayes approach.pdf:pdf},
issn = {19316690},
journal = {Bayesian Analysis},
keywords = {Bayesian model selection,CPDAG,Essential graph,Fractional Bayes factor,Graphical model},
number = {4},
pages = {1231--1256},
title = {{Learning Markov equivalence classes of Directed Acyclic Graphs: An objective bayes approach}},
volume = {13},
year = {2018}
}
@article{Cox2007,
abstract = {Statistical aspects of causality are reviewed in simple form and the impact of recent work discussed. Three distinct notions of causality are set out and implications for densities and for linear dependencies explained. The importance of appreciating the possibility of effect modifiers is stressed, be they intermediate variables, background variables or unobserved confounders. In many contexts the issue of unobserved confounders is salient. The difficulties of interpretation when there are joint effects are discussed and possible modifications of analysis explained. The dangers of uncritical conditioning and marginalization over intermediate response variables are set out and some of the problems of generalizing conclusions to populations and individuals explained. In general terms the importance of search for possibly causal variables is stressed but the need for caution is emphasized. /// On fait une revue critique de la causalit{\'{e}} statistique. On presente trois definitions de la causalit{\'{e}} et on discute les consequences pour l'analyse statistique et l'interpretation. CR - Copyright {\&}{\#}169; 2004 International Statistical Institute (ISI)},
author = {Cox, D.R. and Wermuth, Nanny},
doi = {10.1111/j.1751-5823.2004.tb00237.x},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cox, Wermuth - 2007 - Causality a Statistical View.pdf:pdf},
journal = {International Statistical Review},
keywords = {able,chain block graph,counterfactual,explanation,instrumental vari-,interaction,markov graph,observational study,overview,regression analysis,surrogate variable,unit-treatment additivity,unobserved confounder},
number = {3},
pages = {285--305},
title = {{Causality: a Statistical View}},
volume = {72},
year = {2007}
}
@article{Daniusis2010,
abstract = {We consider two variables that are related to each other by an invertible function. While it has previously been shown that the dependence structure of the noise can provide hints to determine which of the two variables is the cause, we presently show that even in the deterministic (noise-free) case, there are asymmetries that can be exploited for causal inference. Our method is based on the idea that if the function and the probability density of the cause are chosen independently, then the distribution of the effect will, in a certain sense, depend on the function. We provide a theoretical analysis of this method, showing that it also works in the low noise regime, and link it to information geometry. We report strong empirical results on various real-world data sets from different domains.},
author = {Daniusis, Povilas and Janzing, Dominik and Mooij, Joris and Zscheischler, Jakob and Steudel, Bastian and Zhang, Kun and Sch{\"{o}}lkopf, Bernhard},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Daniusis et al. - 2010 - Inferring deterministic causal relations.pdf:pdf},
isbn = {9780974903965},
journal = {Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence, UAI 2010},
pages = {143--150},
title = {{Inferring deterministic causal relations}},
year = {2010}
}
@article{Didelez2005,
author = {Didelez, Vanessa and Dawid, A Philip and Geneletti, Sara},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Didelez, Dawid, Geneletti - 2005 - Direct and Indirect Effects of Sequential Treatments.pdf:pdf},
number = {1993},
title = {{Direct and Indirect Effects of Sequential Treatments}},
year = {2005}
}
@article{Drton2017,
author = {Drton, Mathias and Maathuis, Marloes H},
doi = {10.1146/annurev-statistics-060116-053803},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Drton, Maathuis - 2017 - Structure Learning in Graphical Modeling.pdf:pdf},
keywords = {bayesian network,graphical model,markov random field,model selection,multivariate statistics,network reconstruction},
number = {October 2016},
pages = {1--29},
title = {{Structure Learning in Graphical Modeling}},
year = {2017}
}
@article{Engle1983,
abstract = {The effects of firm pension plan provisions on the retirement decisions of older employees are analyzed. The empirical results are based on data from a large firm, with a typical defined benefit pension plan. The "option value" of continued work is the central feature of the analysis. Estimation relies on a retirement decision rule that is close in spirit to the dynamic programming rule but is considerably less complex than a compre- hensive implementation of that rule, thus greatly facilitating the numerical analysis},
author = {Engle, Robert F and Hendry, David F and Richard, Jean-francois},
doi = {10.2307/1911990},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Engle, Hendry, Richard - 1983 - Exogeneity.pdf:pdf},
issn = {00129682},
journal = {Econometrica},
month = {mar},
number = {2},
pages = {277},
title = {{Exogeneity}},
url = {https://www.jstor.org/stable/1911990?origin=crossref},
volume = {51},
year = {1983}
}
@article{Fisher1935,
author = {Fisher, R.A.},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fisher - 1935 - The Design of Experiments.pdf:pdf},
issn = {00218790},
title = {{The Design of Experiments}},
year = {1935}
}
@book{Fisher1925,
abstract = {Contains revisions of probability formulas and treatment of correlations. Harvard Book List (edited) 1955 94 (PsycINFO Database Record (c) 2010 APA, all rights reserved)},
author = {Fisher, Ra},
booktitle = {Biological monographs and manuals},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fisher - 1925 - Fisher RA Statistical methods for research workers. Edinburgh Genesis Publishing, Oliver and Boyd.pdf:pdf},
isbn = {0050021702},
number = {V},
pages = {xv, 356 p.},
title = {{Fisher RA: Statistical methods for research workers. Edinburgh: Genesis Publishing, Oliver and Boyd;}},
url = {http://psychclassics.yorku.ca/Fisher/Methods},
year = {1925}
}
@article{Gelman2005,
abstract = {Analysis of variance (ANOVA) is an extremely important method in exploratory and confirmatory data analysis. Unfortunately, in complex problems (e.g., split-plot designs), it is not always easy to set up an appropriate ANOVA. We propose a hierarchical analysis that automatically gives the correct ANOVA comparisons even in complex scenarios. The inferences for all means and variances are performed under a model with a separate batch of effects for each row of the ANOVA table. We connect to classical ANOVA by working with finite-sample variance components: fixed and random effects models are characterized by inferences about existing levels of a factor and new levels, respectively. We also introduce a new graphical display showing inferences about the standard deviations of each batch of effects. We illustrate with two examples from our applied data analysis, first illustrating the usefulness of our hierarchical computations and displays, and second showing how the ideas of ANOVA are helpful in understanding a previously fit hierarchical model. {\textcopyright} Institute of Mathematical Statistics, 2005.},
archivePrefix = {arXiv},
arxivId = {arXiv:math/0508530v1},
author = {Gelman, Andrew and Tjur, Tue and McCullagh, Peter and Hox, Joop and Hoijtink, Herbert and Zaslavsky, Alan M.},
doi = {10.1214/009053604000001048},
eprint = {0508530v1},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gelman et al. - 2005 - Discussion paper analysis of variance - Why it is more important than ever.pdf:pdf},
isbn = {0883423060000},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {ANOVA,Bayesian inference,Fixed effects,Hierarchical model,Linear regression,Multilevel model,Random effects,Variance components},
number = {1},
pages = {1--53},
pmid = {827961},
primaryClass = {arXiv:math},
title = {{Discussion paper analysis of variance - Why it is more important than ever}},
volume = {33},
year = {2005}
}
@article{Granger1969,
abstract = {There occurs on some occasions a difficulty in deciding the direction of causality between two related variables and also whether or not feedback is occurring. Testable definitions of causality and feedback are proposed and illustrated by use of simple two-variable models. The important problem of apparent instantaneous causality is discussed and it is suggested that the problem often arises due to slowness in recording information or because a sufficiently wide class of possible causal variables has not been used. It can be shown that the cross spectrum between two variables can be decomposed into two parts, each relating to a single causal arm of a feedback situation. Measures of causal lag and causal strength can then be constructed. A generalisation of this result with the partial cross spectrum is suggested. 1.},
author = {Granger, Clive J. W.},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Granger - 1969 - Investigating Causal Relations by Econometric Models and Cross-spectral Methods Authors ( s ) C . W . J . Granger Publi.pdf:pdf},
journal = {Econometrica},
number = {3},
pages = {424--438},
title = {{Investigating Causal Relations by Econometric Models and Cross-spectral Methods Authors ( s ): C . W . J . Granger Published by : The Econometric Society Stable URL : http://www.jstor.org/stable/1912791 Accessed : 25-03-2016 19 : 26 UTC Your use of the JS}},
volume = {37},
year = {1969}
}
@article{He2008,
author = {He, Yang-bo},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He - 2008 - Active Learning of Causal Networks with Intervention Experiments and Optimal Designs.pdf:pdf},
keywords = {active learning,alence class,causal networks,directed acyclic graphs,intervention,markov equiv-,optimal design,structural learning},
pages = {2523--2547},
title = {{Active Learning of Causal Networks with Intervention Experiments and Optimal Designs}},
volume = {9},
year = {2008}
}
@article{Heckman2015,
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Heckman, James J. and Pinto, Rodrigo},
doi = {10.1080/07474938.2014.944466},
eprint = {NIHMS150003},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Heckman, Pinto - 2015 - Econometric Mediation Analyses Identifying the Sources of Treatment Effects from Experimentally Estimated Produc.pdf:pdf},
isbn = {6176321972},
issn = {0747-4938},
journal = {Econometric Reviews},
keywords = {epiblast,gfp fusion,histone h2b-,icm,lineage specification,live imaging,mouse blastocyst,pdgfr $\alpha$,primitive endoderm},
month = {feb},
number = {1-2},
pages = {6--31},
pmid = {1000000221},
title = {{Econometric Mediation Analyses: Identifying the Sources of Treatment Effects from Experimentally Estimated Production Technologies with Unmeasured and Mismeasured Inputs}},
url = {http://www.tandfonline.com/doi/abs/10.1080/07474938.2014.944466},
volume = {34},
year = {2015}
}
@article{Heinze-Deml2018,
abstract = {Graphical models can represent a multivariate distribution in a convenient and accessible form as a graph. Causal models can be viewed as a special class of graphical models that represent not only the distribution of the observed system but also the distributions under external interventions. They hence enable predictions under hypothetical interventions, which is important for decision making. The challenging task of learning causal models from data always relies on some underlying assumptions. We discuss several recently proposed structure learning algorithms and their assumptions, and we compare their empirical performance under various scenarios.},
archivePrefix = {arXiv},
arxivId = {arXiv:1706.09141v1},
author = {Heinze-Deml, Christina and Maathuis, Marloes H. and Meinshausen, Nicolai},
doi = {10.1146/annurev-statistics-031017-100630},
eprint = {arXiv:1706.09141v1},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Heinze-Deml, Maathuis, Meinshausen - 2018 - Causal Structure Learning.pdf:pdf},
issn = {2326-8298},
journal = {Annual Review of Statistics and Its Application},
month = {mar},
number = {1},
pages = {371--391},
title = {{Causal Structure Learning}},
url = {http://www.annualreviews.org/doi/10.1146/annurev-statistics-031017-100630},
volume = {5},
year = {2018}
}
@article{Heinze-deml2017,
abstract = {When training a deep neural network for image classification, one can broadly distinguish between two types of latent features of images that will drive the classification. We can divide latent features into (i) "core" or "conditionally invariant" features {\$}X{\^{}}\backslashtext{\{}core{\}}{\$} whose distribution {\$}X{\^{}}\backslashtext{\{}core{\}}\backslashvert Y{\$}, conditional on the class {\$}Y{\$}, does not change substantially across domains and (ii) "style" features {\$}X{\^{}}{\{}\backslashtext{\{}style{\}}{\}}{\$} whose distribution {\$}X{\^{}}{\{}\backslashtext{\{}style{\}}{\}} \backslashvert Y{\$} can change substantially across domains. Examples for style features include position, rotation, image quality or brightness but also more complex ones like hair color, image quality or posture for images of persons. Our goal is to minimize a loss that is robust under changes in the distribution of these style features. In contrast to previous work, we assume that the domain itself is not observed and hence a latent variable. We do assume that we can sometimes observe a typically discrete identifier or "{\$}\backslashmathrm{\{}ID{\}}{\$} variable". In some applications we know, for example, that two images show the same person, and {\$}\backslashmathrm{\{}ID{\}}{\$} then refers to the identity of the person. The proposed method requires only a small fraction of images to have {\$}\backslashmathrm{\{}ID{\}}{\$} information. We group observations if they share the same class and identifier {\$}(Y,\backslashmathrm{\{}ID{\}})=(y,\backslashmathrm{\{}id{\}}){\$} and penalize the conditional variance of the prediction or the loss if we condition on {\$}(Y,\backslashmathrm{\{}ID{\}}){\$}. Using a causal framework, this conditional variance regularization (CoRe) is shown to protect asymptotically against shifts in the distribution of the style variables. Empirically, we show that the CoRe penalty improves predictive accuracy substantially in settings where domain changes occur in terms of image quality, brightness and color while we also look at more complex changes such as changes in movement and posture.},
archivePrefix = {arXiv},
arxivId = {1710.11469},
author = {Heinze-Deml, Christina and Meinshausen, Nicolai},
eprint = {1710.11469},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Heinze-Deml, Meinshausen - 2017 - Conditional Variance Penalties and Domain Shift Robustness.pdf:pdf},
keywords = {anti-,causal models,causal prediction,dataset shift,distributional robustness,domain shift,image classification},
month = {oct},
number = {2009},
title = {{Conditional Variance Penalties and Domain Shift Robustness}},
url = {http://arxiv.org/abs/1710.11469},
year = {2017}
}
@article{Heinze-Deml,
abstract = {An important problem in many domains is to predict how a system will respond to interventions. This task is inherently linked to estimating the system's underlying causal structure. To this end, Invariant Causal Prediction (ICP) [10] has been proposed which learns a causal model exploiting the invariance of causal relations in different environments. When considering linear models, the implementation of ICP is relatively straightforward; the nonlinear case, however, is more challenging due to the difficulty of performing nonparametric tests for conditional independence. In this work, we present one method for nonlinear Invariant Causal Prediction. As an example, we consider fertility rate modeling which is central to world population projections. We explore predicting the effect of hypothetical interventions using the accepted models from nonlinear ICP.},
author = {Heinze-Deml, Christina and Peters, Jonas and Meinshausen, Nicolai},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Heinze-Deml, Peters, Meinshausen - Unknown - Predicting the effect of interventions using invariance principles for nonlinear models.pdf:pdf},
pages = {1--8},
title = {{Predicting the effect of interventions using invariance principles for nonlinear models}},
url = {http://www.homepages.ucl.ac.uk/{~}ucgtrbd/whatif/Paper15.pdf},
volume = {1}
}
@article{Holland1986,
author = {Holland, Paul W},
doi = {10.2307/2289069},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Holland - 1986 - Statistics and Causal Inference Rejoinder.pdf:pdf},
issn = {0162-1459},
journal = {J Am Stat Assoc},
keywords = {causal model,philosophy},
number = {396},
pages = {968},
title = {{Statistics and Causal Inference: Rejoinder}},
volume = {81},
year = {1986}
}
@article{Holland1986a,
abstract = {Abstract Problems involving causal inference have dogged at the heels of statistics since its earliest days. Correlation does not imply causation , and yet causal conclusions drawn from a carefully designed experiment are often valid. What can a statistical model say about ... },
author = {Holland, Paul W.},
doi = {10.1080/01621459.1986.10478354},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Holland - 1986 - Statistics and causal inference.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Association,Causal effect,Causal model,Experiments,Granger causality,Hill's nine factors,Koch's postulates,Mill's methods,Path diagrams,Philosophy,Probabilistic causality},
number = {396},
pages = {945--960},
title = {{Statistics and causal inference}},
volume = {81},
year = {1986}
}
@article{Hoover2006,
abstract = {An entry for the New Palgrave Dictionary of Economics. Traces the history of causality in economics and econometrics since David Hume. Examines the main modern approaches to causal inference.},
author = {Hoover, Kevin D.},
doi = {10.2139/ssrn.930739},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hoover - 2006 - Causality in Economics and Econometrics.pdf:pdf},
isbn = {978-0-333-78676-5},
issn = {1556-5068},
journal = {Ssrn},
keywords = {B16,B23,C10,Cowles Commission,Granger,Granger causality,Haavelmo,Hume,Jevons,Mill,Pearl,Simon,Spirtes,Zellner,causal inference,causality,econometrics,graph-theory,identification,structural vector autoregressions},
number = {June},
pmid = {24744601},
title = {{Causality in Economics and Econometrics}},
year = {2006}
}
@article{Hoyer,
author = {Hoyer, Patrik O and Janzing, Dominik and Peters, Jonas and Sch, Bernhard},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hoyer et al. - Unknown - NonlinearCausalDiscovery.pdf:pdf},
title = {{NonlinearCausalDiscovery}}
}
@book{Imbens2015,
author = {Imbens, Guido W. and Rubin, Donald B},
booktitle = {Causal Inference for Statistics, Social, and Biomedical Sciences},
doi = {10.1017/CBO9781139025751},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Imbens, Rubin - 2015 - Causal Inference for Statistics, Social, and Biomedical Sciences.pdf:pdf},
isbn = {9780521885881},
month = {apr},
publisher = {Cambridge University Press},
title = {{Causal Inference for Statistics, Social, and Biomedical Sciences}},
url = {https://www.cambridge.org/core/product/identifier/CBO9781139025751A535/type/book{\_}part https://www.cambridge.org/core/product/identifier/9781139025751/type/book},
year = {2015}
}
@article{Janzing2010,
abstract = {Inferring the causal structure that links n observables is usually based upon detecting statistical dependences and choosing simple graphs that make the joint measure Markovian. Here we argue why causal inference is also possible when the sample size is one. We develop a theory how to generate causal graphs explaining similarities between single objects. To this end, we replace the notion of conditional stochastic independence in the causal Markov condition with the vanishing of conditional algorithmic mutual information and describe the corresponding causal inference rules. We explain why a consistent reformulation of causal inference in terms of algorithmic complexity implies a new inference principle that takes into account also the complexity of conditional probability densities, making it possible to select among Markov equivalent causal graphs. This insight provides a theoretical foundation of a heuristic principle proposed in earlier work. We also sketch some ideas on how to replace Kolmogorov complexity with decidable complexity criteria. This can be seen as an algorithmic analog of replacing the empirically undecidable question of statistical independence with practical independence tests that are based on implicit or explicit assumptions on the underlying distribution. {\textcopyright} 2010 IEEE.},
archivePrefix = {arXiv},
arxivId = {0804.3678},
author = {Janzing, Dominik and Sch{\"{o}}lkopf, Bernhard},
doi = {10.1109/TIT.2010.2060095},
eprint = {0804.3678},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Janzing, Sch{\"{o}}lkopf - 2010 - Causal inference using the algorithmic Markov condition.pdf:pdf},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Algorithmic information,ChurchTuring thesis,data compression,graphical models,probability-free causal inference},
number = {10},
pages = {5168--5194},
title = {{Causal inference using the algorithmic Markov condition}},
volume = {56},
year = {2010}
}
@article{Janzing2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1402.2499v1},
author = {Janzing, Dominik and Shajarisales, Naji},
doi = {10.1007/978-3-319-21852-6},
eprint = {arXiv:1402.2499v1},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Janzing, Shajarisales - 2015 - Justifying Information-Geometric Causal Inference.pdf:pdf},
isbn = {9783319218526},
number = {August},
title = {{Justifying Information-Geometric Causal Inference}},
year = {2015}
}
@article{Johansson2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1605.03661v3},
author = {Johansson, Fredrik D},
eprint = {arXiv:1605.03661v3},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Johansson - 2016 - Learning Representations for Counterfactual Inference.pdf:pdf},
title = {{Learning Representations for Counterfactual Inference}},
volume = {48},
year = {2016}
}
@article{Journal2016,
author = {Journal, Source and Statistical, Royal and Series, Society},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Journal, Statistical, Series - 2016 - The Planning of Observational Studies of Human Populations Author ( s ) W . G . Cochran and S . Pa.pdf:pdf},
number = {2},
pages = {234--266},
title = {{The Planning of Observational Studies of Human Populations Author ( s ): W . G . Cochran and S . Paul Chambers Published by : Wiley for the Royal Statistical Society Stable URL : http://www.jstor.org/stable/2344179 Accessed : 31-07-2016 15 : 51 UTC Your u}},
volume = {128},
year = {2016}
}
@book{Keuzenkamp2000,
author = {Keuzenkamp, Hugo A.},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Keuzenkamp - 2000 - - Probability, econometrics and truth{\_} the methodology of econometrics (2000, Cambridge University Press)-1.pdf:pdf},
title = {{- Probability, econometrics and truth{\_} the methodology of econometrics (2000, Cambridge University Press)-1}},
year = {2000}
}
@article{Kocaoglu2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1709.02023v2},
author = {Kocaoglu, Murat and Snyder, Christopher and Dimakis, Alexandros G},
eprint = {arXiv:1709.02023v2},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kocaoglu, Snyder, Dimakis - 2017 - CausalGAN Learning Causal Implicit Generative Models with Adversarial Training.pdf:pdf},
pages = {1--37},
title = {{CausalGAN : Learning Causal Implicit Generative Models with Adversarial Training}},
year = {2017}
}
@book{Laan2011,
abstract = {The statistics profession is at a unique point in history. The need for valid statistical tools is greater than ever; data sets are massive, often measuring hundreds of thousands of measurements for a single subject. The field is ready to move towards clear objective benchmarks under which tools can be evaluated. Targeted learning allows (1) the full generalization and utilization of cross-validation as an estimator selection tool so that the subjective choices made by humans are now made by the machine, and (2) targeting the fitting of the probability distribution of the data toward the target parameter representing the scientific question of interest. This book is aimed at both statisticians and applied researchers interested in causal inference and general effect estimation for observational and experimental data. Part I is an accessible introduction to super learning and the targeted maximum likelihood estimator, including related concepts necessary to understand and apply these methods. Parts II-IX handle complex data structures and topics applied researchers will immediately recognize from their own research, including time-to-event outcomes, direct and indirect effects, positivity violations, case-control studies, censored data, longitudinal data, and genomic studies."Targeted Learning, by Mark J. van der Laan and Sherri Rose, fills a much needed gap in statistical and causal inference. It protects us from wasting computational, analytical, and data resources on irrelevant aspects of a problem and teaches us how to focus on what is relevant answering questions that researchers truly care about."-Judea Pearl, Computer Science Department, University of California, Los Angeles"In summary, this book should be on the shelf of every investigator who conducts observational research and randomized controlled trials. The concepts and methodology are foundational for causal inference and at the same time stay true to what the data at hand can say about the questions that motivate their collection."-Ira B. Tager, Division of Epidemiology, University of California, Berkeley},
author = {van der Laan, Mark J. and Rose, Sherri},
booktitle = {Springer Series in Statistics},
doi = {10.1007/978-1-4419-9782-1},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Laan, Rose - 2011 - Targeted Learning - preface.pdf:pdf},
isbn = {1441997814},
number = {2},
pages = {626},
title = {{Targeted Learning - preface}},
volume = {27},
year = {2011}
}
@article{Lewis1973,
author = {Lewis, David},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lewis - 1973 - Causation.pdf:pdf},
journal = {The Journal of Philosophy},
number = {17},
pages = {556--567},
title = {{Causation}},
volume = {70},
year = {1973}
}
@article{Little2019,
abstract = {To draw scientifically meaningful conclusions and build reliable models of quantitative phenomena, cause and effect must be taken into consideration (either implicitly or explicitly). This is particularly challenging when the measurements are not from controlled experimental (interventional) settings, since cause and effect can be obscured by spurious, indirect influences. Modern predictive techniques from machine learning are capable of capturing high-dimensional, nonlinear relationships between variables while relying on few parametric or probabilistic model assumptions. However, since these techniques are associational, applied to observational data they are prone to picking up spurious influences from non-experimental (observational) data, making their predictions unreliable. Techniques from causal inference, such as probabilistic causal diagrams and do-calculus, provide powerful (nonparametric) tools for drawing causal inferences from such observational data. However, these techniques are often incompatible with modern, nonparametric machine learning algorithms since they typically require explicit probabilistic models. Here, we develop causal bootstrapping for augmenting classical nonparametric bootstrap resampling with information on the causal relationship between variables. This makes it possible to resample observational data such that, if it is possible to identify an interventional relationship from that data, new data representing that relationship can be simulated from the original observational data. In this way, we can use modern machine learning algorithms unaltered to make statistically powerful, yet causally-robust, predictions. We develop several causal bootstrapping algorithms for drawing interventional inferences from observational data, for classification and regression problems, and demonstrate, using synthetic and real-world examples, the value of this approach.},
archivePrefix = {arXiv},
arxivId = {1910.09648},
author = {Little, Max A. and Badawy, Reham},
eprint = {1910.09648},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Little, Badawy - 2019 - Causal bootstrapping.pdf:pdf},
pages = {1--18},
title = {{Causal bootstrapping}},
url = {http://arxiv.org/abs/1910.09648},
year = {2019}
}
@article{Magliacane2018a,
abstract = {An important goal common to domain adaptation and causal inference is to make accurate predictions when the distributions for the source (or training) domain(s) and target (or test) domain(s) differ. In many cases, these different distributions can be modeled as different contexts of a single underlying system, in which each distribution corresponds to a different perturbation of the system, or in causal terms, an intervention. We focus on a class of such causal domain adaptation problems, where data for one or more source domains are given, and the task is to predict the distribution of a certain target variable from measurements of other variables in one or more target domains. We propose an approach for solving these problems that exploits causal inference and does not rely on prior knowledge of the causal graph, the type of interventions or the intervention targets. We demonstrate our approach by evaluating a possible implementation on simulated and real world data.},
archivePrefix = {arXiv},
arxivId = {1707.06422},
author = {Magliacane, Sara and {Van Ommen}, Thijs and Claassen, Tom and Bongers, Stephan and Mooij, Joris M. and Versteeg, Philip},
eprint = {1707.06422},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Magliacane et al. - 2018 - Predict Invariant Conditional Distributions.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {Nips},
pages = {10846--10856},
title = {{Domain adaptation by using causal inference to predict invariant conditional distributions}},
volume = {2018-Decem},
year = {2018}
}
@article{Magliacane2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1707.06422v3},
author = {Magliacane, Sara and Versteeg, Philip and Claassen, Tom and Bongers, Stephan},
eprint = {arXiv:1707.06422v3},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Magliacane et al. - 2018 - Predict Invariant Conditional Distributions.pdf:pdf},
number = {Nips},
title = {{Predict Invariant Conditional Distributions}},
year = {2018}
}
@article{Margaritis2003,
author = {Margaritis, Dimitris},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Margaritis - 2003 - Learning Bayesian Network Model Structure from Data.pdf:pdf},
title = {{Learning Bayesian Network Model Structure from Data}},
year = {2003}
}
@article{Meinshausen2018,
abstract = {We show some connections between distributional robust and causal inference. Distributional robustness is here understood as optimizing the predictive accuracy for a whole class of distributions instead of just a single target distribution. The class of distributions considered is often a ball around the empirical distribution, using a Wasserstein or Kullback-Leibler distance. Causal inference can be seen as a special case of distributional robustness, where the class of distributions is generated by a causal model. Different choices for these classes are discussed, which correspond to different assumptions about the type of interventions and presence or absence of latent confounding variables.},
author = {Meinshausen, Nicolai},
doi = {10.1109/DSW.2018.8439889},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Meinshausen - 2018 - CAUSALITY from A DISTRIBUTIONAL ROBUSTNESS POINT of VIEW.pdf:pdf},
isbn = {9781538644102},
journal = {2018 IEEE Data Science Workshop, DSW 2018 - Proceedings},
keywords = {Causal inference,distributional robustness,interventions},
number = {1},
pages = {6--10},
publisher = {IEEE},
title = {{CAUSALITY from A DISTRIBUTIONAL ROBUSTNESS POINT of VIEW}},
year = {2018}
}
@article{Claassen,
archivePrefix = {arXiv},
arxivId = {arXiv:1611.10351v3},
author = {Mooij, Joris M and Magliacane, Sara and Claassen, Tom},
eprint = {arXiv:1611.10351v3},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mooij, Magliacane, Claassen - 2016 - Joint Causal Inference from Multiple Datasets.pdf:pdf},
keywords = {causal discovery,inter-,observational and experimental data,randomized controlled trials,structure learning,ventions},
pages = {1--37},
title = {{Joint Causal Inference from Multiple Datasets}},
year = {2016}
}
@article{Mooij2016,
author = {Mooij, Joris M and Peters, Jonas and Janzing, Dominik and Zscheischler, Jakob},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mooij et al. - 2016 - Distinguishing Cause from Effect Using Observational Data Methods and Benchmarks.pdf:pdf},
pages = {1--102},
title = {{Distinguishing Cause from Effect Using Observational Data : Methods and Benchmarks}},
volume = {17},
year = {2016}
}
@techreport{Morgan,
abstract = {In this completely revised and expanded second edition of Counterfactuals and Causal Inference, the essential features of the counterfactual approach to observational data analysis are presented with examples from the social, demographic, and health sciences. Alternative estimation techniques are first introduced using both the potential outcome model and causal graphs; after which conditioning techniques, such as matching and regression, are presented from a potential outcomes perspective. For research scenarios in which important determinants of causal exposure are unobserved, alternative techniques, such as instrumental variable estimators, longitudinal methods, and estimation via causal mechanisms, are then presented. The importance of causal effect heterogeneity is stressed throughout the book, and the need for deep causal explanation via mechanisms is discussed. Sciences and the director of the Center for the Study of Inequality at Cornell University. His current areas of interest include social stratification, the sociology of education, and quantitative methodology. He has published On the Edge of Commitment: Educational Attainment and Race in the United States (2005) and, as editor, the Handbook of Causal Analysis for Social Research (2013). Christopher Winship is the Diker-Tishman Professor of Sociology and a member of the senior faculty of Harvard's Kennedy School of Government. Prior to coming to Harvard in 1992, he was Professor of Sociology and Statistics and by courtesy Economics at Northwestern University. His research focuses on statistical models for causal inference, most recently mechanisms and endogenous selection; how black clergy in Boston have worked with police to reduce youth violence; the effects of education on mental ability; pragmatism as the basis for a theory of action; the implications of advances in cognitive psychology for sociology; and sociological approaches to how individuals understand justice. Since 1995 he has been editor of Sociological Methods and Research.},
author = {Morgan, Stephen L. and Winship, Christopher},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Morgan, Winship - Unknown - COUNTERFACTUALS AND CAUSAL INFERENCE Second Edition.pdf:pdf},
title = {{COUNTERFACTUALS AND CAUSAL INFERENCE Second Edition}}
}
@book{Pearl2011,
abstract = {Written by one of the preeminent researchers in the field, this book provides a comprehensive exposition of modern analysis of causation. It shows how causality has grown from a nebulous concept into a mathematical theory with significant applications in the fields of statistics, artificial intelligence, economics, philosophy, cognitive science, and the health and social sciences. Judea Pearl presents and unifies the probabilistic, manipulative, counterfactual, and structural approaches to causation and devises simple mathematical tools for studying the relationships between causal connections and statistical associations. The book will open the way for including causal analysis in the standard curricula of statistics, artificial intelligence, business, epidemiology, social sciences, and economics. Students in these fields will find natural models, simple inferential procedures, and precise mathematical definitions of causal concepts that traditional texts have evaded or made unduly complicated. The first edition of Causality has led to a paradigmatic change in the way that causality is treated in statistics, philosophy, computer science, social science, and economics. Cited in more than 5,000 scientific publications, it continues to liberate scientists from the traditional molds of statistical thinking. In this revised edition, Judea Pearl elucidates thorny issues, answers readers' questions, and offers a panoramic view of recent advances in this field of research. Causality will be of interests to students and professionals in a wide variety of fields. Anyone who wishes to elucidate meaningful relationships from data, predict effects of actions and policies, assess explanations of reported events, or form theories of causal understanding and causal speech will find this book stimulating and invaluable.},
author = {Pearl, Judea},
booktitle = {Causality: Models, Reasoning, and Inference, Second Edition},
doi = {10.1017/CBO9780511803161},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pearl - 2011 - Causality Models, reasoning, and inference, second edition.pdf:pdf},
isbn = {9780511803161},
pages = {1--464},
title = {{Causality: Models, reasoning, and inference, second edition}},
year = {2011}
}
@book{Pearl2000,
abstract = {Chapter headings: Introduction to Probabilities, Graphs, and Causal Models; A Theory of Inferred Causation; Causal Diagrams and the Identification of Causal Effects; Actions, Plans, and Direct Effects; Causality and Structural Models in Social Science and Economics; Simpson's Paradoxon, Confounding, and Collapsibility; The Logic of Structure-Based Counterfactuals; Imperfect Experiments: Bounding Effects and Counterfactuals; Probability of Causation: Interpretation and Identification; The Actual Cause; Epilogue: The Art and Science of Cause and Effect},
author = {Pearl, Judea},
doi = {citeulike-article-id:3888442},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pearl - 2000 - Causality Second Edition.pdf:pdf},
isbn = {0521773628},
pages = {386},
title = {{Causality Second Edition}},
year = {2000}
}
@article{Peters2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1501.01332v3},
author = {Peters, Jonas and B{\"{u}}hlmann, Peter and Meinshausen, Nicolai},
eprint = {arXiv:1501.01332v3},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Peters, B{\"{u}}hlmann, Meinshausen - 2015 - Causal inference using invariant prediction identification and confidence intervals.pdf:pdf},
pages = {1--51},
title = {{Causal inference using invariant prediction : identification and confidence intervals}},
year = {2015}
}
@book{Peters2017,
abstract = {"The mathematization of causality is a relatively recent development, and has become increasingly important in data science and machine learning. This book offers a self-contained and concise introduction to causal models and how to learn them from data"--Back of book.},
author = {Peters, Jonas and Janzing, Dominik and Sch{\"{o}}lkopf, Bernhard},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Peters, Janzing, Sch{\"{o}}lkopf - 2017 - Elements of causal inference foundations and learning algorithms.pdf:pdf},
isbn = {9780262037310},
number = {December},
pages = {1214--1216},
title = {{Elements of causal inference: foundations and learning algorithms}},
year = {2017}
}
@article{Rojas-carulla2018,
abstract = {Methods of transfer learning try to combine knowledge from several related tasks (or domains) to improve performance on a test task. Inspired by causal methodology, we relax the usual covariate shift assumption and assume that it holds true for a subset of predictor variables: the conditional distribution of the target variable given this subset of predictors is invariant over all tasks. We show how this assumption can be motivated from ideas in the field of causality. We focus on the problem of Domain Generalization, in which no examples from the test task are observed. We prove that in an adversarial setting using this subset for prediction is optimal in Domain Generalization; we further provide examples, in which the tasks are sufficiently diverse and the estimator therefore outperforms pooling the data, even on average. If examples from the test task are available, we also provide a method to transfer knowledge from the training tasks and exploit all available features for prediction. However, we provide no guarantees for this method. We introduce a practical method which allows for automatic inference of the above subset and provide corresponding code. We present results on synthetic data sets and a gene deletion data set.},
author = {Rojas-Carulla, Mateo and Sch{\"{o}}lkopf, Bhoernhard and Turner, Richard and Peters, Jonas},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rojas-Carulla et al. - 2018 - Invariant models for causal transfer learning.pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Causality,Domain adaptation,Domain generalization,Multi-task learning,Transfer learning},
pages = {1--34},
title = {{Invariant models for causal transfer learning}},
volume = {19},
year = {2018}
}
@article{Rosenbaum2005,
abstract = {Before R. A. Fisher introduced randomized experimentation, the literature on empirical methods emphasized reducing heterogeneity of experimental units as the key to inference about the effects caused by treatments. To what extent is heterogeneity relevant to causal inference when ethicalor practical constraints make random assignment infeasible?},
author = {Rosenbaum, Paul R.},
doi = {10.1198/000313005X42831},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rosenbaum - 2005 - Heterogeneity and causality Unit heterogeneity and design sensitivity in observational studies.pdf:pdf},
issn = {00031305},
journal = {American Statistician},
keywords = {Causal effect,Observational study,Randomization inference,Randomized experiment,Sensitivity analysis,Wilcoxon's signed rank test},
number = {2},
pages = {147--152},
title = {{Heterogeneity and causality: Unit heterogeneity and design sensitivity in observational studies}},
volume = {59},
year = {2005}
}
@article{Rothenh2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1801.06229v2},
author = {Rothenh, Dominik and Meinshausen, Nicolai and Peter, B and Peters, Jonas},
eprint = {arXiv:1801.06229v2},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rothenh et al. - 2018 - Anchor regression heterogeneous data meet causality.pdf:pdf},
pages = {1--31},
title = {{Anchor regression : heterogeneous data meet causality}},
year = {2018}
}
@article{Rubin1974,
abstract = {A discussion of matching, randomization, random sampling, and other methods of controlling extraneous variation is presented. The objective is to specify the benefits of randomization in estimating causal effects of treatments. The basic conclusion is that randomization should be employed whenever possible but that the use of carefully controlled nonrandomized data to estimate causal effects is a reasonable and nec-essary procedure in many cases.},
author = {{Rubin D. B}},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rubin D. B - 1974 - Estimating causal effects of treatment in randomized and nonrandomized studies.pdf:pdf},
journal = {Journal of Educational Psychology},
number = {5},
pages = {688--701},
title = {{Estimating causal effects of treatment in randomized and nonrandomized studies}},
url = {http://www.fsb.muohio.edu/lij14/420{\_}paper{\_}Rubin74.pdf},
volume = {66},
year = {1974}
}
@article{Rubin1977,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and proteinâˆ’protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD â‰¤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Rubin, Donald B.},
doi = {10.3102/10769986002001001},
eprint = {arXiv:1011.1669v3},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rubin - 1977 - Assignment to Treatment Group on the Basis of a Covariate.pdf:pdf},
isbn = {9788578110796},
issn = {0362-9791},
journal = {Journal of Educational Statistics},
keywords = {icle},
month = {mar},
number = {1},
pages = {1--26},
pmid = {25246403},
title = {{Assignment to Treatment Group on the Basis of a Covariate}},
url = {http://journals.sagepub.com/doi/10.3102/10769986002001001},
volume = {2},
year = {1977}
}
@inproceedings{Sch2012,
abstract = {We consider the problem of function estimation in the case where an underlying causal model can be inferred. This has implications for popular scenarios such as covariate shift, concept drift, transfer learning and semi-supervised learning. We argue that causal knowledge may facilitate some approaches for a given problem, and rule out others. In particular, we formulate a hypothesis for when semi-supervised learning can help, and corroborate it with empirical results.},
author = {Sch{\"{o}}lkopf, Bernhard and Janzing, Dominik and Peters, Jonas and Sgouritsa, Eleni and Zhang, Kun and Mooij, Joris},
booktitle = {Proceedings of the 29th International Conference on Machine Learning, ICML 2012},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sch{\"{o}}lkopf et al. - 2012 - On causal and anticausal learning.pdf:pdf},
isbn = {9781450312851},
pages = {1255--1262},
title = {{On causal and anticausal learning}},
volume = {2},
year = {2012}
}
@article{Schulam2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1812.04597v2},
author = {Schulam, Peter},
eprint = {arXiv:1812.04597v2},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schulam - 2019 - Preventing Failures Due to Dataset Shift Learning Predictive Models That Transport.pdf:pdf},
title = {{Preventing Failures Due to Dataset Shift: Learning Predictive Models That Transport}},
volume = {89},
year = {2019}
}
@article{Sgouritsa2015,
abstract = {We address the problem of causal discov-ery in the two-variable case given a sample from their joint distribution. The proposed method is based on a known assumption that, if X â†’ Y (X causes Y), the marginal distri-bution of the cause, P (X), contains no in-formation about the conditional distribution P (Y |X). Consequently, estimating P (Y |X) from P (X) should not be possible. However, estimating P (X|Y) based on P (Y) may be possible. This paper employs this asymmetry to pro-pose CURE, a causal discovery method which decides upon the causal direction by com-paring the accuracy of the estimations of P (Y |X) and P (X|Y). To this end, we pro-pose a method for estimating a conditional from samples of the corresponding marginal, which we call unsupervised inverse GP re-gression. We evaluate CURE on synthetic and real data. On the latter, our method outperforms existing causal inference meth-ods.},
author = {Sgouritsa, Eleni and Janzing, Dominik and Hennig, Philipp and Sch{\"{o}}lkopf, Bernhard},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sgouritsa et al. - 2015 - Inference of Cause and Effect with Unsupervised Inverse Regression.pdf:pdf},
issn = {15337928},
journal = {Artificial Intelligence and Statistics},
pages = {847----855},
title = {{Inference of Cause and Effect with Unsupervised Inverse Regression}},
volume = {38},
year = {2015}
}
@article{Shah2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1804.07203v2},
author = {Shah, Rajen D},
eprint = {arXiv:1804.07203v2},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shah - 2018 - The Hardness of Conditional Independence Testing and the Generalised Covariance Measure.pdf:pdf},
number = {July 2017},
title = {{The Hardness of Conditional Independence Testing and the Generalised Covariance Measure}},
year = {2018}
}
@article{Speed1992,
author = {Speed, T. P.},
doi = {10.1007/978-1-4612-4380-9_7},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Speed - 1992 - Introduction to Fisher (1926) The Arrangement of Field Experiments.pdf:pdf},
number = {1926},
pages = {71--81},
title = {{Introduction to Fisher (1926) The Arrangement of Field Experiments}},
year = {1992}
}
@article{Spirtes1991,
abstract = {Previous asymptotically correct algorithms for recovering causal structure from sample probabilities have been limited even in sparse causal graphs to a few variables. We describe an asymptotically correct algorithm whose complexity for fixed graph connectivity increases polynomially in the number of vertices, and may in practice recover sparse graphs with several hundred variables. From sample data with n = 20,000, an implementation of the algorithm on a DECStation 3100 recovers the edges in a linear version of the ALARM network with 37 vertices and 46 edges. Fewer than 8{\%} of the undirected edges are incorrectly identified in the output. Without prior ordering information, the program also determines the direction of edges for the ALARM graph with an error rate of 14{\%}. Processing time is less than 10 seconds. Keywords DAGS, Causal Modelling.},
author = {Spirtes, Peter and Glymour, Clark},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Spirtes, Glymour - 1991 - An Algorithm for Fast Recovery of Sparse Causal Graphs.pdf:pdf},
journal = {Social Science Computer Review},
keywords = {Graph theory.},
number = {1},
pages = {62--72},
title = {{An Algorithm for Fast Recovery of Sparse Causal Graphs}},
url = {https://doi.org/10.1177/089443939100900106},
volume = {9},
year = {1991}
}
@book{Spirtes,
author = {Spirtes, Peter and Glymour, Clark},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Spirtes, Glymour - Unknown - Causation, Prediction, and Search.pdf:pdf},
isbn = {0262194406},
title = {{Causation, Prediction, and Search}}
}
@article{Spirtes1990,
abstract = {NOVELTY - The tibial implant component has a tibial tray (26) adjacent the augmentation location and a depending structure extending longitudinally from the tibial tray. The tibial tray has a proximal surface and an opposite distal surface. An abutment is made on the depending structure, which extends transversely along the depending structure adjacent the augmentation location. The securing element is coupled with the tibial augment for selective rotation about a longitudinal axis of rotation located adjacent the abutment when the tibial augment is placed at the augmentation location and juxtaposed with the tibial tray. USE - For facilitating securement of a selected augment to an implant component, such as the securement of a selected tibial augment to a tibial implant component of a prosthetic knee implant. ADVANTAGE - Enables a surgeon quickly and effectively to attach a selected augment to an implant component, such as the attachment of a selected tibial augment to a tibial implant component, inter operatively, without resorting to cementing and clamping, and without the necessity for assembling loose fasteners with the augment and the implant component. Avoids deleterious effects which otherwise could result from migrating particulate polyethylene, as well as cold flow of polyethylene due to excessive stress at surfaces where the bearing member of a tibial implant is supported by the tibial tray of a tibial implant component. DETAILED DESCRIPTION - A securing surface on the securing element is arranged for longitudinal displacement relative to the abutment upon rotation of the securing element. The longitudinal displacement engages the securing surface with the abutment to establish a longitudinal securing force between the securing element and the depending structure for urging the tibial augment against the tibial implant component. This ensures the securement of the tibial augment to the tibial implant component at the augmentation location. An INDEPENDENT CLAIM is given for a method of facilitating securement of a selected augment at an augmentation location. DESCRIPTION OF DRAWING(S) - The figure shows an exploded view of the tibial augment and tibial implant component. tibial augment (20) tibial implant component (24) tibial fixing (26) stem boss (28) stabilizing keel (30) securing mechanism (60)},
author = {Spirtes, Peter and Glymour, Clark N and Scheines, Richard},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Spirtes, Glymour, Scheines - 1990 - Causality from probability.pdf:pdf},
journal = {Advanced Computing for the Social Sciences},
keywords = {Statistical hypothesis testing.},
title = {{Causality from probability}},
year = {1990}
}
@article{Suter,
archivePrefix = {arXiv},
arxivId = {arXiv:1811.00007v1},
author = {Suter, Raphael and Bauer, Stefan and Sch, Bernhard},
eprint = {arXiv:1811.00007v1},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Suter, Bauer, Sch - Unknown - Interventional Robustness of Deep Latent Variable Models.pdf:pdf},
keywords = {causal-,disentanglement,representation learning,variational auto-encoders},
pages = {1--34},
title = {{Interventional Robustness of Deep Latent Variable Models}}
}
@article{Suter2018,
abstract = {The ability to learn disentangled representations that split underlying sources of variation in high dimensional, unstructured data is important for data efficient and robust use of neural networks. While various approaches aiming towards this goal have been proposed in recent times, a commonly accepted definition and validation procedure is missing. We provide a causal perspective on representation learning which covers disentanglement and domain shift robustness as special cases. Our causal framework allows us to introduce a new metric for the quantitative evaluation of deep latent variable models. We show how this metric can be estimated from labeled observational data and further provide an efficient estimation algorithm that scales linearly in the dataset size.},
archivePrefix = {arXiv},
arxivId = {1811.00007},
author = {Suter, Raphael and Miladinovi{\'{c}}, ÄorÄ‘e and Sch{\"{o}}lkopf, Bernhard and Bauer, Stefan},
eprint = {1811.00007},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Suter et al. - 2018 - Robustly Disentangled Causal Mechanisms Validating Deep Representations for Interventional Robustness.pdf:pdf},
title = {{Robustly Disentangled Causal Mechanisms: Validating Deep Representations for Interventional Robustness}},
url = {http://arxiv.org/abs/1811.00007},
year = {2018}
}
@book{VanderLaan2018,
author = {van der Laan, Mark J. and Rose, Sherri},
doi = {10.1007/978-3-319-65304-4},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/van der Laan, Rose - 2018 - Targeted Learning in Data Science.pdf:pdf},
isbn = {978-3-319-65303-7},
title = {{Targeted Learning in Data Science}},
url = {http://link.springer.com/10.1007/978-3-319-65304-4},
year = {2018}
}
@book{VanderWeele2015,
abstract = {The book provides an accessible but comprehensive overview of methods for mediation and interaction. There has been considerable and rapid methodological development on mediation and moderation/interaction analysis within the causal-inference literature over the last ten years. Much of this material appears in a variety of specialized journals, and some of the papers are quite technical. There has also been considerable interest in these developments from empirical researchers in the social and biomedical sciences. However, much of the material is not currently in a format that is accessible t},
author = {VanderWeele, Tyler},
doi = {http://dx.doi.org/10.1177/0146167212443896},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/VanderWeele - 2015 - Explanation in Causal Inference.pdf:pdf},
isbn = {9780199325870},
issn = {0146-1672, 0146-1672},
pages = {729},
pmid = {29982528},
title = {{Explanation in Causal Inference}},
url = {http://gbv.eblib.com/patron/FullRecord.aspx?p=1920742},
year = {2015}
}
@article{Zhang2015,
abstract = {Recent developments in structural equation modeling have produced several methods that can usually distinguish cause from effect in the two-variable case. For that purpose, however, one has to impose substantial structural constraints or smoothness assumptions on the functional causal models. In this paper, we consider the problem of determining the causal direction from a related but different point of view, and propose a new framework for causal direction determination. We show that it is possible to perform causal inference based on the condition that the cause is "exogenous" for the parameters involved in the generating process from the cause to the effect. In this way, we avoid the structural constraints required by the SEM-based approaches. In particular, we exploit nonparametric methods to estimate marginal and conditional distributions, and propose a bootstrap-based approach to test for the exogeneity condition; the testing results indicate the causal direction between two variables. The proposed method is validated on both synthetic and real data.},
archivePrefix = {arXiv},
arxivId = {1504.05651},
author = {Zhang, Kun and Zhang, Jiji and Sch{\"{o}}lkopf, Bernhard},
eprint = {1504.05651},
file = {:home/nandan/.var/app/com.elsevier.MendeleyDesktop/data/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Zhang, Sch{\"{o}}lkopf - 2015 - Distinguishing Cause from Effect Based on Exogeneity.pdf:pdf},
keywords = {bootstrap,causal direction,causal discovery,dependence,exogeneity,statistical in-},
month = {apr},
title = {{Distinguishing Cause from Effect Based on Exogeneity}},
url = {http://arxiv.org/abs/1504.05651},
year = {2015}
}
